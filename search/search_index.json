{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IOTStack Wiki \u00b6 Welcome to the IOTstack Wiki: Use the list of contents at the left of this page to explore this Wiki. If you are viewing this on a device that does not show the list by default, click the \"\u2261\" icon. If you are looking for information on a specific container, click on the \"Containers\" folder at the bottom of the list. If you are just getting started with IOTstack, see Getting Started . To contribute see Contributing","title":"IOTStack Wiki"},{"location":"#iotstack-wiki","text":"Welcome to the IOTstack Wiki: Use the list of contents at the left of this page to explore this Wiki. If you are viewing this on a device that does not show the list by default, click the \"\u2261\" icon. If you are looking for information on a specific container, click on the \"Containers\" folder at the bottom of the list. If you are just getting started with IOTstack, see Getting Started . To contribute see Contributing","title":"IOTStack Wiki"},{"location":"Basic_setup/","text":"Getting Started \u00b6 introduction to IOTstack - videos \u00b6 Andreas Spiess Video #295: Raspberry Pi Server based on Docker, with VPN, Dropbox backup, Influx, Grafana, etc: IOTstack Andreas Spiess Video #352: Raspberry Pi4 Home Automation Server (incl. Docker, OpenHAB, HASSIO, NextCloud) assumptions \u00b6 IOTstack makes the following assumptions: Your hardware is a Raspberry Pi (typically a 3B+ or 4B). Note: The Raspberry Pi Zero W2 has been tested with IOTstack. It works but the 512MB RAM means you should not try to run too many containers concurrently. Your Raspberry Pi has a reasonably-recent version of 32-bit Raspberry Pi OS (aka \"Raspbian\") installed. You can download operating-system images: Current release Prior releases Note: If you use the first link, \"Raspberry Pi OS with desktop\" is recommended. The second link only offers \"Raspberry Pi OS with desktop\" images. Your operating system has been kept up-to-date with: $ sudo apt update $ sudo apt upgrade -y You are logged-in as the user \"pi\". User \"pi\" has the user ID 1000. The home directory for user \"pi\" is /home/pi/ . IOTstack is installed at /home/pi/IOTstack (with that exact spelling). If the first three assumptions hold, assumptions four through six are Raspberry Pi defaults on a clean installation. The seventh is what you get if you follow these instructions faithfully. Please don't read these assumptions as saying that IOTstack will not run on other hardware, other operating systems, or as a different user. It is just that IOTstack gets most of its testing under these conditions. The further you get from these implicit assumptions, the more your mileage may vary. other platforms \u00b6 Users have reported success on other platforms, including: Orange Pi WinPlus new installation \u00b6 automatic (recommended) \u00b6 Install curl : $ sudo apt install -y curl Run the following command: $ curl -fsSL https://raw.githubusercontent.com/SensorsIot/IOTstack/master/install.sh | bash Run the menu and choose your containers: $ cd ~/IOTstack $ ./menu.sh Bring up your stack: $ cd ~/IOTstack $ docker-compose up -d manual \u00b6 Install git : $ sudo apt install -y git Clone IOTstack: If you want \"new menu\": $ git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack If you prefer \"old menu\": $ git clone -b old-menu https://github.com/SensorsIot/IOTstack.git ~/IOTstack Run the menu and choose your containers: $ cd ~/IOTstack $ ./menu.sh Note: If you are running \"old menu\" for the first time, you will be guided to \"Install Docker\". That will end in a reboot, after which you should re-enter the menu and choose your containers. Bring up your stack: $ cd ~/IOTstack $ docker-compose up -d scripted \u00b6 If you prefer to automate your installations using scripts, see: Installing Docker for IOTstack . migrating from the old repo (gcgarner)? \u00b6 If you are still running on gcgarner/IOTstack and need to migrate to SensorsIot/IOTstack, see: Migrating IOTstack from gcgarner to SensorsIot . recommended system patches \u00b6 patch 1 \u2013 restrict DHCP \u00b6 Run the following commands: $ sudo bash -c '[ $(egrep -c \"^allowinterfaces eth\\*,wlan\\*\" /etc/dhcpcd.conf) -eq 0 ] && echo \"allowinterfaces eth*,wlan*\" >> /etc/dhcpcd.conf' $ sudo reboot See Issue 219 and Issue 253 for more information. patch 2 \u2013 update libseccomp2 \u00b6 This patch is ONLY for Raspbian Buster. Do NOT install this patch if you are running Raspbian Bullseye. step 1: check your OS release \u00b6 Run the following command: $ grep \"PRETTY_NAME\" /etc/os-release PRETTY_NAME = \"Raspbian GNU/Linux 10 (buster)\" If you see the word \"buster\", proceed to step 2. Otherwise, skip this patch. step 2: if you are running \"buster\" \u2026 \u00b6 You need this patch if you are running Raspbian Buster. Without this patch, Docker images will fail if: the image is based on Alpine and the image's maintainer updates to Alpine 3.13 ; and/or an image's maintainer updates to a library that depends on 64-bit values for Unix epoch time (the so-called Y2038 problem). To install the patch: $ sudo apt-key adv --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys 04EE7237B7D453EC 648ACFD622F3D138 $ echo \"deb http://httpredir.debian.org/debian buster-backports main contrib non-free\" | sudo tee -a \"/etc/apt/sources.list.d/debian-backports.list\" $ sudo apt update $ sudo apt install libseccomp2 -t buster-backports patch 3 - kernel control groups \u00b6 Kernel control groups need to be enabled in order to monitor container specific usage. This makes commands like docker stats fully work. Also needed for full monitoring of docker resource usage by the telegraf container. Enable by running (takes effect after reboot): echo $(cat /boot/cmdline.txt) cgroup_memory=1 cgroup_enable=memory | sudo tee /boot/cmdline.txt a word about the sudo command \u00b6 Many first-time users of IOTstack get into difficulty by misusing the sudo command. The problem is best understood by example. In the following, you would expect ~ (tilde) to expand to /home/pi . It does: $ echo ~/IOTstack /home/pi/IOTstack The command below sends the same echo command to bash for execution. This is what happens when you type the name of a shell script. You get a new instance of bash to run the script: $ bash -c 'echo ~/IOTstack' /home/pi/IOTstack Same answer. Again, this is what you expect. But now try it with sudo on the front: $ sudo bash -c 'echo ~/IOTstack' /root/IOTstack Different answer. It is different because sudo means \"become root, and then run the command\". The process of becoming root changes the home directory, and that changes the definition of ~ . Any script designed for working with IOTstack assumes ~ (or the equivalent $HOME variable) expands to /home/pi . That assumption is invalidated if the script is run by sudo . Of necessity, any script designed for working with IOTstack will have to invoke sudo inside the script when it is required . You do not need to second-guess the script's designer. Please try to minimise your use of sudo when you are working with IOTstack. Here are some rules of thumb: Is what you are about to run a script? If yes, check whether the script already contains sudo commands. Using menu.sh as the example: $ grep -c 'sudo' ~/IOTstack/menu.sh 28 There are numerous uses of sudo within menu.sh . That means the designer thought about when sudo was needed. Did the command you just executed work without sudo ? Note the emphasis on the past tense. If yes, then your work is done. If no, and the error suggests elevated privileges are necessary, then re-execute the last command like this: $ sudo !! It takes time, patience and practice to learn when sudo is actually needed. Over-using sudo out of habit, or because you were following a bad example you found on the web, is a very good way to find that you have created so many problems for yourself that will need to reinstall your IOTstack. Please err on the side of caution! the IOTstack menu \u00b6 The menu is used to install Docker and then build the docker-compose.yml file which is necessary for starting the stack. The menu is only an aid. It is a good idea to learn the docker and docker-compose commands if you plan on using Docker in the long run. menu item: Install Docker (old menu only) \u00b6 Please do not try to install docker and docker-compose via sudo apt install . There's more to it than that. Docker needs to be installed by menu.sh . The menu will prompt you to install docker if it detects that docker is not already installed. You can manually install it from within the Native Installs menu: $ cd ~/IOTstack $ ./menu.sh Select \"Native Installs\" Select \"Install Docker and Docker-Compose\" Follow the prompts. The process finishes by asking you to reboot. Do that! Note: New menu (master branch) automates this step. menu item: Build Stack \u00b6 docker-compose uses a docker-compose.yml file to configure all your services. The docker-compose.yml file is created by the menu: $ cd ~/IOTstack $ ./menu.sh Select \"Build Stack\" Follow the on-screen prompts and select the containers you need. The best advice we can give is \"start small\". Limit yourself to the core containers you actually need (eg Mosquitto, Node-RED, InfluxDB, Grafana, Portainer). You can always add more containers later. Some users have gone overboard with their initial selections and have run into what seem to be Raspberry Pi OS limitations. Key point: If you are running \"new menu\" (master branch) and you select Node-RED, you must press the right-arrow and choose at least one add-on node. If you skip this step, Node-RED will not build properly. Old menu forces you to choose add-on nodes for Node-RED. The process finishes by asking you to bring up the stack: $ cd ~/IOTstack $ docker-compose up -d The first time you run up the stack docker will download all the images from DockerHub. How long this takes will depend on how many containers you selected and the speed of your internet connection. Some containers also need to be built locally. Node-RED is an example. Depending on the Node-RED nodes you select, building the image can also take a very long time. This is especially true if you select the SQLite node. Be patient (and ignore the huge number of warnings). menu item: Docker commands \u00b6 The commands in this menu execute shell scripts in the root of the project. other menu items \u00b6 The old and new menus differ in the options they offer. You should come back and explore them once your stack is built and running. switching menus \u00b6 At the time of writing, IOTstack supports three menus: \"Old Menu\" on the old-menu branch. This was inherited from gcgarner/IOTstack . \"New Menu\" on the master branch. This is the current menu. \"New New Menu\" on the experimental branch. This is under development. With a few precautions, you can switch between git branches as much as you like without breaking anything. The basic check you should perform is: $ cd ~/IOTstack $ git status Check the results to see if any files are marked as \"modified\". For example: modified: .templates/mosquitto/Dockerfile Key point: Files marked \"untracked\" do not matter. You only need to check for \"modified\" files because those have the potential to stop you from switching branches cleanly. The way to avoid potential problems is to move any modified files to one side and restore the unmodified original. For example: $ mv .templates/mosquitto/Dockerfile .templates/mosquitto/Dockerfile.save $ git checkout -- .templates/mosquitto/Dockerfile When git status reports no more \"modified\" files, it is safe to switch your branch. current menu (master branch) \u00b6 $ cd ~/IOTstack/ $ git pull $ git checkout master $ ./menu.sh old menu (old-menu branch) \u00b6 $ cd ~/IOTstack/ $ git pull $ git checkout old-menu $ ./menu.sh experimental branch \u00b6 Switch to the experimental branch to try the latest and greatest features. $ cd ~/IOTstack/ $ git pull $ git checkout experimental $ ./menu.sh Notes: Please make sure you have a good backup before you start. The experimental branch may be broken, or may break your setup. Please report any issues. Remember: you can switch git branches as much as you like without breaking anything. simply launching the menu (any version) won't change anything providing you exit before letting the menu complete. running the menu to completion will change your docker-compose.yml and supporting structures in ~/IOTstack/services . running docker-compose up -d will change your running containers. The way back is to take down your stack, restore a backup, and bring up your stack again. useful commands: docker & docker-compose \u00b6 Handy rules: docker commands can be executed from anywhere, but docker-compose commands need to be executed from within ~/IOTstack starting your IOTstack \u00b6 To start the stack: $ cd ~/IOTstack $ docker-compose up -d Once the stack has been brought up, it will stay up until you take it down. This includes shutdowns and reboots of your Raspberry Pi. If you do not want the stack to start automatically after a reboot, you need to stop the stack before you issue the reboot command. logging journald errors \u00b6 If you get docker logging error like: Cannot create container for service [service name here]: unknown log opt 'max-file' for journald log driver Run the command: $ sudo nano /etc/docker/daemon.json change: \"log-driver\": \"journald\", to: \"log-driver\": \"json-file\", Logging limits were added to prevent Docker using up lots of RAM if log2ram is enabled, or SD cards being filled with log data and degraded from unnecessary IO. See Docker Logging configurations You can also turn logging off or set it to use another option for any service by using the IOTstack docker-compose-override.yml file mentioned at IOTstack/Custom . starting an individual container \u00b6 To start a particular container: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb stopping your IOTstack \u00b6 Stopping aka \"downing\" the stack stops and deletes all containers, and removes the internal network: $ cd ~/IOTstack $ docker-compose down To stop the stack without removing containers, run: $ cd ~/IOTstack $ docker-compose stop stopping an individual container \u00b6 stop can also be used to stop individual containers, like this: $ cd ~/IOTstack $ docker-compose stop \u00abcontainer\u00bb This puts the container in a kind of suspended animation. You can resume the container with $ cd ~/IOTstack $ docker-compose start \u00abcontainer\u00bb There is no equivalent of down for a single container. It needs: $ cd ~/IOTstack $ docker-compose rm --force --stop -v \u00abcontainer\u00bb To reactivate a container which has been stopped and removed: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb checking container status \u00b6 You can check the status of containers with: $ docker ps or $ cd ~/IOTstack $ docker-compose ps viewing container logs \u00b6 You can inspect the logs of most containers like this: $ docker logs \u00abcontainer\u00bb for example: $ docker logs nodered You can also follow a container's log as new entries are added by using the -f flag: $ docker logs -f nodered Terminate with a Control+C. Note that restarting a container will also terminate a followed log. restarting a container \u00b6 You can restart a container in several ways: $ cd ~/IOTstack $ docker-compose restart \u00abcontainer\u00bb This kind of restart is the least-powerful form of restart. A good way to think of it is \"the container is only restarted, it is not rebuilt\". If you change a docker-compose.yml setting for a container and/or an environment variable file referenced by docker-compose.yml then a restart is usually not enough to bring the change into effect. You need to make docker-compose notice the change: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb This type of \"restart\" rebuilds the container. Alternatively, to force a container to rebuild (without changing either docker-compose.yml or an environment variable file): $ cd ~/IOTstack $ docker-compose up -d --force-recreate \u00abcontainer\u00bb See also updating images built from Dockerfiles if you need to force docker-compose to notice a change to a Dockerfile. persistent data \u00b6 Docker allows a container's designer to map folders inside a container to a folder on your disk (SD, SSD, HD). This is done with the \"volumes\" key in docker-compose.yml . Consider the following snippet for Node-RED: volumes : - ./volumes/nodered/data:/data You read this as two paths, separated by a colon. The: external path is ./volumes/nodered/data internal path is /data In this context, the leading \".\" means \"the folder containing docker-compose.yml \", so the external path is actually: ~/IOTstack/volumes/nodered/data If a process running inside the container modifies any file or folder within: /data the change is mirrored outside the container at the same relative path within: ~/IOTstack/volumes/nodered/data The same is true in reverse. Any change made to any file or folder outside the container within: ~/IOTstack/volumes/nodered/data is mirrored at the same relative path inside the container at: /data deleting persistent data \u00b6 If you need a \"clean slate\" for a container, you can delete its volumes. Using InfluxDB as an example: $ cd ~/IOTstack $ docker-compose rm --force --stop -v influxdb $ sudo rm -rf ./volumes/influxdb $ docker-compose up -d influxdb When docker-compose tries to bring up InfluxDB, it will notice this volume mapping in docker-compose.yml : volumes : - ./volumes/influxdb/data:/var/lib/influxdb and check to see whether ./volumes/influxdb/data is present. Finding it not there, it does the equivalent of: $ sudo mkdir -p ./volumes/influxdb/data When InfluxDB starts, it sees that the folder on right-hand-side of the volumes mapping ( /var/lib/influxdb ) is empty and initialises new databases. This is how most containers behave. There are exceptions so it's always a good idea to keep a backup. stack maintenance \u00b6 update Raspberry Pi OS \u00b6 You should keep your Raspberry Pi up-to-date. Despite the word \"container\" suggesting that containers are fully self-contained, they sometimes depend on operating system components (\"WireGuard\" is an example). $ sudo apt update $ sudo apt upgrade -y git pull \u00b6 Although the menu will generally do this for you, it does not hurt to keep your local copy of the IOTstack repository in sync with the master version on GitHub. $ cd ~/IOTstack $ git pull container image updates \u00b6 There are two kinds of images used in IOTstack: Those not built using Dockerfiles (the majority) Those built using Dockerfiles (special cases) A Dockerfile is a set of instructions designed to customise an image before it is instantiated to become a running container. The easiest way to work out which type of image you are looking at is to inspect the container's service definition in your docker-compose.yml file. If the service definition contains the: image: keyword then the image is not built using a Dockerfile. build: keyword then the image is built using a Dockerfile. updating images not built from Dockerfiles \u00b6 If new versions of this type of image become available on DockerHub, your local IOTstack copies can be updated by a pull command: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune The pull downloads any new images. It does this without disrupting the running stack. The up -d notices any newly-downloaded images, builds new containers, and swaps old-for-new. There is barely any downtime for affected containers. updating images built from Dockerfiles \u00b6 Containers built using Dockerfiles have a two-step process: A base image is downloaded from from DockerHub; and then The Dockerfile \"runs\" to build a local image. Node-RED is a good example of a container built from a Dockerfile. The Dockerfile defines some (or possibly all) of your add-on nodes, such as those needed for InfluxDB or Tasmota. There are two separate update situations that you need to consider: If your Dockerfile changes; or If a newer base image appears on DockerHub Node-RED also provides a good example of why your Dockerfile might change: if you decide to add or remove add-on nodes. Note: You can also add nodes to Node-RED using Manage Palette. when Dockerfile changes ( local image only) \u00b6 When your Dockerfile changes, you need to rebuild like this: $ cd ~/IOTstack $ docker-compose up --build -d \u00abcontainer\u00bb $ docker system prune This only rebuilds the local image and, even then, only if docker-compose senses a material change to the Dockerfile. If you are trying to force the inclusion of a later version of an add-on node, you need to treat it like a DockerHub update . Key point: The base image is not affected by this type of update. Note: You can also use this type of build if you get an error after modifying Node-RED's environment: $ cd ~/IOTstack $ docker-compose up --build -d nodered when DockerHub updates ( base and local images) \u00b6 When a newer version of the base image appears on DockerHub, you need to rebuild like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull \u00abcontainer\u00bb $ docker-compose up -d \u00abcontainer\u00bb $ docker system prune $ docker system prune This causes DockerHub to be checked for the later version of the base image, downloading it as needed. Then, the Dockerfile is run to produce a new local image. The Dockerfile run happens even if a new base image was not downloaded in the previous step. deleting unused images \u00b6 As your system evolves and new images come down from DockerHub, you may find that more disk space is being occupied than you expected. Try running: $ docker system prune This recovers anything no longer in use. Sometimes multiple prune commands are needed (eg the first removes an old local image, the second removes the old base image). If you add a container via menu.sh and later remove it (either manually or via menu.sh ), the associated images(s) will probably persist. You can check which images are installed via: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE influxdb latest 1361b14bf545 5 days ago 264MB grafana/grafana latest b9dfd6bb8484 13 days ago 149MB iotstack_nodered latest 21d5a6b7b57b 2 weeks ago 540MB portainer/portainer-ce latest 5526251cc61f 5 weeks ago 163MB eclipse-mosquitto latest 4af162db6b4c 6 weeks ago 8 .65MB nodered/node-red latest fa3bc6f20464 2 months ago 376MB portainer/portainer latest dbf28ba50432 2 months ago 62 .5MB Both \"Portainer CE\" and \"Portainer\" are in that list. Assuming \"Portainer\" is no longer in use, it can be removed by using either its repository name or its Image ID. In other words, the following two commands are synonyms: $ docker rmi portainer/portainer $ docker rmi dbf28ba50432 In general, you can use the repository name to remove an image but the Image ID is sometimes needed. The most common situation where you are likely to need the Image ID is after an image has been updated on DockerHub and pulled down to your Raspberry Pi. You will find two containers with the same name. One will be tagged \"latest\" (the running version) while the other will be tagged \"\\<none>\" (the prior version). You use the Image ID to resolve the ambiguity. pinning to specific versions \u00b6 See container image updates to understand how to tell the difference between images that are used \"as is\" from DockerHub versus those that are built from local Dockerfiles. Note: You should always visit an image's DockerHub page before pinning to a specific version. This is the only way to be certain that you are choosing the appropriate version suffix. To pin an image to a specific version: If the image comes straight from DockerHub, you apply the pin in docker-compose.yml . For example, to pin Grafana to version 7.5.7, you change: grafana : container_name : grafana image : grafana/grafana:latest \u2026 to: grafana : container_name : grafana image : grafana/grafana:7.5.7 \u2026 To apply the change, \"up\" the container: $ cd ~/IOTstack $ docker-compose up -d grafana If the image is built using a local Dockerfile, you apply the pin in the Dockerfile. For example, to pin Mosquitto to version 1.6.15, edit ~/IOTstack/.templates/mosquitto/Dockerfile to change: # Download base image FROM eclipse-mosquitto:latest \u2026 to: # Download base image FROM eclipse-mosquitto:1.6.15 \u2026 To apply the change, \"up\" the container and pass the --build flag: $ cd ~/IOTstack $ docker-compose up -d --build mosquitto the nuclear option - use with caution \u00b6 If you create a mess and can't see how to recover, try proceeding like this: $ cd ~/IOTstack $ docker-compose down $ cd $ mv IOTstack IOTstack.old $ git clone https://github.com/SensorsIot/IOTstack.git IOTstack In words: Be in the right directory. Take the stack down. The cd command without any arguments changes your working directory to your home directory (variously known as ~ or $HOME or /home/pi ). Move your existing IOTstack directory out of the way. If you get a permissions problem: Re-try the command with sudo ; and Read a word about the sudo command . Needing sudo in this situation is an example of over-using sudo . Check out a clean copy of IOTstack. Now, you have a clean slate. You can either start afresh by running the menu: $ cd ~/IOTstack $ ./menu.sh Alternatively, you can mix and match by making selective copies from the old directory. For example: $ cd $ cp IOTstack.old/docker-compose.yml IOTstack/ The IOTstack.old directory remains available as a reference for as long as you need it. Once you have no further use for it, you can clean it up via: $ cd $ sudo rm -rf ./IOTstack.old The sudo command is needed in this situation because some files and folders (eg the \"volumes\" directory and most of its contents) are owned by root.","title":"Getting Started"},{"location":"Basic_setup/#getting-started","text":"","title":"Getting Started"},{"location":"Basic_setup/#introduction-to-iotstack-videos","text":"Andreas Spiess Video #295: Raspberry Pi Server based on Docker, with VPN, Dropbox backup, Influx, Grafana, etc: IOTstack Andreas Spiess Video #352: Raspberry Pi4 Home Automation Server (incl. Docker, OpenHAB, HASSIO, NextCloud)","title":"introduction to IOTstack - videos"},{"location":"Basic_setup/#assumptions","text":"IOTstack makes the following assumptions: Your hardware is a Raspberry Pi (typically a 3B+ or 4B). Note: The Raspberry Pi Zero W2 has been tested with IOTstack. It works but the 512MB RAM means you should not try to run too many containers concurrently. Your Raspberry Pi has a reasonably-recent version of 32-bit Raspberry Pi OS (aka \"Raspbian\") installed. You can download operating-system images: Current release Prior releases Note: If you use the first link, \"Raspberry Pi OS with desktop\" is recommended. The second link only offers \"Raspberry Pi OS with desktop\" images. Your operating system has been kept up-to-date with: $ sudo apt update $ sudo apt upgrade -y You are logged-in as the user \"pi\". User \"pi\" has the user ID 1000. The home directory for user \"pi\" is /home/pi/ . IOTstack is installed at /home/pi/IOTstack (with that exact spelling). If the first three assumptions hold, assumptions four through six are Raspberry Pi defaults on a clean installation. The seventh is what you get if you follow these instructions faithfully. Please don't read these assumptions as saying that IOTstack will not run on other hardware, other operating systems, or as a different user. It is just that IOTstack gets most of its testing under these conditions. The further you get from these implicit assumptions, the more your mileage may vary.","title":"assumptions"},{"location":"Basic_setup/#other-platforms","text":"Users have reported success on other platforms, including: Orange Pi WinPlus","title":"other platforms"},{"location":"Basic_setup/#new-installation","text":"","title":"new installation"},{"location":"Basic_setup/#automatic-recommended","text":"Install curl : $ sudo apt install -y curl Run the following command: $ curl -fsSL https://raw.githubusercontent.com/SensorsIot/IOTstack/master/install.sh | bash Run the menu and choose your containers: $ cd ~/IOTstack $ ./menu.sh Bring up your stack: $ cd ~/IOTstack $ docker-compose up -d","title":"automatic (recommended)"},{"location":"Basic_setup/#manual","text":"Install git : $ sudo apt install -y git Clone IOTstack: If you want \"new menu\": $ git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack If you prefer \"old menu\": $ git clone -b old-menu https://github.com/SensorsIot/IOTstack.git ~/IOTstack Run the menu and choose your containers: $ cd ~/IOTstack $ ./menu.sh Note: If you are running \"old menu\" for the first time, you will be guided to \"Install Docker\". That will end in a reboot, after which you should re-enter the menu and choose your containers. Bring up your stack: $ cd ~/IOTstack $ docker-compose up -d","title":"manual"},{"location":"Basic_setup/#scripted","text":"If you prefer to automate your installations using scripts, see: Installing Docker for IOTstack .","title":"scripted"},{"location":"Basic_setup/#migrating-from-the-old-repo-gcgarner","text":"If you are still running on gcgarner/IOTstack and need to migrate to SensorsIot/IOTstack, see: Migrating IOTstack from gcgarner to SensorsIot .","title":"migrating from the old repo (gcgarner)?"},{"location":"Basic_setup/#recommended-system-patches","text":"","title":"recommended system patches"},{"location":"Basic_setup/#patch-1-restrict-dhcp","text":"Run the following commands: $ sudo bash -c '[ $(egrep -c \"^allowinterfaces eth\\*,wlan\\*\" /etc/dhcpcd.conf) -eq 0 ] && echo \"allowinterfaces eth*,wlan*\" >> /etc/dhcpcd.conf' $ sudo reboot See Issue 219 and Issue 253 for more information.","title":"patch 1 \u2013 restrict DHCP"},{"location":"Basic_setup/#patch-2-update-libseccomp2","text":"This patch is ONLY for Raspbian Buster. Do NOT install this patch if you are running Raspbian Bullseye.","title":"patch 2 \u2013 update libseccomp2"},{"location":"Basic_setup/#step-1-check-your-os-release","text":"Run the following command: $ grep \"PRETTY_NAME\" /etc/os-release PRETTY_NAME = \"Raspbian GNU/Linux 10 (buster)\" If you see the word \"buster\", proceed to step 2. Otherwise, skip this patch.","title":"step 1: check your OS release"},{"location":"Basic_setup/#step-2-if-you-are-running-buster","text":"You need this patch if you are running Raspbian Buster. Without this patch, Docker images will fail if: the image is based on Alpine and the image's maintainer updates to Alpine 3.13 ; and/or an image's maintainer updates to a library that depends on 64-bit values for Unix epoch time (the so-called Y2038 problem). To install the patch: $ sudo apt-key adv --keyserver hkps://keyserver.ubuntu.com:443 --recv-keys 04EE7237B7D453EC 648ACFD622F3D138 $ echo \"deb http://httpredir.debian.org/debian buster-backports main contrib non-free\" | sudo tee -a \"/etc/apt/sources.list.d/debian-backports.list\" $ sudo apt update $ sudo apt install libseccomp2 -t buster-backports","title":"step 2: if you are running \"buster\" \u2026"},{"location":"Basic_setup/#patch-3-kernel-control-groups","text":"Kernel control groups need to be enabled in order to monitor container specific usage. This makes commands like docker stats fully work. Also needed for full monitoring of docker resource usage by the telegraf container. Enable by running (takes effect after reboot): echo $(cat /boot/cmdline.txt) cgroup_memory=1 cgroup_enable=memory | sudo tee /boot/cmdline.txt","title":"patch 3 - kernel control groups"},{"location":"Basic_setup/#a-word-about-the-sudo-command","text":"Many first-time users of IOTstack get into difficulty by misusing the sudo command. The problem is best understood by example. In the following, you would expect ~ (tilde) to expand to /home/pi . It does: $ echo ~/IOTstack /home/pi/IOTstack The command below sends the same echo command to bash for execution. This is what happens when you type the name of a shell script. You get a new instance of bash to run the script: $ bash -c 'echo ~/IOTstack' /home/pi/IOTstack Same answer. Again, this is what you expect. But now try it with sudo on the front: $ sudo bash -c 'echo ~/IOTstack' /root/IOTstack Different answer. It is different because sudo means \"become root, and then run the command\". The process of becoming root changes the home directory, and that changes the definition of ~ . Any script designed for working with IOTstack assumes ~ (or the equivalent $HOME variable) expands to /home/pi . That assumption is invalidated if the script is run by sudo . Of necessity, any script designed for working with IOTstack will have to invoke sudo inside the script when it is required . You do not need to second-guess the script's designer. Please try to minimise your use of sudo when you are working with IOTstack. Here are some rules of thumb: Is what you are about to run a script? If yes, check whether the script already contains sudo commands. Using menu.sh as the example: $ grep -c 'sudo' ~/IOTstack/menu.sh 28 There are numerous uses of sudo within menu.sh . That means the designer thought about when sudo was needed. Did the command you just executed work without sudo ? Note the emphasis on the past tense. If yes, then your work is done. If no, and the error suggests elevated privileges are necessary, then re-execute the last command like this: $ sudo !! It takes time, patience and practice to learn when sudo is actually needed. Over-using sudo out of habit, or because you were following a bad example you found on the web, is a very good way to find that you have created so many problems for yourself that will need to reinstall your IOTstack. Please err on the side of caution!","title":"a word about the sudo command"},{"location":"Basic_setup/#the-iotstack-menu","text":"The menu is used to install Docker and then build the docker-compose.yml file which is necessary for starting the stack. The menu is only an aid. It is a good idea to learn the docker and docker-compose commands if you plan on using Docker in the long run.","title":"the IOTstack menu"},{"location":"Basic_setup/#menu-item-install-docker-old-menu-only","text":"Please do not try to install docker and docker-compose via sudo apt install . There's more to it than that. Docker needs to be installed by menu.sh . The menu will prompt you to install docker if it detects that docker is not already installed. You can manually install it from within the Native Installs menu: $ cd ~/IOTstack $ ./menu.sh Select \"Native Installs\" Select \"Install Docker and Docker-Compose\" Follow the prompts. The process finishes by asking you to reboot. Do that! Note: New menu (master branch) automates this step.","title":"menu item: Install Docker (old menu only)"},{"location":"Basic_setup/#menu-item-build-stack","text":"docker-compose uses a docker-compose.yml file to configure all your services. The docker-compose.yml file is created by the menu: $ cd ~/IOTstack $ ./menu.sh Select \"Build Stack\" Follow the on-screen prompts and select the containers you need. The best advice we can give is \"start small\". Limit yourself to the core containers you actually need (eg Mosquitto, Node-RED, InfluxDB, Grafana, Portainer). You can always add more containers later. Some users have gone overboard with their initial selections and have run into what seem to be Raspberry Pi OS limitations. Key point: If you are running \"new menu\" (master branch) and you select Node-RED, you must press the right-arrow and choose at least one add-on node. If you skip this step, Node-RED will not build properly. Old menu forces you to choose add-on nodes for Node-RED. The process finishes by asking you to bring up the stack: $ cd ~/IOTstack $ docker-compose up -d The first time you run up the stack docker will download all the images from DockerHub. How long this takes will depend on how many containers you selected and the speed of your internet connection. Some containers also need to be built locally. Node-RED is an example. Depending on the Node-RED nodes you select, building the image can also take a very long time. This is especially true if you select the SQLite node. Be patient (and ignore the huge number of warnings).","title":"menu item: Build Stack"},{"location":"Basic_setup/#menu-item-docker-commands","text":"The commands in this menu execute shell scripts in the root of the project.","title":"menu item: Docker commands"},{"location":"Basic_setup/#other-menu-items","text":"The old and new menus differ in the options they offer. You should come back and explore them once your stack is built and running.","title":"other menu items"},{"location":"Basic_setup/#switching-menus","text":"At the time of writing, IOTstack supports three menus: \"Old Menu\" on the old-menu branch. This was inherited from gcgarner/IOTstack . \"New Menu\" on the master branch. This is the current menu. \"New New Menu\" on the experimental branch. This is under development. With a few precautions, you can switch between git branches as much as you like without breaking anything. The basic check you should perform is: $ cd ~/IOTstack $ git status Check the results to see if any files are marked as \"modified\". For example: modified: .templates/mosquitto/Dockerfile Key point: Files marked \"untracked\" do not matter. You only need to check for \"modified\" files because those have the potential to stop you from switching branches cleanly. The way to avoid potential problems is to move any modified files to one side and restore the unmodified original. For example: $ mv .templates/mosquitto/Dockerfile .templates/mosquitto/Dockerfile.save $ git checkout -- .templates/mosquitto/Dockerfile When git status reports no more \"modified\" files, it is safe to switch your branch.","title":"switching menus"},{"location":"Basic_setup/#current-menu-master-branch","text":"$ cd ~/IOTstack/ $ git pull $ git checkout master $ ./menu.sh","title":"current menu (master branch)"},{"location":"Basic_setup/#old-menu-old-menu-branch","text":"$ cd ~/IOTstack/ $ git pull $ git checkout old-menu $ ./menu.sh","title":" old menu (old-menu branch)"},{"location":"Basic_setup/#experimental-branch","text":"Switch to the experimental branch to try the latest and greatest features. $ cd ~/IOTstack/ $ git pull $ git checkout experimental $ ./menu.sh Notes: Please make sure you have a good backup before you start. The experimental branch may be broken, or may break your setup. Please report any issues. Remember: you can switch git branches as much as you like without breaking anything. simply launching the menu (any version) won't change anything providing you exit before letting the menu complete. running the menu to completion will change your docker-compose.yml and supporting structures in ~/IOTstack/services . running docker-compose up -d will change your running containers. The way back is to take down your stack, restore a backup, and bring up your stack again.","title":"experimental branch"},{"location":"Basic_setup/#useful-commands-docker-docker-compose","text":"Handy rules: docker commands can be executed from anywhere, but docker-compose commands need to be executed from within ~/IOTstack","title":"useful commands: docker &amp; docker-compose"},{"location":"Basic_setup/#starting-your-iotstack","text":"To start the stack: $ cd ~/IOTstack $ docker-compose up -d Once the stack has been brought up, it will stay up until you take it down. This includes shutdowns and reboots of your Raspberry Pi. If you do not want the stack to start automatically after a reboot, you need to stop the stack before you issue the reboot command.","title":"starting your IOTstack"},{"location":"Basic_setup/#logging-journald-errors","text":"If you get docker logging error like: Cannot create container for service [service name here]: unknown log opt 'max-file' for journald log driver Run the command: $ sudo nano /etc/docker/daemon.json change: \"log-driver\": \"journald\", to: \"log-driver\": \"json-file\", Logging limits were added to prevent Docker using up lots of RAM if log2ram is enabled, or SD cards being filled with log data and degraded from unnecessary IO. See Docker Logging configurations You can also turn logging off or set it to use another option for any service by using the IOTstack docker-compose-override.yml file mentioned at IOTstack/Custom .","title":"logging journald errors"},{"location":"Basic_setup/#starting-an-individual-container","text":"To start a particular container: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb","title":"starting an individual container"},{"location":"Basic_setup/#stopping-your-iotstack","text":"Stopping aka \"downing\" the stack stops and deletes all containers, and removes the internal network: $ cd ~/IOTstack $ docker-compose down To stop the stack without removing containers, run: $ cd ~/IOTstack $ docker-compose stop","title":"stopping your IOTstack"},{"location":"Basic_setup/#stopping-an-individual-container","text":"stop can also be used to stop individual containers, like this: $ cd ~/IOTstack $ docker-compose stop \u00abcontainer\u00bb This puts the container in a kind of suspended animation. You can resume the container with $ cd ~/IOTstack $ docker-compose start \u00abcontainer\u00bb There is no equivalent of down for a single container. It needs: $ cd ~/IOTstack $ docker-compose rm --force --stop -v \u00abcontainer\u00bb To reactivate a container which has been stopped and removed: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb","title":"stopping an individual container"},{"location":"Basic_setup/#checking-container-status","text":"You can check the status of containers with: $ docker ps or $ cd ~/IOTstack $ docker-compose ps","title":"checking container status"},{"location":"Basic_setup/#viewing-container-logs","text":"You can inspect the logs of most containers like this: $ docker logs \u00abcontainer\u00bb for example: $ docker logs nodered You can also follow a container's log as new entries are added by using the -f flag: $ docker logs -f nodered Terminate with a Control+C. Note that restarting a container will also terminate a followed log.","title":"viewing container logs"},{"location":"Basic_setup/#restarting-a-container","text":"You can restart a container in several ways: $ cd ~/IOTstack $ docker-compose restart \u00abcontainer\u00bb This kind of restart is the least-powerful form of restart. A good way to think of it is \"the container is only restarted, it is not rebuilt\". If you change a docker-compose.yml setting for a container and/or an environment variable file referenced by docker-compose.yml then a restart is usually not enough to bring the change into effect. You need to make docker-compose notice the change: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb This type of \"restart\" rebuilds the container. Alternatively, to force a container to rebuild (without changing either docker-compose.yml or an environment variable file): $ cd ~/IOTstack $ docker-compose up -d --force-recreate \u00abcontainer\u00bb See also updating images built from Dockerfiles if you need to force docker-compose to notice a change to a Dockerfile.","title":"restarting a container"},{"location":"Basic_setup/#persistent-data","text":"Docker allows a container's designer to map folders inside a container to a folder on your disk (SD, SSD, HD). This is done with the \"volumes\" key in docker-compose.yml . Consider the following snippet for Node-RED: volumes : - ./volumes/nodered/data:/data You read this as two paths, separated by a colon. The: external path is ./volumes/nodered/data internal path is /data In this context, the leading \".\" means \"the folder containing docker-compose.yml \", so the external path is actually: ~/IOTstack/volumes/nodered/data If a process running inside the container modifies any file or folder within: /data the change is mirrored outside the container at the same relative path within: ~/IOTstack/volumes/nodered/data The same is true in reverse. Any change made to any file or folder outside the container within: ~/IOTstack/volumes/nodered/data is mirrored at the same relative path inside the container at: /data","title":"persistent data"},{"location":"Basic_setup/#deleting-persistent-data","text":"If you need a \"clean slate\" for a container, you can delete its volumes. Using InfluxDB as an example: $ cd ~/IOTstack $ docker-compose rm --force --stop -v influxdb $ sudo rm -rf ./volumes/influxdb $ docker-compose up -d influxdb When docker-compose tries to bring up InfluxDB, it will notice this volume mapping in docker-compose.yml : volumes : - ./volumes/influxdb/data:/var/lib/influxdb and check to see whether ./volumes/influxdb/data is present. Finding it not there, it does the equivalent of: $ sudo mkdir -p ./volumes/influxdb/data When InfluxDB starts, it sees that the folder on right-hand-side of the volumes mapping ( /var/lib/influxdb ) is empty and initialises new databases. This is how most containers behave. There are exceptions so it's always a good idea to keep a backup.","title":"deleting persistent data"},{"location":"Basic_setup/#stack-maintenance","text":"","title":"stack maintenance"},{"location":"Basic_setup/#update-raspberry-pi-os","text":"You should keep your Raspberry Pi up-to-date. Despite the word \"container\" suggesting that containers are fully self-contained, they sometimes depend on operating system components (\"WireGuard\" is an example). $ sudo apt update $ sudo apt upgrade -y","title":"update Raspberry Pi OS"},{"location":"Basic_setup/#git-pull","text":"Although the menu will generally do this for you, it does not hurt to keep your local copy of the IOTstack repository in sync with the master version on GitHub. $ cd ~/IOTstack $ git pull","title":"git pull"},{"location":"Basic_setup/#container-image-updates","text":"There are two kinds of images used in IOTstack: Those not built using Dockerfiles (the majority) Those built using Dockerfiles (special cases) A Dockerfile is a set of instructions designed to customise an image before it is instantiated to become a running container. The easiest way to work out which type of image you are looking at is to inspect the container's service definition in your docker-compose.yml file. If the service definition contains the: image: keyword then the image is not built using a Dockerfile. build: keyword then the image is built using a Dockerfile.","title":"container image updates"},{"location":"Basic_setup/#updating-images-not-built-from-dockerfiles","text":"If new versions of this type of image become available on DockerHub, your local IOTstack copies can be updated by a pull command: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune The pull downloads any new images. It does this without disrupting the running stack. The up -d notices any newly-downloaded images, builds new containers, and swaps old-for-new. There is barely any downtime for affected containers.","title":"updating images not built from Dockerfiles"},{"location":"Basic_setup/#updating-images-built-from-dockerfiles","text":"Containers built using Dockerfiles have a two-step process: A base image is downloaded from from DockerHub; and then The Dockerfile \"runs\" to build a local image. Node-RED is a good example of a container built from a Dockerfile. The Dockerfile defines some (or possibly all) of your add-on nodes, such as those needed for InfluxDB or Tasmota. There are two separate update situations that you need to consider: If your Dockerfile changes; or If a newer base image appears on DockerHub Node-RED also provides a good example of why your Dockerfile might change: if you decide to add or remove add-on nodes. Note: You can also add nodes to Node-RED using Manage Palette.","title":"updating images built from Dockerfiles"},{"location":"Basic_setup/#when-dockerfile-changes-local-image-only","text":"When your Dockerfile changes, you need to rebuild like this: $ cd ~/IOTstack $ docker-compose up --build -d \u00abcontainer\u00bb $ docker system prune This only rebuilds the local image and, even then, only if docker-compose senses a material change to the Dockerfile. If you are trying to force the inclusion of a later version of an add-on node, you need to treat it like a DockerHub update . Key point: The base image is not affected by this type of update. Note: You can also use this type of build if you get an error after modifying Node-RED's environment: $ cd ~/IOTstack $ docker-compose up --build -d nodered","title":"when Dockerfile changes (local image only)"},{"location":"Basic_setup/#when-dockerhub-updates-base-and-local-images","text":"When a newer version of the base image appears on DockerHub, you need to rebuild like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull \u00abcontainer\u00bb $ docker-compose up -d \u00abcontainer\u00bb $ docker system prune $ docker system prune This causes DockerHub to be checked for the later version of the base image, downloading it as needed. Then, the Dockerfile is run to produce a new local image. The Dockerfile run happens even if a new base image was not downloaded in the previous step.","title":"when DockerHub updates (base and local images)"},{"location":"Basic_setup/#deleting-unused-images","text":"As your system evolves and new images come down from DockerHub, you may find that more disk space is being occupied than you expected. Try running: $ docker system prune This recovers anything no longer in use. Sometimes multiple prune commands are needed (eg the first removes an old local image, the second removes the old base image). If you add a container via menu.sh and later remove it (either manually or via menu.sh ), the associated images(s) will probably persist. You can check which images are installed via: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE influxdb latest 1361b14bf545 5 days ago 264MB grafana/grafana latest b9dfd6bb8484 13 days ago 149MB iotstack_nodered latest 21d5a6b7b57b 2 weeks ago 540MB portainer/portainer-ce latest 5526251cc61f 5 weeks ago 163MB eclipse-mosquitto latest 4af162db6b4c 6 weeks ago 8 .65MB nodered/node-red latest fa3bc6f20464 2 months ago 376MB portainer/portainer latest dbf28ba50432 2 months ago 62 .5MB Both \"Portainer CE\" and \"Portainer\" are in that list. Assuming \"Portainer\" is no longer in use, it can be removed by using either its repository name or its Image ID. In other words, the following two commands are synonyms: $ docker rmi portainer/portainer $ docker rmi dbf28ba50432 In general, you can use the repository name to remove an image but the Image ID is sometimes needed. The most common situation where you are likely to need the Image ID is after an image has been updated on DockerHub and pulled down to your Raspberry Pi. You will find two containers with the same name. One will be tagged \"latest\" (the running version) while the other will be tagged \"\\<none>\" (the prior version). You use the Image ID to resolve the ambiguity.","title":"deleting unused images"},{"location":"Basic_setup/#pinning-to-specific-versions","text":"See container image updates to understand how to tell the difference between images that are used \"as is\" from DockerHub versus those that are built from local Dockerfiles. Note: You should always visit an image's DockerHub page before pinning to a specific version. This is the only way to be certain that you are choosing the appropriate version suffix. To pin an image to a specific version: If the image comes straight from DockerHub, you apply the pin in docker-compose.yml . For example, to pin Grafana to version 7.5.7, you change: grafana : container_name : grafana image : grafana/grafana:latest \u2026 to: grafana : container_name : grafana image : grafana/grafana:7.5.7 \u2026 To apply the change, \"up\" the container: $ cd ~/IOTstack $ docker-compose up -d grafana If the image is built using a local Dockerfile, you apply the pin in the Dockerfile. For example, to pin Mosquitto to version 1.6.15, edit ~/IOTstack/.templates/mosquitto/Dockerfile to change: # Download base image FROM eclipse-mosquitto:latest \u2026 to: # Download base image FROM eclipse-mosquitto:1.6.15 \u2026 To apply the change, \"up\" the container and pass the --build flag: $ cd ~/IOTstack $ docker-compose up -d --build mosquitto","title":"pinning to specific versions"},{"location":"Basic_setup/#the-nuclear-option-use-with-caution","text":"If you create a mess and can't see how to recover, try proceeding like this: $ cd ~/IOTstack $ docker-compose down $ cd $ mv IOTstack IOTstack.old $ git clone https://github.com/SensorsIot/IOTstack.git IOTstack In words: Be in the right directory. Take the stack down. The cd command without any arguments changes your working directory to your home directory (variously known as ~ or $HOME or /home/pi ). Move your existing IOTstack directory out of the way. If you get a permissions problem: Re-try the command with sudo ; and Read a word about the sudo command . Needing sudo in this situation is an example of over-using sudo . Check out a clean copy of IOTstack. Now, you have a clean slate. You can either start afresh by running the menu: $ cd ~/IOTstack $ ./menu.sh Alternatively, you can mix and match by making selective copies from the old directory. For example: $ cd $ cp IOTstack.old/docker-compose.yml IOTstack/ The IOTstack.old directory remains available as a reference for as long as you need it. Once you have no further use for it, you can clean it up via: $ cd $ sudo rm -rf ./IOTstack.old The sudo command is needed in this situation because some files and folders (eg the \"volumes\" directory and most of its contents) are owned by root.","title":"the nuclear option - use with caution"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/","text":"Accessing your device from the internet \u00b6 The challenge most of us face with remotely accessing our home networks is that our routers usually have a dynamically-allocated IP address on the public (WAN) interface. From time to time the IP address that your ISP assigns changes and it's difficult to keep up. Fortunately, there is a solution: Dynamic DNS. The section below shows you how to set up an easy-to-remember domain name that follows your public IP address no matter when it changes. Secondly, how do you get into your home network? Your router has a firewall that is designed to keep the rest of the internet out of your network to protect you. The solution to that is a Virtual Private Network (VPN) or \"tunnel\". Dynamic DNS \u00b6 There are two parts to a Dynamic DNS service: You have to register with a Dynamic DNS service provider and obtain a domain name that is not already taken by someone else. Something on your side of the network needs to propagate updates so that your chosen domain name remains in sync with your router's dynamically-allocated public IP address. Register with a Dynamic DNS service provider \u00b6 The first part is fairly simple and there are quite a few Dynamic DNS service providers including: DuckDNS.org NoIP.com You can find more service providers by Googling \"Dynamic DNS service\" . Some router vendors also provide their own built-in Dynamic DNS capabilities for registered customers so it's a good idea to check your router's capabilities before you plough ahead. Dynamic DNS propagation \u00b6 The \"something\" on your side of the network propagating WAN IP address changes can be either: your router; or a \"behind the router\" technique, typically a periodic job running on the same Raspberry Pi that is hosting IOTstack and WireGuard. If you have the choice, your router is to be preferred. That's because your router is usually the only device in your network that actually knows when its WAN IP address changes. A Dynamic DNS client running on your router will propagate changes immediately and will only transmit updates when necessary. More importantly, it will persist through network interruptions or Dynamic DNS service provider outages until it receives an acknowledgement that the update has been accepted. Nevertheless, your router may not support the Dynamic DNS service provider you wish to use, or may come with constraints that you find unsatisfactory so any behind-the-router technique is always a viable option, providing you understand its limitations. A behind-the-router technique usually relies on sending updates according to a schedule. An example is a cron job that runs every five minutes. That means any router WAN IP address changes won't be propagated until the next scheduled update. In the event of network interruptions or service provider outages, it may take some time before everything is back in sync. Moreover, given that WAN IP address changes are infrequent events, most scheduled updates will be sending information unnecessarily, contributing unnecessarily to server load. This seems to be a problem for DuckDNS which takes a beating because almost every person using it is sending an update bang-on five minutes. DuckDNS client \u00b6 IOTstack provides a solution for DuckDNS. The best approach to running it is: $ mkdir -p ~/.local/bin $ cp ~/IOTstack/duck/duck.sh ~/.local/bin The reason for recommending that you make a copy of duck.sh is because the \"original\" is under Git control. If you change the \"original\", Git will keep telling you that the file has changed and it may block incoming updates from GitHub. Then edit ~/.local/bin/duck.sh to add your DuckDNS domain name(s) and token: DOMAINS = \"YOURS.duckdns.org\" DUCKDNS_TOKEN = \"YOUR_DUCKDNS_TOKEN\" For example: DOMAINS = \"downunda.duckdns.org\" DUCKDNS_TOKEN = \"8a38f294-b5b6-4249-b244-936e997c6c02\" Note: The DOMAINS= variable can be simplified to just \"YOURS\", with the .duckdns.org portion implied, as in: DOMAINS = \"downunda\" Once your credentials are in place, test the result by running: $ ~/.local/bin/duck.sh ddd, dd mmm yyyy hh:mm:ss \u00b1zzzz - updating DuckDNS OK The timestamp is produced by the duck.sh script. The expected responses from the DuckDNS service are: \"OK\" - indicating success; or \"KO\" - indicating failure. Check your work if you get \"KO\" or any other errors. Next, assuming dig is installed on your Raspberry Pi ( sudo apt install dnsutils ), you can test propagation by sending a directed query to a DuckDNS name server. For example, assuming the domain name you registered was downunda.duckdns.org , you would query like this: $ dig @ns1.duckdns.org downunda.duckdns.org +short The expected result is the IP address of your router's WAN interface. It is a good idea to confirm that it is the same as you get from whatismyipaddress.com . A null result indicates failure so check your work. Remember, the Domain Name System is a distributed database. It takes time for changes to propagate. The response you get from directing a query to ns1.duckdns.org may not be the same as the response you get from any other DNS server. You often have to wait until cached records expire and a recursive query reaches the authoritative DuckDNS name-servers. Running the DuckDNS client automatically \u00b6 The recommended arrangement for keeping your Dynamic DNS service up-to-date is to invoke duck.sh from cron at five minute intervals. If you are new to cron , see these guides for more information about setting up and editing your crontab : raspberrytips.com pimylifeup.com A typical crontab will look like this: SHELL = /bin/bash HOME = /home/pi PATH = /home/pi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin */5 * * * * duck.sh >/dev/null 2 > & 1 The first three lines construct the runtime environment correctly and should be at the start of any crontab . The last line means \"run duck.sh every five minutes\". See crontab.guru if you want to understand the syntax of the last line. When launched in the background by cron , the script supplied with IOTstack adds a random delay of up to one minute to try to reduce the \"hammering effect\" of a large number of users updating DuckDNS simultaneously. Standard output and standard error are redirected to /dev/null which is appropriate in this instance. When DuckDNS is working correctly (which is most of the time), the only output from the curl command is \"OK\". Logging that every five minutes would add wear and tear to SD cards for no real benefit. If you suspect DuckDNS is misbehaving, you can run the duck.sh command from a terminal session, in which case you will see all the curl output in the terminal window. If you wish to keep a log of duck.sh activity, the following will get the job done: Make a directory to hold log files: $ mkdir -p ~/Logs Edit the last line of the crontab like this: */5 * * * * duck.sh >>./Logs/duck.log 2 > & 1 Remember to prune the log from time to time. The generally-accepted approach is: $ cat /dev/null >~/Logs/duck.log Virtual Private Network \u00b6 WireGuard \u00b6 WireGuard is supplied as part of IOTstack. See WireGuard documentation . PiVPN \u00b6 pimylifeup.com has an excellent tutorial on how to install PiVPN In point 17 and 18 they mention using noip for their dynamic DNS. Here you can use the DuckDNS address if you created one. Don't forget you need to open the port 1194 on your firewall. Most people won't be able to VPN from inside their network so download OpenVPN client for your mobile phone and try to connect over mobile data. ( More info. ) Once you activate your VPN (from your phone/laptop/work computer) you will effectively be on your home network and you can access your devices as if you were on the wifi at home. I personally use the VPN any time I'm on public wifi, all your traffic is secure. Zerotier \u00b6 https://www.zerotier.com/ Zerotier is an alternative to PiVPN that doesn't require port forwarding on your router. It does however require registering for their free tier service here . Kevin Zhang has written a how to guide here . Just note that the install link is outdated and should be: curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \\ if z = $( curl -s 'https://install.zerotier.com/' | gpg ) ; then echo \" $z \" | sudo bash ; fi","title":"Accessing your device from the internet"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#accessing-your-device-from-the-internet","text":"The challenge most of us face with remotely accessing our home networks is that our routers usually have a dynamically-allocated IP address on the public (WAN) interface. From time to time the IP address that your ISP assigns changes and it's difficult to keep up. Fortunately, there is a solution: Dynamic DNS. The section below shows you how to set up an easy-to-remember domain name that follows your public IP address no matter when it changes. Secondly, how do you get into your home network? Your router has a firewall that is designed to keep the rest of the internet out of your network to protect you. The solution to that is a Virtual Private Network (VPN) or \"tunnel\".","title":"Accessing your device from the internet"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#dynamic-dns","text":"There are two parts to a Dynamic DNS service: You have to register with a Dynamic DNS service provider and obtain a domain name that is not already taken by someone else. Something on your side of the network needs to propagate updates so that your chosen domain name remains in sync with your router's dynamically-allocated public IP address.","title":"Dynamic DNS"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#register-with-a-dynamic-dns-service-provider","text":"The first part is fairly simple and there are quite a few Dynamic DNS service providers including: DuckDNS.org NoIP.com You can find more service providers by Googling \"Dynamic DNS service\" . Some router vendors also provide their own built-in Dynamic DNS capabilities for registered customers so it's a good idea to check your router's capabilities before you plough ahead.","title":"Register with a Dynamic DNS service provider"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#dynamic-dns-propagation","text":"The \"something\" on your side of the network propagating WAN IP address changes can be either: your router; or a \"behind the router\" technique, typically a periodic job running on the same Raspberry Pi that is hosting IOTstack and WireGuard. If you have the choice, your router is to be preferred. That's because your router is usually the only device in your network that actually knows when its WAN IP address changes. A Dynamic DNS client running on your router will propagate changes immediately and will only transmit updates when necessary. More importantly, it will persist through network interruptions or Dynamic DNS service provider outages until it receives an acknowledgement that the update has been accepted. Nevertheless, your router may not support the Dynamic DNS service provider you wish to use, or may come with constraints that you find unsatisfactory so any behind-the-router technique is always a viable option, providing you understand its limitations. A behind-the-router technique usually relies on sending updates according to a schedule. An example is a cron job that runs every five minutes. That means any router WAN IP address changes won't be propagated until the next scheduled update. In the event of network interruptions or service provider outages, it may take some time before everything is back in sync. Moreover, given that WAN IP address changes are infrequent events, most scheduled updates will be sending information unnecessarily, contributing unnecessarily to server load. This seems to be a problem for DuckDNS which takes a beating because almost every person using it is sending an update bang-on five minutes.","title":"Dynamic DNS propagation"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#duckdns-client","text":"IOTstack provides a solution for DuckDNS. The best approach to running it is: $ mkdir -p ~/.local/bin $ cp ~/IOTstack/duck/duck.sh ~/.local/bin The reason for recommending that you make a copy of duck.sh is because the \"original\" is under Git control. If you change the \"original\", Git will keep telling you that the file has changed and it may block incoming updates from GitHub. Then edit ~/.local/bin/duck.sh to add your DuckDNS domain name(s) and token: DOMAINS = \"YOURS.duckdns.org\" DUCKDNS_TOKEN = \"YOUR_DUCKDNS_TOKEN\" For example: DOMAINS = \"downunda.duckdns.org\" DUCKDNS_TOKEN = \"8a38f294-b5b6-4249-b244-936e997c6c02\" Note: The DOMAINS= variable can be simplified to just \"YOURS\", with the .duckdns.org portion implied, as in: DOMAINS = \"downunda\" Once your credentials are in place, test the result by running: $ ~/.local/bin/duck.sh ddd, dd mmm yyyy hh:mm:ss \u00b1zzzz - updating DuckDNS OK The timestamp is produced by the duck.sh script. The expected responses from the DuckDNS service are: \"OK\" - indicating success; or \"KO\" - indicating failure. Check your work if you get \"KO\" or any other errors. Next, assuming dig is installed on your Raspberry Pi ( sudo apt install dnsutils ), you can test propagation by sending a directed query to a DuckDNS name server. For example, assuming the domain name you registered was downunda.duckdns.org , you would query like this: $ dig @ns1.duckdns.org downunda.duckdns.org +short The expected result is the IP address of your router's WAN interface. It is a good idea to confirm that it is the same as you get from whatismyipaddress.com . A null result indicates failure so check your work. Remember, the Domain Name System is a distributed database. It takes time for changes to propagate. The response you get from directing a query to ns1.duckdns.org may not be the same as the response you get from any other DNS server. You often have to wait until cached records expire and a recursive query reaches the authoritative DuckDNS name-servers.","title":"DuckDNS client"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#running-the-duckdns-client-automatically","text":"The recommended arrangement for keeping your Dynamic DNS service up-to-date is to invoke duck.sh from cron at five minute intervals. If you are new to cron , see these guides for more information about setting up and editing your crontab : raspberrytips.com pimylifeup.com A typical crontab will look like this: SHELL = /bin/bash HOME = /home/pi PATH = /home/pi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin */5 * * * * duck.sh >/dev/null 2 > & 1 The first three lines construct the runtime environment correctly and should be at the start of any crontab . The last line means \"run duck.sh every five minutes\". See crontab.guru if you want to understand the syntax of the last line. When launched in the background by cron , the script supplied with IOTstack adds a random delay of up to one minute to try to reduce the \"hammering effect\" of a large number of users updating DuckDNS simultaneously. Standard output and standard error are redirected to /dev/null which is appropriate in this instance. When DuckDNS is working correctly (which is most of the time), the only output from the curl command is \"OK\". Logging that every five minutes would add wear and tear to SD cards for no real benefit. If you suspect DuckDNS is misbehaving, you can run the duck.sh command from a terminal session, in which case you will see all the curl output in the terminal window. If you wish to keep a log of duck.sh activity, the following will get the job done: Make a directory to hold log files: $ mkdir -p ~/Logs Edit the last line of the crontab like this: */5 * * * * duck.sh >>./Logs/duck.log 2 > & 1 Remember to prune the log from time to time. The generally-accepted approach is: $ cat /dev/null >~/Logs/duck.log","title":"Running the DuckDNS client automatically"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#virtual-private-network","text":"","title":"Virtual Private Network"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#wireguard","text":"WireGuard is supplied as part of IOTstack. See WireGuard documentation .","title":"WireGuard"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#pivpn","text":"pimylifeup.com has an excellent tutorial on how to install PiVPN In point 17 and 18 they mention using noip for their dynamic DNS. Here you can use the DuckDNS address if you created one. Don't forget you need to open the port 1194 on your firewall. Most people won't be able to VPN from inside their network so download OpenVPN client for your mobile phone and try to connect over mobile data. ( More info. ) Once you activate your VPN (from your phone/laptop/work computer) you will effectively be on your home network and you can access your devices as if you were on the wifi at home. I personally use the VPN any time I'm on public wifi, all your traffic is secure.","title":"PiVPN"},{"location":"Basic_setup/Accessing-your-Device-from-the-internet/#zerotier","text":"https://www.zerotier.com/ Zerotier is an alternative to PiVPN that doesn't require port forwarding on your router. It does however require registering for their free tier service here . Kevin Zhang has written a how to guide here . Just note that the install link is outdated and should be: curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \\ if z = $( curl -s 'https://install.zerotier.com/' | gpg ) ; then echo \" $z \" | sudo bash ; fi","title":"Zerotier"},{"location":"Basic_setup/Backup-and-Restore/","text":"Backing up and restoring IOTstack \u00b6 This page explains how to use the backup and restore functionality of IOTstack. Backup \u00b6 The backup command can be executed from IOTstack's menu, or from a cronjob. Running backup \u00b6 To ensure that all your data is saved correctly, the stack should be brought down. This is mainly due to databases potentially being in a state that could cause data loss. There are 2 ways to run backups: From the menu: Backup and Restore > Run backup Running the following command: bash ./scripts/backup.sh The command that's run from the command line can also be executed from a cronjob: 0 2 * * * cd /home/pi/IOTstack && /bin/bash ./scripts/backup.sh The current directory of bash must be in IOTstack's directory, to ensure that it can find the relative paths of the files it's meant to back up. In the example above, it's assume that it's inside the pi user's home directory. Arguments \u00b6 ./scripts/backup.sh {TYPE=3} {USER=$(whoami)} Types: 1 = Backup with Date A tarball file will be created that contains the date and time the backup was started, in the filename. 2 = Rolling Date A tarball file will be created that contains the day of the week (0-6) the backup was started, in the filename. If a tarball already exists with the same name, it will be overwritten. 3 = Both User: This parameter only becomes active if run as root. This script will default to the current logged in user If this parameter is not supplied when run as root, the script will ask for the username as input Backups: You can find the backups in the ./backups/ folder. With rolling being in ./backups/rolling/ and date backups in ./backups/backup/ Log files can also be found in the ./backups/logs/ directory. Examples: \u00b6 ./scripts/backup.sh ./scripts/backup.sh 3 Either of these will run both backups. ./scripts/backup.sh 2 This will only produce a backup in the rollowing folder. It will be called 'backup_XX.tar.gz' where XX is the current day of the week (as an int) sudo bash ./scripts/backup.sh 2 pi This will only produce a backup in the rollowing folder and change all the permissions to the 'pi' user. Restore \u00b6 There are 2 ways to run a restore: From the menu: Backup and Restore > Restore from backup Running the following command: bash ./scripts/restore.sh Important : The restore script assumes that the IOTstack directory is fresh, as if it was just cloned. If it is not fresh, errors may occur, or your data may not correctly be restored even if no errors are apparent. Note : It is suggested that you test that your backups can be restored after initially setting up, and anytime you add or remove a service. Major updates to services can also break backups. Arguments \u00b6 ./scripts/restore.sh {FILENAME=backup.tar.gz} {noask} The restore script takes 2 arguments: Filename: The name of the backup file. The file must be present in the ./backups/ directory, or a subfolder in it. That means it should be moved from ./backups/backup to ./backups/ , or that you need to specify the backup portion of the directory (see examples) NoAsk: If a second parameter is present, is acts as setting the no ask flag to true. Pre and post script hooks \u00b6 The script checks if there are any pre and post back up hooks to execute commands. Both of these files will be included in the backup, and have also been added to the .gitignore file, so that they will not be touched when IOTstack updates. Prebackup script hook \u00b6 The prebackup hook script is executed before any compression happens and before anything is written to the temporary backup manifest file ( ./.tmp/backup-list_{{NAME}}.txt ). It can be used to prepare any services (such as databases that IOTstack isn't aware of) for backing up. To use it, simple create a ./pre_backup.sh file in IOTstack's main directory. It will be executed next time a backup runs. Postbackup script hook \u00b6 The postbackup hook script is executed after the tarball file has been written to disk, and before the final backup log information is written to disk. To use it, simple create a ./post_backup.sh file in IOTstack's main directory. It will be executed after the next time a backup runs. Post restore script hook \u00b6 The post restore hook script is executed after all files have been extracted and written to disk. It can be used to apply permissions that your custom services may require. To use it, simple create a ./post_restore.sh file in IOTstack's main directory. It will be executed after a restore happens. Third party integration \u00b6 This section explains how to backup your files with 3rd party software. Dropbox \u00b6 Coming soon. Google Drive \u00b6 Coming soon. rsync \u00b6 Coming soon. Duplicati \u00b6 Coming soon. SFTP \u00b6 Coming soon.","title":"Backing up and restoring IOTstack"},{"location":"Basic_setup/Backup-and-Restore/#backing-up-and-restoring-iotstack","text":"This page explains how to use the backup and restore functionality of IOTstack.","title":"Backing up and restoring IOTstack"},{"location":"Basic_setup/Backup-and-Restore/#backup","text":"The backup command can be executed from IOTstack's menu, or from a cronjob.","title":"Backup"},{"location":"Basic_setup/Backup-and-Restore/#running-backup","text":"To ensure that all your data is saved correctly, the stack should be brought down. This is mainly due to databases potentially being in a state that could cause data loss. There are 2 ways to run backups: From the menu: Backup and Restore > Run backup Running the following command: bash ./scripts/backup.sh The command that's run from the command line can also be executed from a cronjob: 0 2 * * * cd /home/pi/IOTstack && /bin/bash ./scripts/backup.sh The current directory of bash must be in IOTstack's directory, to ensure that it can find the relative paths of the files it's meant to back up. In the example above, it's assume that it's inside the pi user's home directory.","title":"Running backup"},{"location":"Basic_setup/Backup-and-Restore/#arguments","text":"./scripts/backup.sh {TYPE=3} {USER=$(whoami)} Types: 1 = Backup with Date A tarball file will be created that contains the date and time the backup was started, in the filename. 2 = Rolling Date A tarball file will be created that contains the day of the week (0-6) the backup was started, in the filename. If a tarball already exists with the same name, it will be overwritten. 3 = Both User: This parameter only becomes active if run as root. This script will default to the current logged in user If this parameter is not supplied when run as root, the script will ask for the username as input Backups: You can find the backups in the ./backups/ folder. With rolling being in ./backups/rolling/ and date backups in ./backups/backup/ Log files can also be found in the ./backups/logs/ directory.","title":"Arguments"},{"location":"Basic_setup/Backup-and-Restore/#examples","text":"./scripts/backup.sh ./scripts/backup.sh 3 Either of these will run both backups. ./scripts/backup.sh 2 This will only produce a backup in the rollowing folder. It will be called 'backup_XX.tar.gz' where XX is the current day of the week (as an int) sudo bash ./scripts/backup.sh 2 pi This will only produce a backup in the rollowing folder and change all the permissions to the 'pi' user.","title":"Examples:"},{"location":"Basic_setup/Backup-and-Restore/#restore","text":"There are 2 ways to run a restore: From the menu: Backup and Restore > Restore from backup Running the following command: bash ./scripts/restore.sh Important : The restore script assumes that the IOTstack directory is fresh, as if it was just cloned. If it is not fresh, errors may occur, or your data may not correctly be restored even if no errors are apparent. Note : It is suggested that you test that your backups can be restored after initially setting up, and anytime you add or remove a service. Major updates to services can also break backups.","title":"Restore"},{"location":"Basic_setup/Backup-and-Restore/#arguments_1","text":"./scripts/restore.sh {FILENAME=backup.tar.gz} {noask} The restore script takes 2 arguments: Filename: The name of the backup file. The file must be present in the ./backups/ directory, or a subfolder in it. That means it should be moved from ./backups/backup to ./backups/ , or that you need to specify the backup portion of the directory (see examples) NoAsk: If a second parameter is present, is acts as setting the no ask flag to true.","title":"Arguments"},{"location":"Basic_setup/Backup-and-Restore/#pre-and-post-script-hooks","text":"The script checks if there are any pre and post back up hooks to execute commands. Both of these files will be included in the backup, and have also been added to the .gitignore file, so that they will not be touched when IOTstack updates.","title":"Pre and post script hooks"},{"location":"Basic_setup/Backup-and-Restore/#prebackup-script-hook","text":"The prebackup hook script is executed before any compression happens and before anything is written to the temporary backup manifest file ( ./.tmp/backup-list_{{NAME}}.txt ). It can be used to prepare any services (such as databases that IOTstack isn't aware of) for backing up. To use it, simple create a ./pre_backup.sh file in IOTstack's main directory. It will be executed next time a backup runs.","title":"Prebackup script hook"},{"location":"Basic_setup/Backup-and-Restore/#postbackup-script-hook","text":"The postbackup hook script is executed after the tarball file has been written to disk, and before the final backup log information is written to disk. To use it, simple create a ./post_backup.sh file in IOTstack's main directory. It will be executed after the next time a backup runs.","title":"Postbackup script hook"},{"location":"Basic_setup/Backup-and-Restore/#post-restore-script-hook","text":"The post restore hook script is executed after all files have been extracted and written to disk. It can be used to apply permissions that your custom services may require. To use it, simple create a ./post_restore.sh file in IOTstack's main directory. It will be executed after a restore happens.","title":"Post restore script hook"},{"location":"Basic_setup/Backup-and-Restore/#third-party-integration","text":"This section explains how to backup your files with 3rd party software.","title":"Third party integration"},{"location":"Basic_setup/Backup-and-Restore/#dropbox","text":"Coming soon.","title":"Dropbox"},{"location":"Basic_setup/Backup-and-Restore/#google-drive","text":"Coming soon.","title":"Google Drive"},{"location":"Basic_setup/Backup-and-Restore/#rsync","text":"Coming soon.","title":"rsync"},{"location":"Basic_setup/Backup-and-Restore/#duplicati","text":"Coming soon.","title":"Duplicati"},{"location":"Basic_setup/Backup-and-Restore/#sftp","text":"Coming soon.","title":"SFTP"},{"location":"Basic_setup/Custom/","text":"Custom services and overriding default settings for IOTstack \u00b6 You can specify modifcations to the docker-compose.yml file, including your own networks and custom containers/services. Create a file called compose-override.yml in the main directory, and place your modifications into it. These changes will be merged into the docker-compose.yml file next time you run the build script. The compose-override.yml file has been added to the .gitignore file, so it shouldn't be touched when upgrading IOTstack. It has been added to the backup script, and so will be included when you back up and restore IOTstack. Always test your backups though! New versions of IOTstack may break previous builds. How it works \u00b6 After the build process has been completed, a temporary docker compose file is created in the tmp directory. The script then checks if compose-override.yml exists: If it exists, then continue to step 3 If it does not exist, copy the temporary docker compose file to the main directory and rename it to docker-compose.yml . Using the yaml_merge.py script, merge both the compose-override.yml and the temporary docker compose file together; Using the temporary file as the default values and interating through each level of the yaml structure, check to see if the compose-override.yml has a value set. Output the final file to the main directory, calling it docker-compose.yml . A word of caution \u00b6 If you specify an override for a service, and then rebuild the docker-compose.yml file, but deselect the service from the list, then the YAML merging will still produce that override. For example, lets say NodeRed was selected to have have the following override specified in compose-override.yml : services: nodered: restart: always When rebuilding the menu, ensure to have NodeRed service always included because if it's no longer included, the only values showing in the final docker-compose.yml file for NodeRed will be the restart key and its value. Docker Compose will error with the following message: Service nodered has neither an image nor a build context specified. At least one must be provided. When attempting to bring the services up with docker-compose up -d . Either remove the override for NodeRed in compose-override.yml and rebuild the stack, or ensure that NodeRed is built with the stack to fix this. Examples \u00b6 Overriding default settings \u00b6 Lets assume you put the following into the compose-override.yml file: services: mosquitto: ports: - 1996:1996 - 9001:9001 Normally the mosquitto service would be built like this inside the docker-compose.yml file: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1883:1883 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Take special note of the ports list. If you run the build script with the compose-override.yml file in place, and open up the final docker-compose.yml file, you will notice that the port list have been replaced with the ones you specified in the compose-override.yml file. version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Do note that it will replace the entire list, if you were to specify services: mosquitto: ports: - 1996:1996 Then the final output will be: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Using env files instead of docker-compose variables \u00b6 If you need or prefer to use *.env files for docker-compose environment variables in a separate file instead of using overrides, you can do so like this: services: grafana: env_file: - ./services/grafana/grafana.env environment: This will remove the default environment variables set in the template, and tell docker-compose to use the variables specified in your file. It is not mandatory that the .env file be placed in the service's service directory, but is strongly suggested. Keep in mind the PostBuild Script functionality to automatically copy your .env files into their directories on successful build if you need to. Adding custom services \u00b6 Custom services can be added in a similar way to overriding default settings for standard services. Lets add a Minecraft and rcon server to IOTstack. Firstly, put the following into compose-override.yml : services: mosquitto: ports: - 1996:1996 - 9001:9001 minecraft: image: itzg/minecraft-server ports: - \"25565:25565\" volumes: - \"./volumes/minecraft:/data\" environment: EULA: \"TRUE\" TYPE: \"PAPER\" ENABLE_RCON: \"true\" RCON_PASSWORD: \"PASSWORD\" RCON_PORT: 28016 VERSION: \"1.15.2\" REPLACE_ENV_VARIABLES: \"TRUE\" ENV_VARIABLE_PREFIX: \"CFG_\" CFG_DB_HOST: \"http://localhost:3306\" CFG_DB_NAME: \"IOTstack Minecraft\" CFG_DB_PASSWORD_FILE: \"/run/secrets/db_password\" restart: unless-stopped rcon: image: itzg/rcon ports: - \"4326:4326\" - \"4327:4327\" volumes: - \"./volumes/rcon_data:/opt/rcon-web-admin/db\" secrets: db_password: file: ./db_password Then create the service directory that the new instance will use to store persistant data: mkdir -p ./volumes/minecraft and mkdir -p ./volumes/rcon_data Obviously you will need to give correct folder names depending on the volumes you specify for your custom services. If your new service doesn't require persistant storage, then you can skip this step. Then simply run the ./menu.sh command, and rebuild the stack with what ever services you had before. Using the Mosquitto example above, the final docker-compose.yml file will look like: version: '3.6' services: mosquitto: ports: - 1996:1996 - 9001:9001 container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: '1883' volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl minecraft: image: itzg/minecraft-server ports: - 25565:25565 volumes: - ./volumes/minecraft:/data environment: EULA: 'TRUE' TYPE: PAPER ENABLE_RCON: 'true' RCON_PASSWORD: PASSWORD RCON_PORT: 28016 VERSION: 1.15.2 REPLACE_ENV_VARIABLES: 'TRUE' ENV_VARIABLE_PREFIX: CFG_ CFG_DB_HOST: http://localhost:3306 CFG_DB_NAME: IOTstack Minecraft CFG_DB_PASSWORD_FILE: /run/secrets/db_password restart: unless-stopped rcon: image: itzg/rcon ports: - 4326:4326 - 4327:4327 volumes: - ./volumes/rcon_data:/opt/rcon-web-admin/db secrets: db_password: file: ./db_password Do note that the order of the YAML keys is not guaranteed.","title":"Custom services and overriding default settings for IOTstack"},{"location":"Basic_setup/Custom/#custom-services-and-overriding-default-settings-for-iotstack","text":"You can specify modifcations to the docker-compose.yml file, including your own networks and custom containers/services. Create a file called compose-override.yml in the main directory, and place your modifications into it. These changes will be merged into the docker-compose.yml file next time you run the build script. The compose-override.yml file has been added to the .gitignore file, so it shouldn't be touched when upgrading IOTstack. It has been added to the backup script, and so will be included when you back up and restore IOTstack. Always test your backups though! New versions of IOTstack may break previous builds.","title":"Custom services and overriding default settings for IOTstack"},{"location":"Basic_setup/Custom/#how-it-works","text":"After the build process has been completed, a temporary docker compose file is created in the tmp directory. The script then checks if compose-override.yml exists: If it exists, then continue to step 3 If it does not exist, copy the temporary docker compose file to the main directory and rename it to docker-compose.yml . Using the yaml_merge.py script, merge both the compose-override.yml and the temporary docker compose file together; Using the temporary file as the default values and interating through each level of the yaml structure, check to see if the compose-override.yml has a value set. Output the final file to the main directory, calling it docker-compose.yml .","title":"How it works"},{"location":"Basic_setup/Custom/#a-word-of-caution","text":"If you specify an override for a service, and then rebuild the docker-compose.yml file, but deselect the service from the list, then the YAML merging will still produce that override. For example, lets say NodeRed was selected to have have the following override specified in compose-override.yml : services: nodered: restart: always When rebuilding the menu, ensure to have NodeRed service always included because if it's no longer included, the only values showing in the final docker-compose.yml file for NodeRed will be the restart key and its value. Docker Compose will error with the following message: Service nodered has neither an image nor a build context specified. At least one must be provided. When attempting to bring the services up with docker-compose up -d . Either remove the override for NodeRed in compose-override.yml and rebuild the stack, or ensure that NodeRed is built with the stack to fix this.","title":"A word of caution"},{"location":"Basic_setup/Custom/#examples","text":"","title":"Examples"},{"location":"Basic_setup/Custom/#overriding-default-settings","text":"Lets assume you put the following into the compose-override.yml file: services: mosquitto: ports: - 1996:1996 - 9001:9001 Normally the mosquitto service would be built like this inside the docker-compose.yml file: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1883:1883 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Take special note of the ports list. If you run the build script with the compose-override.yml file in place, and open up the final docker-compose.yml file, you will notice that the port list have been replaced with the ones you specified in the compose-override.yml file. version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Do note that it will replace the entire list, if you were to specify services: mosquitto: ports: - 1996:1996 Then the final output will be: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl","title":"Overriding default settings"},{"location":"Basic_setup/Custom/#using-env-files-instead-of-docker-compose-variables","text":"If you need or prefer to use *.env files for docker-compose environment variables in a separate file instead of using overrides, you can do so like this: services: grafana: env_file: - ./services/grafana/grafana.env environment: This will remove the default environment variables set in the template, and tell docker-compose to use the variables specified in your file. It is not mandatory that the .env file be placed in the service's service directory, but is strongly suggested. Keep in mind the PostBuild Script functionality to automatically copy your .env files into their directories on successful build if you need to.","title":"Using env files instead of docker-compose variables"},{"location":"Basic_setup/Custom/#adding-custom-services","text":"Custom services can be added in a similar way to overriding default settings for standard services. Lets add a Minecraft and rcon server to IOTstack. Firstly, put the following into compose-override.yml : services: mosquitto: ports: - 1996:1996 - 9001:9001 minecraft: image: itzg/minecraft-server ports: - \"25565:25565\" volumes: - \"./volumes/minecraft:/data\" environment: EULA: \"TRUE\" TYPE: \"PAPER\" ENABLE_RCON: \"true\" RCON_PASSWORD: \"PASSWORD\" RCON_PORT: 28016 VERSION: \"1.15.2\" REPLACE_ENV_VARIABLES: \"TRUE\" ENV_VARIABLE_PREFIX: \"CFG_\" CFG_DB_HOST: \"http://localhost:3306\" CFG_DB_NAME: \"IOTstack Minecraft\" CFG_DB_PASSWORD_FILE: \"/run/secrets/db_password\" restart: unless-stopped rcon: image: itzg/rcon ports: - \"4326:4326\" - \"4327:4327\" volumes: - \"./volumes/rcon_data:/opt/rcon-web-admin/db\" secrets: db_password: file: ./db_password Then create the service directory that the new instance will use to store persistant data: mkdir -p ./volumes/minecraft and mkdir -p ./volumes/rcon_data Obviously you will need to give correct folder names depending on the volumes you specify for your custom services. If your new service doesn't require persistant storage, then you can skip this step. Then simply run the ./menu.sh command, and rebuild the stack with what ever services you had before. Using the Mosquitto example above, the final docker-compose.yml file will look like: version: '3.6' services: mosquitto: ports: - 1996:1996 - 9001:9001 container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: '1883' volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl minecraft: image: itzg/minecraft-server ports: - 25565:25565 volumes: - ./volumes/minecraft:/data environment: EULA: 'TRUE' TYPE: PAPER ENABLE_RCON: 'true' RCON_PASSWORD: PASSWORD RCON_PORT: 28016 VERSION: 1.15.2 REPLACE_ENV_VARIABLES: 'TRUE' ENV_VARIABLE_PREFIX: CFG_ CFG_DB_HOST: http://localhost:3306 CFG_DB_NAME: IOTstack Minecraft CFG_DB_PASSWORD_FILE: /run/secrets/db_password restart: unless-stopped rcon: image: itzg/rcon ports: - 4326:4326 - 4327:4327 volumes: - ./volumes/rcon_data:/opt/rcon-web-admin/db secrets: db_password: file: ./db_password Do note that the order of the YAML keys is not guaranteed.","title":"Adding custom services"},{"location":"Basic_setup/Default-Configs/","text":"Default Passwords and ports \u00b6 Here you can find a list of the default configurations for IOTstack for quick referece. A word of caution \u00b6 While it is convienent to leave passwords and ports set to their factory value, for security reasons we strongly encourage you to use a randomly generated password for services that require passwords, and/or setup a reverse nginx proxy to require authentication before proxying to services. Only allowing connections originating from LAN or VPN is another way to help secure your services. Security requires a multi-pronged approach. Do note that the ports listed are not all of the ports containers use. They are mearly the WUI ports. List of defaults \u00b6 Service Name Default Username Default Password Default External HTTP/S WUI Port Multiple Passwords adminer none none 9080 No blynk_server none none 8180 No deconz none IOtSt4ckDec0nZ 8090 No diyhue none none 8070 No domoticz none none 8883 No dozzle none none 8889 No esphome admin random 6052 No espruinohub none none none No gitea none none 7920 No grafana none none 3000 No home_assistant none none 8123 No homebridge none none 4040 No influxdb none none none Yes mariadb mariadbuser IOtSt4ckmariaDbPw none Yes mosquitto none none none No motioneye none none 8765 No nextcloud none none 9321 No nodered nodered nodered 1880 No openhab none none 4050 No pihole none IOtSt4ckP1Hol3 8089 No plex none none none No portainer none none 9002 No portainer-ce none none 9001 No postgres postuser IOtSt4ckpostgresDbPw none Yes python none none none No rtl_433 none none none No tasmoadmin none none 8088 No telegraf none none none No timescaledb timescaleuser IOtSt4ckTim3Scale none No transmission none none 9091 No webthingsio_gateway none none 4060 No zigbee2mqtt none none none No zigbee2mqtt_assistant none none none No Note: random - a random password is autogenerated. Check docker-compose.yml for what it actually is.","title":"Default Passwords and ports"},{"location":"Basic_setup/Default-Configs/#default-passwords-and-ports","text":"Here you can find a list of the default configurations for IOTstack for quick referece.","title":"Default Passwords and ports"},{"location":"Basic_setup/Default-Configs/#a-word-of-caution","text":"While it is convienent to leave passwords and ports set to their factory value, for security reasons we strongly encourage you to use a randomly generated password for services that require passwords, and/or setup a reverse nginx proxy to require authentication before proxying to services. Only allowing connections originating from LAN or VPN is another way to help secure your services. Security requires a multi-pronged approach. Do note that the ports listed are not all of the ports containers use. They are mearly the WUI ports.","title":"A word of caution"},{"location":"Basic_setup/Default-Configs/#list-of-defaults","text":"Service Name Default Username Default Password Default External HTTP/S WUI Port Multiple Passwords adminer none none 9080 No blynk_server none none 8180 No deconz none IOtSt4ckDec0nZ 8090 No diyhue none none 8070 No domoticz none none 8883 No dozzle none none 8889 No esphome admin random 6052 No espruinohub none none none No gitea none none 7920 No grafana none none 3000 No home_assistant none none 8123 No homebridge none none 4040 No influxdb none none none Yes mariadb mariadbuser IOtSt4ckmariaDbPw none Yes mosquitto none none none No motioneye none none 8765 No nextcloud none none 9321 No nodered nodered nodered 1880 No openhab none none 4050 No pihole none IOtSt4ckP1Hol3 8089 No plex none none none No portainer none none 9002 No portainer-ce none none 9001 No postgres postuser IOtSt4ckpostgresDbPw none Yes python none none none No rtl_433 none none none No tasmoadmin none none 8088 No telegraf none none none No timescaledb timescaleuser IOtSt4ckTim3Scale none No transmission none none 9091 No webthingsio_gateway none none 4060 No zigbee2mqtt none none none No zigbee2mqtt_assistant none none none No Note: random - a random password is autogenerated. Check docker-compose.yml for what it actually is.","title":"List of defaults"},{"location":"Basic_setup/Docker-commands/","text":"Docker commands \u00b6 Aliases \u00b6 I've added bash aliases for stopping and starting the stack. They can be installed in the docker commands menu. These commands no longer need to be executed from the IOTstack directory and can be executed in any directory alias iotstack_up = \"docker-compose -f ~/IOTstack/docker-compose.yml up -d\" alias iotstack_down = \"docker-compose -f ~/IOTstack/docker-compose.yml down\" alias iotstack_start = \"docker-compose -f ~/IOTstack/docker-compose.yml start\" alias iotstack_stop = \"docker-compose -f ~/IOTstack/docker-compose.yml stop\" alias iotstack_update = \"docker-compose -f ~/IOTstack/docker-compose.yml pull\" alias iotstack_build = \"docker-compose -f ~/IOTstack/docker-compose.yml build\" You can now type iotstack_up , they even accept additional parameters iotstack_stop portainer","title":"Docker commands"},{"location":"Basic_setup/Docker-commands/#docker-commands","text":"","title":"Docker commands"},{"location":"Basic_setup/Docker-commands/#aliases","text":"I've added bash aliases for stopping and starting the stack. They can be installed in the docker commands menu. These commands no longer need to be executed from the IOTstack directory and can be executed in any directory alias iotstack_up = \"docker-compose -f ~/IOTstack/docker-compose.yml up -d\" alias iotstack_down = \"docker-compose -f ~/IOTstack/docker-compose.yml down\" alias iotstack_start = \"docker-compose -f ~/IOTstack/docker-compose.yml start\" alias iotstack_stop = \"docker-compose -f ~/IOTstack/docker-compose.yml stop\" alias iotstack_update = \"docker-compose -f ~/IOTstack/docker-compose.yml pull\" alias iotstack_build = \"docker-compose -f ~/IOTstack/docker-compose.yml build\" You can now type iotstack_up , they even accept additional parameters iotstack_stop portainer","title":"Aliases"},{"location":"Basic_setup/How-the-script-works/","text":"How the script works \u00b6 The build script creates the ./services directory and populates it from the template file in .templates . The script then appends the text withing each service.yml file to the docker-compose.yml . When the stack is rebuilt the menu does not overwrite the service folder if it already exists. Make sure to sync any alterations you have made to the docker-compose.yml file with the respective service.yml so that on your next build your changes pull through. The .gitignore file is setup such that if you do a git pull origin master it does not overwrite the files you have already created. Because the build script does not overwrite your service directory any changes in the .templates directory will have no affect on the services you have already made. You will need to move your service folder out to get the latest version of the template.","title":"How the script works"},{"location":"Basic_setup/How-the-script-works/#how-the-script-works","text":"The build script creates the ./services directory and populates it from the template file in .templates . The script then appends the text withing each service.yml file to the docker-compose.yml . When the stack is rebuilt the menu does not overwrite the service folder if it already exists. Make sure to sync any alterations you have made to the docker-compose.yml file with the respective service.yml so that on your next build your changes pull through. The .gitignore file is setup such that if you do a git pull origin master it does not overwrite the files you have already created. Because the build script does not overwrite your service directory any changes in the .templates directory will have no affect on the services you have already made. You will need to move your service folder out to get the latest version of the template.","title":"How the script works"},{"location":"Basic_setup/Misc/","text":"Miscellaneous \u00b6 log2ram \u00b6 https://github.com/azlux/log2ram One of the drawbacks of an sd card is that it has a limited lifespan. One way to reduce the load on the sd card is to move your log files to RAM. log2ram is a convenient tool to simply set this up. It can be installed from the miscellaneous menu. Dropbox-Uploader \u00b6 This a great utility to easily upload data from your PI to the cloud. https://magpi.raspberrypi.org/articles/dropbox-raspberry-pi The MagPi has an excellent explanation of the process of setting up the Dropbox API. Dropbox-Uploader is used in the backup script.","title":"Miscellaneous"},{"location":"Basic_setup/Misc/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"Basic_setup/Misc/#log2ram","text":"https://github.com/azlux/log2ram One of the drawbacks of an sd card is that it has a limited lifespan. One way to reduce the load on the sd card is to move your log files to RAM. log2ram is a convenient tool to simply set this up. It can be installed from the miscellaneous menu.","title":"log2ram"},{"location":"Basic_setup/Misc/#dropbox-uploader","text":"This a great utility to easily upload data from your PI to the cloud. https://magpi.raspberrypi.org/articles/dropbox-raspberry-pi The MagPi has an excellent explanation of the process of setting up the Dropbox API. Dropbox-Uploader is used in the backup script.","title":"Dropbox-Uploader"},{"location":"Basic_setup/Native-RTL_433/","text":"Native RTL_433 \u00b6 RTL_433 can be installed from the \"Native install sections\" This video demonstrates how to use RTL_433","title":"Native RTL_433"},{"location":"Basic_setup/Native-RTL_433/#native-rtl_433","text":"RTL_433 can be installed from the \"Native install sections\" This video demonstrates how to use RTL_433","title":"Native RTL_433"},{"location":"Basic_setup/Networking/","text":"Networking \u00b6 The docker-compose instruction creates an internal network for the containers to communicate in, the ports get exposed to the PI's IP address when you want to connect from outside. It also creates a \"DNS\" the name being the container name. So it is important to note that when one container talks to another they talk by name. All the containers names are lowercase like nodered, influxdb... An easy way to find out your IP is by typing ip address in the terminal and look next to eth0 or wlan0 for your IP. It is highly recommended that you set a static IP for your PI or at least reserve an IP on your router so that you know it Check the docker-compose.yml to see which ports have been used Examples \u00b6 You want to connect your nodered to your mqtt server. In nodered drop an mqtt node, when you need to specify the address type mosquitto You want to connect to your influxdb from grafana. You are in the Docker network and you need to use the name of the Container. The address you specify in the grafana is http://influxdb:8086 You want to connect to the web interface of grafana from your laptop. Now you are outside the container environment you type PI's IP eg 192.168.n.m:3000 Ports \u00b6 Many containers try to use popular ports such as 80,443,8080. For example openHAB and Adminer both want to use port 8080 for their web interface. Adminer's port has been moved 9080 to accommodate this. Please check the description of the container in the README to see if there are any changes as they may not be the same as the port you are used to. Port mapping is done in the docker-compose.yml file. Each service should have a section that reads like this: ports: - HOST_PORT:CONTAINER_PORT For adminer: ports: - 9080:8080 Port 9080 on Host Pi is mapped to port 8080 of the container. Therefore 127.0.0.1:8080 will take you to openHAB, where 127.0.0.1:9080 will take you to adminer","title":"Networking"},{"location":"Basic_setup/Networking/#networking","text":"The docker-compose instruction creates an internal network for the containers to communicate in, the ports get exposed to the PI's IP address when you want to connect from outside. It also creates a \"DNS\" the name being the container name. So it is important to note that when one container talks to another they talk by name. All the containers names are lowercase like nodered, influxdb... An easy way to find out your IP is by typing ip address in the terminal and look next to eth0 or wlan0 for your IP. It is highly recommended that you set a static IP for your PI or at least reserve an IP on your router so that you know it Check the docker-compose.yml to see which ports have been used","title":"Networking"},{"location":"Basic_setup/Networking/#examples","text":"You want to connect your nodered to your mqtt server. In nodered drop an mqtt node, when you need to specify the address type mosquitto You want to connect to your influxdb from grafana. You are in the Docker network and you need to use the name of the Container. The address you specify in the grafana is http://influxdb:8086 You want to connect to the web interface of grafana from your laptop. Now you are outside the container environment you type PI's IP eg 192.168.n.m:3000","title":"Examples"},{"location":"Basic_setup/Networking/#ports","text":"Many containers try to use popular ports such as 80,443,8080. For example openHAB and Adminer both want to use port 8080 for their web interface. Adminer's port has been moved 9080 to accommodate this. Please check the description of the container in the README to see if there are any changes as they may not be the same as the port you are used to. Port mapping is done in the docker-compose.yml file. Each service should have a section that reads like this: ports: - HOST_PORT:CONTAINER_PORT For adminer: ports: - 9080:8080 Port 9080 on Host Pi is mapped to port 8080 of the container. Therefore 127.0.0.1:8080 will take you to openHAB, where 127.0.0.1:9080 will take you to adminer","title":"Ports"},{"location":"Basic_setup/RPIEasy_native/","text":"RPIEasy \u00b6 RPIEasy can now be installed under the native menu The installer will install any dependencies. If ~/rpieasy exists it will update the project to its latest, if not it will clone the project Running Running RPIEasy \u00b6 RPIEasy can be run by sudo ~/rpieasy/RPIEasy.py To have RPIEasy start on boot in the webui under hardware look for \"RPIEasy autostart at boot\" Ports \u00b6 RPIEasy will select its ports from the first available one in the list (80,8080,8008). If you run Hass.io then there will be a conflict so check the next available port","title":"RPIEasy"},{"location":"Basic_setup/RPIEasy_native/#rpieasy","text":"RPIEasy can now be installed under the native menu The installer will install any dependencies. If ~/rpieasy exists it will update the project to its latest, if not it will clone the project","title":"RPIEasy"},{"location":"Basic_setup/RPIEasy_native/#running-running-rpieasy","text":"RPIEasy can be run by sudo ~/rpieasy/RPIEasy.py To have RPIEasy start on boot in the webui under hardware look for \"RPIEasy autostart at boot\"","title":"Running Running RPIEasy"},{"location":"Basic_setup/RPIEasy_native/#ports","text":"RPIEasy will select its ports from the first available one in the list (80,8080,8008). If you run Hass.io then there will be a conflict so check the next available port","title":"Ports"},{"location":"Basic_setup/Understanding-Containers/","text":"What is Docker? \u00b6 In simple terms, Docker is a software platform that simplifies the process of building, running, managing and distributing applications. It does this by virtualizing the operating system of the computer on which it is installed and running. The Problem \u00b6 Let\u2019s say you have three different Python-based applications that you plan to host on a single server (which could either be a physical or a virtual machine). Each of these applications makes use of a different version of Python, as well as the associated libraries and dependencies, differ from one application to another. Since we cannot have different versions of Python installed on the same machine, this prevents us from hosting all three applications on the same computer. The Solution \u00b6 Let\u2019s look at how we could solve this problem without making use of Docker. In such a scenario, we could solve this problem either by having three physical machines, or a single physical machine, which is powerful enough to host and run three virtual machines on it. Both the options would allow us to install different versions of Python on each of these machines, along with their associated dependencies. The machine on which Docker is installed and running is usually referred to as a Docker Host or Host in simple terms. So, whenever you plan to deploy an application on the host, it would create a logical entity on it to host that application. In Docker terminology, we call this logical entity a Container or Docker Container to be more precise. Whereas the kernel of the host\u2019s operating system is shared across all the containers that are running on it. This allows each container to be isolated from the other present on the same host. Thus it supports multiple containers with different application requirements and dependencies to run on the same host, as long as they have the same operating system requirements. Docker Terminology \u00b6 Docker Images and Docker Containers are the two essential things that you will come across daily while working with Docker. In simple terms, a Docker Image is a template that contains the application, and all the dependencies required to run that application on Docker. On the other hand, as stated earlier, a Docker Container is a logical entity. In more precise terms, it is a running instance of the Docker Image. What is Docker-Compose? \u00b6 Docker Compose provides a way to orchestrate multiple containers that work together. Docker compose is a simple yet powerful tool that is used to run multiple containers as a single service. For example, suppose you have an application which requires Mqtt as a communication service between IOT devices and OpenHAB instance as a Smarthome application service. In this case by docker-compose, you can create one single file (docker-compose.yml) which will create both the containers as a single service without starting each separately. It wires up the networks (literally), mounts all volumes and exposes the ports. The IOTstack with the templates and menu is a generator for that docker-compose service descriptor. How Docker Compose Works? \u00b6 use yaml files to configure application services (docker-compose.yaml) can start all the services with a single command ( docker-compose up ) can stop all the service with a single command ( docker-compose down ) How are the containers connected \u00b6 The containers are automagically connected when we run the stack with docker-compose up. The containers using same logical network (by default) where the instances can access each other with the instance logical name. Means if there is an instance called mosquitto and an openhab , when openHAB instance need to access mqtt on that case the domain name of mosquitto will be resolved as the runnuning instance of mosquitto. How the container are connected to host machine \u00b6 Volumes \u00b6 The containers are enclosed processes which state are lost with the restart of container. To be able to persist states volumes (images or directories) can be used to share data with the host. Which means if you need to persist some database, configuration or any state you have to bind volumes where the running service inside the container will write files to that binded volume. In order to understand what a Docker volume is, we first need to be clear about how the filesystem normally works in Docker. Docker images are stored as series of read-only layers. When we start a container, Docker takes the read-only image and adds a read-write layer on top. If the running container modifies an existing file, the file is copied out of the underlying read-only layer and into the top-most read-write layer where the changes are applied. The version in the read-write layer hides the underlying file, but does not destroy it -- it still exists in the underlying layer. When a Docker container is deleted, relaunching the image will start a fresh container without any of the changes made in the previously running container -- those changes are lost, thats the reason that configs, databases are not persisted, Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. In IOTstack project uses the volumes directory in general to bind these container volumes. Ports \u00b6 When containers running a we would like to delegate some services to the outside world, for example OpenHAB web frontend have to be accessible for users. There are several ways to achive that. One is mounting the port to the most machine, this called port binding. On that case service will have a dedicated port which can be accessed, one drawback is one host port can be used one serice only. Another way is reverse proxy. The term reverse proxy (or Load Balancer in some terminology) is normally applied to a service that sits in front of one or more servers (in our case containers), accepting requests from clients for resources located on the server(s). From the client point of view, the reverse proxy appears to be the web server and so is totally transparent to the remote user. Which means several service can share same port the server will route the request by the URL (virtual domain or context path). For example, there is grafana and openHAB instances, where the opeanhab.domain.tld request will be routed to openHAB instance 8181 port while grafana.domain.tld to grafana instance 3000 port. On that case the proxy have to be mapped for host port 80 and/or 444 on host machine, the proxy server will access the containers via the docker virtual network. Source materials used: https://takacsmark.com/docker-compose-tutorial-beginners-by-example-basics/ https://www.freecodecamp.org/news/docker-simplified-96639a35ff36/ https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/ https://blog.container-solutions.com/understanding-volumes-docker","title":"What is Docker?"},{"location":"Basic_setup/Understanding-Containers/#what-is-docker","text":"In simple terms, Docker is a software platform that simplifies the process of building, running, managing and distributing applications. It does this by virtualizing the operating system of the computer on which it is installed and running.","title":"What is Docker?"},{"location":"Basic_setup/Understanding-Containers/#the-problem","text":"Let\u2019s say you have three different Python-based applications that you plan to host on a single server (which could either be a physical or a virtual machine). Each of these applications makes use of a different version of Python, as well as the associated libraries and dependencies, differ from one application to another. Since we cannot have different versions of Python installed on the same machine, this prevents us from hosting all three applications on the same computer.","title":"The Problem"},{"location":"Basic_setup/Understanding-Containers/#the-solution","text":"Let\u2019s look at how we could solve this problem without making use of Docker. In such a scenario, we could solve this problem either by having three physical machines, or a single physical machine, which is powerful enough to host and run three virtual machines on it. Both the options would allow us to install different versions of Python on each of these machines, along with their associated dependencies. The machine on which Docker is installed and running is usually referred to as a Docker Host or Host in simple terms. So, whenever you plan to deploy an application on the host, it would create a logical entity on it to host that application. In Docker terminology, we call this logical entity a Container or Docker Container to be more precise. Whereas the kernel of the host\u2019s operating system is shared across all the containers that are running on it. This allows each container to be isolated from the other present on the same host. Thus it supports multiple containers with different application requirements and dependencies to run on the same host, as long as they have the same operating system requirements.","title":"The Solution"},{"location":"Basic_setup/Understanding-Containers/#docker-terminology","text":"Docker Images and Docker Containers are the two essential things that you will come across daily while working with Docker. In simple terms, a Docker Image is a template that contains the application, and all the dependencies required to run that application on Docker. On the other hand, as stated earlier, a Docker Container is a logical entity. In more precise terms, it is a running instance of the Docker Image.","title":"Docker Terminology"},{"location":"Basic_setup/Understanding-Containers/#what-is-docker-compose","text":"Docker Compose provides a way to orchestrate multiple containers that work together. Docker compose is a simple yet powerful tool that is used to run multiple containers as a single service. For example, suppose you have an application which requires Mqtt as a communication service between IOT devices and OpenHAB instance as a Smarthome application service. In this case by docker-compose, you can create one single file (docker-compose.yml) which will create both the containers as a single service without starting each separately. It wires up the networks (literally), mounts all volumes and exposes the ports. The IOTstack with the templates and menu is a generator for that docker-compose service descriptor.","title":"What is Docker-Compose?"},{"location":"Basic_setup/Understanding-Containers/#how-docker-compose-works","text":"use yaml files to configure application services (docker-compose.yaml) can start all the services with a single command ( docker-compose up ) can stop all the service with a single command ( docker-compose down )","title":"How Docker Compose Works?"},{"location":"Basic_setup/Understanding-Containers/#how-are-the-containers-connected","text":"The containers are automagically connected when we run the stack with docker-compose up. The containers using same logical network (by default) where the instances can access each other with the instance logical name. Means if there is an instance called mosquitto and an openhab , when openHAB instance need to access mqtt on that case the domain name of mosquitto will be resolved as the runnuning instance of mosquitto.","title":"How are the containers connected"},{"location":"Basic_setup/Understanding-Containers/#how-the-container-are-connected-to-host-machine","text":"","title":"How the container are connected to host machine"},{"location":"Basic_setup/Understanding-Containers/#volumes","text":"The containers are enclosed processes which state are lost with the restart of container. To be able to persist states volumes (images or directories) can be used to share data with the host. Which means if you need to persist some database, configuration or any state you have to bind volumes where the running service inside the container will write files to that binded volume. In order to understand what a Docker volume is, we first need to be clear about how the filesystem normally works in Docker. Docker images are stored as series of read-only layers. When we start a container, Docker takes the read-only image and adds a read-write layer on top. If the running container modifies an existing file, the file is copied out of the underlying read-only layer and into the top-most read-write layer where the changes are applied. The version in the read-write layer hides the underlying file, but does not destroy it -- it still exists in the underlying layer. When a Docker container is deleted, relaunching the image will start a fresh container without any of the changes made in the previously running container -- those changes are lost, thats the reason that configs, databases are not persisted, Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. In IOTstack project uses the volumes directory in general to bind these container volumes.","title":"Volumes"},{"location":"Basic_setup/Understanding-Containers/#ports","text":"When containers running a we would like to delegate some services to the outside world, for example OpenHAB web frontend have to be accessible for users. There are several ways to achive that. One is mounting the port to the most machine, this called port binding. On that case service will have a dedicated port which can be accessed, one drawback is one host port can be used one serice only. Another way is reverse proxy. The term reverse proxy (or Load Balancer in some terminology) is normally applied to a service that sits in front of one or more servers (in our case containers), accepting requests from clients for resources located on the server(s). From the client point of view, the reverse proxy appears to be the web server and so is totally transparent to the remote user. Which means several service can share same port the server will route the request by the URL (virtual domain or context path). For example, there is grafana and openHAB instances, where the opeanhab.domain.tld request will be routed to openHAB instance 8181 port while grafana.domain.tld to grafana instance 3000 port. On that case the proxy have to be mapped for host port 80 and/or 444 on host machine, the proxy server will access the containers via the docker virtual network. Source materials used: https://takacsmark.com/docker-compose-tutorial-beginners-by-example-basics/ https://www.freecodecamp.org/news/docker-simplified-96639a35ff36/ https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/ https://blog.container-solutions.com/understanding-volumes-docker","title":"Ports"},{"location":"Basic_setup/Updates/New-Menu-Release-Notes/","text":"New IOTstack Menu \u00b6 Background \u00b6 Originally this script was written in bash. After a while it became obvious that bash wasn't well suited to dealing with all the different types of configuration files, and logic that goes with configuring everything. IOTstack needs to be accessible to all levels of programmers and tinkerers, not just ones experienced with Linux and bash. For this reason, it was rewritten in Python since the language syntax is easier to understand, and is more commonly used for scripting and programming than bash. Bash is still used in IOTstack where it makes sense to use it, but the menu system itself uses Python. The code is intentionally made so that beginners and experienced programmers could contribute to the project. We are always open to improvements if you have suggestions. On-going improvements \u00b6 There are many features that are needing to be introduced into the new menu system. From meta tags on services for filtering, to optional nginx autoconfiguration and authentication. For this reason you may initially experience bugs (very hard to test every type of configuration!). The new menu system has been worked on and tested for 6 months and we think it's stable enough to merge into the master branch for mainstream usage. The code still needs some work to make it easier to add new services and to not require copy pasting the same code for each new service. Also to make the menu system not be needed at all (so it can be automated with bash scripts). Breaking changes \u00b6 There are a few changes that you need to be aware of: Docker Environmental *.env files are no longer a thing by default. Everything needed is specified in the service.yml file, you can still optionally use them though either with Custom Overrides or with the PostBuild script. Specific config files for certain services still work as they once did. Python 3, pip3, PyYAML and Blessed are all required to be installed. Not backwards compatible with old menu system. You will be able to switch back to the old menu system for a period of time by changing to the old-menu branch. It will be unmaintained except for critical updates. It will eventually be removed - but not before everyone is ready to leave it. Test that your backups are working before you switch. The old-menu branch will become avaiable just before the new menu is merged into master to ensure it has the latest commits applied. Full change list \u00b6 Menu and everything that goes with it rewritten in Python and Blessed Easy installation script All services rewritten to be compatible with PyYAML Optional port selection for services Issue checking for services before building Options for services now in menu (no more editing service.yml files) Automatic password generation for each service Pre and post scripts for customising services Removed env files Backup and restoring more streamlined Documentation updated for all services No longer needs to be installed in the home directory ~ .","title":"New IOTstack Menu"},{"location":"Basic_setup/Updates/New-Menu-Release-Notes/#new-iotstack-menu","text":"","title":"New IOTstack Menu"},{"location":"Basic_setup/Updates/New-Menu-Release-Notes/#background","text":"Originally this script was written in bash. After a while it became obvious that bash wasn't well suited to dealing with all the different types of configuration files, and logic that goes with configuring everything. IOTstack needs to be accessible to all levels of programmers and tinkerers, not just ones experienced with Linux and bash. For this reason, it was rewritten in Python since the language syntax is easier to understand, and is more commonly used for scripting and programming than bash. Bash is still used in IOTstack where it makes sense to use it, but the menu system itself uses Python. The code is intentionally made so that beginners and experienced programmers could contribute to the project. We are always open to improvements if you have suggestions.","title":"Background"},{"location":"Basic_setup/Updates/New-Menu-Release-Notes/#on-going-improvements","text":"There are many features that are needing to be introduced into the new menu system. From meta tags on services for filtering, to optional nginx autoconfiguration and authentication. For this reason you may initially experience bugs (very hard to test every type of configuration!). The new menu system has been worked on and tested for 6 months and we think it's stable enough to merge into the master branch for mainstream usage. The code still needs some work to make it easier to add new services and to not require copy pasting the same code for each new service. Also to make the menu system not be needed at all (so it can be automated with bash scripts).","title":"On-going improvements"},{"location":"Basic_setup/Updates/New-Menu-Release-Notes/#breaking-changes","text":"There are a few changes that you need to be aware of: Docker Environmental *.env files are no longer a thing by default. Everything needed is specified in the service.yml file, you can still optionally use them though either with Custom Overrides or with the PostBuild script. Specific config files for certain services still work as they once did. Python 3, pip3, PyYAML and Blessed are all required to be installed. Not backwards compatible with old menu system. You will be able to switch back to the old menu system for a period of time by changing to the old-menu branch. It will be unmaintained except for critical updates. It will eventually be removed - but not before everyone is ready to leave it. Test that your backups are working before you switch. The old-menu branch will become avaiable just before the new menu is merged into master to ensure it has the latest commits applied.","title":"Breaking changes"},{"location":"Basic_setup/Updates/New-Menu-Release-Notes/#full-change-list","text":"Menu and everything that goes with it rewritten in Python and Blessed Easy installation script All services rewritten to be compatible with PyYAML Optional port selection for services Issue checking for services before building Options for services now in menu (no more editing service.yml files) Automatic password generation for each service Pre and post scripts for customising services Removed env files Backup and restoring more streamlined Documentation updated for all services No longer needs to be installed in the home directory ~ .","title":"Full change list"},{"location":"Basic_setup/Updates/Updating-the-Project/","text":"Updating the project \u00b6 Periodically updates are made to project which include new or modified container template, changes to backups or additional features. As these are released your local copy of this project will become out of date. This section deals with how to bring your project to the latest published state. Quick instructions \u00b6 backup your current settings: cp docker-compose.yml docker-compose.yml.bak check git status for any local changes you may have made to project files. Save and preserve your changes by doing a commit: git commit -a -m \"local customization\" . Or revert them using: git checkout -- path/to/changed_file . update project files from github: git pull origin master -r get latest images from the web: docker-compose pull rebuild localy created images from new Dockerfiles: docker-compose build --pull --no-cache update running containers to latest: docker-compose up --build -d Troubleshooting: if a container fails to restart after update try restarting the whole stack: docker-compose restart backup your stack settings: cp docker-compose.yml docker-compose.yml.bak Check log output of the failing service: docker-compose logs *service-name* try googling and fixing problems in docker-compose.yml manually. try recreating the failing service definition using menu.sh: ./menu.sh , select Build Stack, unselect the failing service, press enter to build, and then exit. ./menu.sh , select Build Stack, select the service back again, press enter to build, and then exit. Try starting now: docker-compose up -d Go to the IOTStack Discord and describe your problem. We're happy to help. Details, partly outdated \u00b6 Warning If you ran git checkout -- 'git ls-files -m' as suggested in the old wiki entry then please check your duck.sh because it removed your domain and token Git offers build in functionality to fetch the latest changes. git pull origin master will fetch the latest changes from GitHub without overwriting files that you have modified yourself. If you have done a local commit then your project may to handle a merge conflict. This can be verified by running git status . You can ignore if it reports duck.sh as being modified. Should you have any modified scripts or templates they can be reset to the latest version with git checkout -- scripts/ .templates/ With the new latest version of the project you can now use the menu to build your stack. If there is a particular container you would like to update its template then you can select that at the overwrite option for your container. You have the choice to not to overwrite, preserve env files or to completely overwrite any changes (passwords) After your stack had been rebuild you can run docker-compose up -d to pull in the latest changes. If you have not update your images in a while consider running the ./scripts/update.sh to get the latest version of the image from Docker hub as well","title":"Updating the project"},{"location":"Basic_setup/Updates/Updating-the-Project/#updating-the-project","text":"Periodically updates are made to project which include new or modified container template, changes to backups or additional features. As these are released your local copy of this project will become out of date. This section deals with how to bring your project to the latest published state.","title":"Updating the project"},{"location":"Basic_setup/Updates/Updating-the-Project/#quick-instructions","text":"backup your current settings: cp docker-compose.yml docker-compose.yml.bak check git status for any local changes you may have made to project files. Save and preserve your changes by doing a commit: git commit -a -m \"local customization\" . Or revert them using: git checkout -- path/to/changed_file . update project files from github: git pull origin master -r get latest images from the web: docker-compose pull rebuild localy created images from new Dockerfiles: docker-compose build --pull --no-cache update running containers to latest: docker-compose up --build -d Troubleshooting: if a container fails to restart after update try restarting the whole stack: docker-compose restart backup your stack settings: cp docker-compose.yml docker-compose.yml.bak Check log output of the failing service: docker-compose logs *service-name* try googling and fixing problems in docker-compose.yml manually. try recreating the failing service definition using menu.sh: ./menu.sh , select Build Stack, unselect the failing service, press enter to build, and then exit. ./menu.sh , select Build Stack, select the service back again, press enter to build, and then exit. Try starting now: docker-compose up -d Go to the IOTStack Discord and describe your problem. We're happy to help.","title":"Quick instructions"},{"location":"Basic_setup/Updates/Updating-the-Project/#details-partly-outdated","text":"Warning If you ran git checkout -- 'git ls-files -m' as suggested in the old wiki entry then please check your duck.sh because it removed your domain and token Git offers build in functionality to fetch the latest changes. git pull origin master will fetch the latest changes from GitHub without overwriting files that you have modified yourself. If you have done a local commit then your project may to handle a merge conflict. This can be verified by running git status . You can ignore if it reports duck.sh as being modified. Should you have any modified scripts or templates they can be reset to the latest version with git checkout -- scripts/ .templates/ With the new latest version of the project you can now use the menu to build your stack. If there is a particular container you would like to update its template then you can select that at the overwrite option for your container. You have the choice to not to overwrite, preserve env files or to completely overwrite any changes (passwords) After your stack had been rebuild you can run docker-compose up -d to pull in the latest changes. If you have not update your images in a while consider running the ./scripts/update.sh to get the latest version of the image from Docker hub as well","title":"Details, partly outdated"},{"location":"Basic_setup/Updates/gcgarner-migration/","text":"Migrating from gcgarner to SensorsIot \u00b6 These instructions explain how to migrate from gcgarner/IOTstack to SensorsIot/IOTstack . Migrating to SensorsIot/IOTstack was fairly easy when this repository was first forked from gcgarner/IOTstack. Unfortunately, what was a fairly simple switching procedure no longer works properly because conflicts have emerged. The probability of conflicts developing increases as a function of time since the fork. Conflicts were and are pretty much inevitable so a more involved procedure is needed. Migration Steps \u00b6 Step 1 \u2013 Check your assumptions \u00b6 Make sure that you are, actually , on gcgarner. Don't assume! $ git remote -v origin https://github.com/gcgarner/IOTstack.git (fetch) origin https://github.com/gcgarner/IOTstack.git (push) Do not proceed if you don't see those URLs! Step 2 \u2013 Take IOTstack down \u00b6 Take your stack down. This is not strictly necessary but we'll be moving the goalposts a bit so it's better to be on the safe side. $ cd ~/IOTstack $ docker-compose down Step 3 \u2013 Choose your migration method \u00b6 There are two basic approaches to switching from gcgarner/IOTstack to SensorsIot/IOTstack: Migration by changing upstream repository Migration by clone and merge You can think of the first as \"working with git\" while the second is \"using brute force\". The first approach will work if you haven't tried any other migration steps and/or have not made too many changes to items in your gcgarner/IOTstack that are under git control. If you are already stuck or you try the first approach and get a mess, or it all looks far too hard to sort out, then try the Migration by clone and merge approach. Migration Option 1 \u2013 change upstream repository \u00b6 Check for local changes \u00b6 Make sure you are on the master branch (you probably are so this is just a precaution), and then see if Git thinks you have made any local changes: $ cd ~/IOTstack $ git checkout master $ git status If Git reports any \"modified\" files, those will probably get in the way of a successful migration so it's a good idea to get those out of the way. For example, suppose you edited menu.sh at some point. Git would report that as: modified: menu.sh The simplest way to deal with modified files is to rename them to move them out of the way, and then restore the original: Rename your customised version by adding your initials to the end of the filename. Later, you can come back and compare your customised version with the version from GitHub and see if you want to preserve any changes. Here I'm assuming your initials are \"jqh\": $ mv menu.sh menu.sh.jqh Tell git to restore the unmodified version: $ git checkout -- menu.sh Now, repeat the Git command that complained about the file: $ git status The modified file will show up as \"untracked\" which is OK (ignore it) Untracked files: (use \"git add <file>...\" to include in what will be committed) menu.sh.jqh Synchronise with gcgarner on GitHub \u00b6 Make sure your local copy of gcgarner is in sync with GitHub. $ git pull Get rid of any upstream reference \u00b6 There may or may not be any \"upstream\" set. The most likely reason for this to happen is if you used your local copy as the basis of a Pull Request. The next command will probably return an error, which you should ignore. It's just a precaution. $ git remote remove upstream Point to SensorsIot \u00b6 Change your local repository to point to SensorsIot. $ git remote set-url origin https://github.com/SensorsIot/IOTstack.git Synchronise with SensorsIot on GitHub \u00b6 This is where things can get a bit tricky so please read these instructions carefully before you proceed. When you run the next command, it will probably give you a small fright by opening a text-editor window. Don't panic - just keep reading. Now, run this command: $ git pull -X theirs origin master The text editor window will look something like this: Merge branch 'master' of https://github.com/SensorsIot/IOTstack # Please enter a commit message to explain why this merge is necessary, # especially if it merges an updated upstream into a topic branch. # # Lines starting with '#' will be ignored, and an empty message aborts # the commit. The first line is a pre-prepared commit message, the remainder is boilerplate instructions which you can ignore. Exactly which text editor opens is a function of your EDITOR environment variable and the core.editor set in your global Git configuration. If you: remember changing EDITOR and/or core.editor then, presumably, you will know how to interact with your chosen text editor. You don't need to make any changes to this file. All you need to do is save the file and exit; don't remember changing either EDITOR or core.editor then the editor will probably be the default vi (aka vim ). You need to type \":wq\" (without the quotes) and then press return. The \":\" puts vi into command mode, the \"w\" says \"save the file\" and \"q\" means \"quit vi \". Pressing return runs the commands. Git will display a long list of stuff. It's very tempting to ignore it but it's a good idea to take a closer look, particularly for signs of error or any lines beginning with: Auto-merging At the time of writing, you can expect Git to mention these two files: Auto-merging menu.sh Auto-merging .templates/zigbee2mqtt/service.yml Those are known issues and the merge strategy -X theirs on the git pull command you have just executed deals with both, correctly, by preferring the SensorsIot version. Similar conflicts may emerge in future and those will probably be dealt with, correctly, by the same merge strategy. Nevertheless, you should still check the output very carefully for other signs of merge conflict so that you can at least be alive to the possibility that the affected files may warrant closer inspection. For example, suppose you saw: Auto-merging .templates/someRandomService/service.yml If you don't use someRandomService then you could safely ignore this on the basis that it was \"probably right\". However, if you did use that service and it started to misbehave after migration, you would know that the service.yml file was a good place to start looking for explanations. Finish with a pull \u00b6 At this point, only the migrated master branch is present on your local copy of the repository. The next command brings you fully in-sync with GitHub: $ git pull Migration Option 2 \u2013 clone and merge \u00b6 If you have been following the process correctly, your IOTstack will already be down. Rename your existing IOTstack folder \u00b6 Move your old IOTstack folder out of the way, like this: $ cd ~ $ mv IOTstack IOTstack.old Note: You should not need sudo for the mv command but it is OK to use it if necessary. Fetch a clean clone of SensorsIot/IOTstack \u00b6 $ git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack Explore the result: $ tree -aFL 1 --noreport ~/IOTstack /home/pi/IOTstack \u251c\u2500\u2500 .bash_aliases \u251c\u2500\u2500 .git/ \u251c\u2500\u2500 .github/ \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 .native/ \u251c\u2500\u2500 .templates/ \u251c\u2500\u2500 .tmp/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docs/ \u251c\u2500\u2500 duck/ \u251c\u2500\u2500 install.sh* \u251c\u2500\u2500 menu.sh* \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 scripts/ Note: If the tree command is not installed for some reason, use ls -A1F ~/IOTstack . Observe what is not there: There is no docker-compose.yml There is no backups directory There is no services directory There is no volumes directory From this, it should be self-evident that a clean checkout from GitHub is the factory for all IOTstack installations, while the contents of backups , services , volumes and docker-compose.yml represent each user's individual choices, configuration options and data. Merge old into new \u00b6 Execute the following commands: $ mv ~/IOTstack.old/docker-compose.yml ~/IOTstack $ mv ~/IOTstack.old/services ~/IOTstack $ sudo mv ~/IOTstack.old/volumes ~/IOTstack You should not need to use sudo for the first two commands. However, if you get a permissions conflict on either, you should proceed like this: docker-compose.yml $ sudo mv ~/IOTstack.old/docker-compose.yml ~/IOTstack $ sudo chown pi:pi ~/IOTstack/docker-compose.yml services $ sudo mv ~/IOTstack.old/services ~/IOTstack $ sudo chown -R pi:pi ~/IOTstack/services There is no need to migrate the backups directory. You are better off creating it by hand: $ mkdir ~/IOTstack/backups Step 4 \u2013 Choose your menu \u00b6 If you have reached this point, you have migrated to SensorsIot/IOTstack where you are on the \"master\" branch. This implies \"new menu\". The choice of menu is entirely up to you. Differences include: New menu takes a lot more screen real-estate than old menu. If you do a fair bit of work on small screens (eg iPad) you might find it hard to work with new menu. New menu creates a large number of internal Docker networks whereas old menu has one internal network to rule them all . The practical consequence is that most users see error messages for networks being defined but not used, and occasionally run into problems where two containers can't talk to each other without tinkering with the networks. Neither of those happen under old menu. See Issue 245 if you want more information on this. New menu has moved the definition of environment variables into docker-compose.yml . Old menu keeps environment variables in \"environment files\" in ~/IOTstack/services . There is no \"right\" or \"better\" about either approach. It's just something to be aware of. Under new menu, the service.yml files in ~/IOTstack/.templates have all been left-shifted by two spaces. That means you can no longer use copy and paste to test containers - you're stuck with the extra work of re-adding the spaces. Again, this doesn't matter but you do need to be aware of it. What you give up when you choose old menu is summarised in the following. If a container appears on the right hand side but not the left then it is only available in new menu. old-menu master (new menu) \u251c\u2500\u2500 adminer \u251c\u2500\u2500 adminer \u251c\u2500\u2500 blynk_server \u251c\u2500\u2500 blynk_server \u251c\u2500\u2500 dashmachine \u251c\u2500\u2500 dashmachine \u251c\u2500\u2500 deconz \u251c\u2500\u2500 deconz \u251c\u2500\u2500 diyhue \u251c\u2500\u2500 diyhue \u251c\u2500\u2500 domoticz \u251c\u2500\u2500 domoticz \u251c\u2500\u2500 dozzle \u251c\u2500\u2500 dozzle \u251c\u2500\u2500 espruinohub \u251c\u2500\u2500 espruinohub > \u251c\u2500\u2500 example_template \u251c\u2500\u2500 gitea \u251c\u2500\u2500 gitea \u251c\u2500\u2500 grafana \u251c\u2500\u2500 grafana \u251c\u2500\u2500 heimdall \u251c\u2500\u2500 heimdall > \u251c\u2500\u2500 home_assistant \u251c\u2500\u2500 homebridge \u251c\u2500\u2500 homebridge \u251c\u2500\u2500 homer \u251c\u2500\u2500 homer \u251c\u2500\u2500 influxdb \u251c\u2500\u2500 influxdb \u251c\u2500\u2500 mariadb \u251c\u2500\u2500 mariadb \u251c\u2500\u2500 mosquitto \u251c\u2500\u2500 mosquitto \u251c\u2500\u2500 motioneye \u251c\u2500\u2500 motioneye \u251c\u2500\u2500 nextcloud \u251c\u2500\u2500 nextcloud \u251c\u2500\u2500 nodered \u251c\u2500\u2500 nodered \u251c\u2500\u2500 openhab \u251c\u2500\u2500 openhab \u251c\u2500\u2500 pihole \u251c\u2500\u2500 pihole \u251c\u2500\u2500 plex \u251c\u2500\u2500 plex \u251c\u2500\u2500 portainer \u251c\u2500\u2500 portainer \u251c\u2500\u2500 portainer_agent \u251c\u2500\u2500 portainer_agent \u251c\u2500\u2500 portainer-ce \u251c\u2500\u2500 portainer-ce \u251c\u2500\u2500 postgres \u251c\u2500\u2500 postgres \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 python \u251c\u2500\u2500 python \u251c\u2500\u2500 qbittorrent \u251c\u2500\u2500 qbittorrent \u251c\u2500\u2500 rtl_433 \u251c\u2500\u2500 rtl_433 \u251c\u2500\u2500 tasmoadmin \u251c\u2500\u2500 tasmoadmin \u251c\u2500\u2500 telegraf \u251c\u2500\u2500 telegraf \u251c\u2500\u2500 timescaledb \u251c\u2500\u2500 timescaledb \u251c\u2500\u2500 transmission \u251c\u2500\u2500 transmission \u251c\u2500\u2500 webthings_gateway \u251c\u2500\u2500 webthings_gateway \u251c\u2500\u2500 wireguard \u251c\u2500\u2500 wireguard \u2514\u2500\u2500 zigbee2mqtt \u251c\u2500\u2500 zigbee2mqtt > \u2514\u2500\u2500 zigbee2mqtt_assistant You also give up the compose-override.yml functionality. On the other hand, Docker has its own docker-compose.override.yml which works with both menus. If you want to switch to the old menu: $ git checkout old-menu Any time you want to switch back to the new menu: $ git checkout master You can switch back and forth as much as you like and as often as you like. It's no harm, no foul. The branch you are on just governs what you see when you run: $ ./menu.sh Although you can freely change branches, it's probably not a good idea to try to mix-and-match your menus. Pick one menu and stick to it. Even so, nothing will change until you run your chosen menu to completion and allow it to generate a new docker-compose.yml . Step 5 \u2013 Bring up your stack \u00b6 Unless you have gotten ahead of yourself and have already run the menu (old or new) then nothing will have changed in the parts of your ~/IOTstack folder that define your IOTstack implementation. You can safely: $ docker-compose up -d See also \u00b6 There is another gist Installing Docker for IOTstack which explains how to overcome problems with outdated Docker and Docker-Compose installations. Depending on the age of your gcgarner installation, you may run into problems which will be cured by working through that gist.","title":"Migrating from gcgarner to SensorsIot"},{"location":"Basic_setup/Updates/gcgarner-migration/#migrating-from-gcgarner-to-sensorsiot","text":"These instructions explain how to migrate from gcgarner/IOTstack to SensorsIot/IOTstack . Migrating to SensorsIot/IOTstack was fairly easy when this repository was first forked from gcgarner/IOTstack. Unfortunately, what was a fairly simple switching procedure no longer works properly because conflicts have emerged. The probability of conflicts developing increases as a function of time since the fork. Conflicts were and are pretty much inevitable so a more involved procedure is needed.","title":"Migrating from gcgarner to SensorsIot"},{"location":"Basic_setup/Updates/gcgarner-migration/#migration-steps","text":"","title":"Migration Steps"},{"location":"Basic_setup/Updates/gcgarner-migration/#step-1-check-your-assumptions","text":"Make sure that you are, actually , on gcgarner. Don't assume! $ git remote -v origin https://github.com/gcgarner/IOTstack.git (fetch) origin https://github.com/gcgarner/IOTstack.git (push) Do not proceed if you don't see those URLs!","title":"Step 1 \u2013 Check your assumptions"},{"location":"Basic_setup/Updates/gcgarner-migration/#step-2-take-iotstack-down","text":"Take your stack down. This is not strictly necessary but we'll be moving the goalposts a bit so it's better to be on the safe side. $ cd ~/IOTstack $ docker-compose down","title":"Step 2 \u2013 Take IOTstack down"},{"location":"Basic_setup/Updates/gcgarner-migration/#step-3-choose-your-migration-method","text":"There are two basic approaches to switching from gcgarner/IOTstack to SensorsIot/IOTstack: Migration by changing upstream repository Migration by clone and merge You can think of the first as \"working with git\" while the second is \"using brute force\". The first approach will work if you haven't tried any other migration steps and/or have not made too many changes to items in your gcgarner/IOTstack that are under git control. If you are already stuck or you try the first approach and get a mess, or it all looks far too hard to sort out, then try the Migration by clone and merge approach.","title":"Step 3 \u2013 Choose your migration method"},{"location":"Basic_setup/Updates/gcgarner-migration/#migration-option-1-change-upstream-repository","text":"","title":"Migration Option 1 \u2013 change upstream repository"},{"location":"Basic_setup/Updates/gcgarner-migration/#check-for-local-changes","text":"Make sure you are on the master branch (you probably are so this is just a precaution), and then see if Git thinks you have made any local changes: $ cd ~/IOTstack $ git checkout master $ git status If Git reports any \"modified\" files, those will probably get in the way of a successful migration so it's a good idea to get those out of the way. For example, suppose you edited menu.sh at some point. Git would report that as: modified: menu.sh The simplest way to deal with modified files is to rename them to move them out of the way, and then restore the original: Rename your customised version by adding your initials to the end of the filename. Later, you can come back and compare your customised version with the version from GitHub and see if you want to preserve any changes. Here I'm assuming your initials are \"jqh\": $ mv menu.sh menu.sh.jqh Tell git to restore the unmodified version: $ git checkout -- menu.sh Now, repeat the Git command that complained about the file: $ git status The modified file will show up as \"untracked\" which is OK (ignore it) Untracked files: (use \"git add <file>...\" to include in what will be committed) menu.sh.jqh","title":"Check for local changes"},{"location":"Basic_setup/Updates/gcgarner-migration/#synchronise-with-gcgarner-on-github","text":"Make sure your local copy of gcgarner is in sync with GitHub. $ git pull","title":"Synchronise with gcgarner on GitHub"},{"location":"Basic_setup/Updates/gcgarner-migration/#get-rid-of-any-upstream-reference","text":"There may or may not be any \"upstream\" set. The most likely reason for this to happen is if you used your local copy as the basis of a Pull Request. The next command will probably return an error, which you should ignore. It's just a precaution. $ git remote remove upstream","title":"Get rid of any upstream reference"},{"location":"Basic_setup/Updates/gcgarner-migration/#point-to-sensorsiot","text":"Change your local repository to point to SensorsIot. $ git remote set-url origin https://github.com/SensorsIot/IOTstack.git","title":"Point to SensorsIot"},{"location":"Basic_setup/Updates/gcgarner-migration/#synchronise-with-sensorsiot-on-github","text":"This is where things can get a bit tricky so please read these instructions carefully before you proceed. When you run the next command, it will probably give you a small fright by opening a text-editor window. Don't panic - just keep reading. Now, run this command: $ git pull -X theirs origin master The text editor window will look something like this: Merge branch 'master' of https://github.com/SensorsIot/IOTstack # Please enter a commit message to explain why this merge is necessary, # especially if it merges an updated upstream into a topic branch. # # Lines starting with '#' will be ignored, and an empty message aborts # the commit. The first line is a pre-prepared commit message, the remainder is boilerplate instructions which you can ignore. Exactly which text editor opens is a function of your EDITOR environment variable and the core.editor set in your global Git configuration. If you: remember changing EDITOR and/or core.editor then, presumably, you will know how to interact with your chosen text editor. You don't need to make any changes to this file. All you need to do is save the file and exit; don't remember changing either EDITOR or core.editor then the editor will probably be the default vi (aka vim ). You need to type \":wq\" (without the quotes) and then press return. The \":\" puts vi into command mode, the \"w\" says \"save the file\" and \"q\" means \"quit vi \". Pressing return runs the commands. Git will display a long list of stuff. It's very tempting to ignore it but it's a good idea to take a closer look, particularly for signs of error or any lines beginning with: Auto-merging At the time of writing, you can expect Git to mention these two files: Auto-merging menu.sh Auto-merging .templates/zigbee2mqtt/service.yml Those are known issues and the merge strategy -X theirs on the git pull command you have just executed deals with both, correctly, by preferring the SensorsIot version. Similar conflicts may emerge in future and those will probably be dealt with, correctly, by the same merge strategy. Nevertheless, you should still check the output very carefully for other signs of merge conflict so that you can at least be alive to the possibility that the affected files may warrant closer inspection. For example, suppose you saw: Auto-merging .templates/someRandomService/service.yml If you don't use someRandomService then you could safely ignore this on the basis that it was \"probably right\". However, if you did use that service and it started to misbehave after migration, you would know that the service.yml file was a good place to start looking for explanations.","title":"Synchronise with SensorsIot on GitHub"},{"location":"Basic_setup/Updates/gcgarner-migration/#finish-with-a-pull","text":"At this point, only the migrated master branch is present on your local copy of the repository. The next command brings you fully in-sync with GitHub: $ git pull","title":"Finish with a pull"},{"location":"Basic_setup/Updates/gcgarner-migration/#migration-option-2-clone-and-merge","text":"If you have been following the process correctly, your IOTstack will already be down.","title":"Migration Option 2 \u2013 clone and merge"},{"location":"Basic_setup/Updates/gcgarner-migration/#rename-your-existing-iotstack-folder","text":"Move your old IOTstack folder out of the way, like this: $ cd ~ $ mv IOTstack IOTstack.old Note: You should not need sudo for the mv command but it is OK to use it if necessary.","title":"Rename your existing IOTstack folder"},{"location":"Basic_setup/Updates/gcgarner-migration/#fetch-a-clean-clone-of-sensorsiotiotstack","text":"$ git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack Explore the result: $ tree -aFL 1 --noreport ~/IOTstack /home/pi/IOTstack \u251c\u2500\u2500 .bash_aliases \u251c\u2500\u2500 .git/ \u251c\u2500\u2500 .github/ \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 .native/ \u251c\u2500\u2500 .templates/ \u251c\u2500\u2500 .tmp/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docs/ \u251c\u2500\u2500 duck/ \u251c\u2500\u2500 install.sh* \u251c\u2500\u2500 menu.sh* \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 scripts/ Note: If the tree command is not installed for some reason, use ls -A1F ~/IOTstack . Observe what is not there: There is no docker-compose.yml There is no backups directory There is no services directory There is no volumes directory From this, it should be self-evident that a clean checkout from GitHub is the factory for all IOTstack installations, while the contents of backups , services , volumes and docker-compose.yml represent each user's individual choices, configuration options and data.","title":"Fetch a clean clone of SensorsIot/IOTstack"},{"location":"Basic_setup/Updates/gcgarner-migration/#merge-old-into-new","text":"Execute the following commands: $ mv ~/IOTstack.old/docker-compose.yml ~/IOTstack $ mv ~/IOTstack.old/services ~/IOTstack $ sudo mv ~/IOTstack.old/volumes ~/IOTstack You should not need to use sudo for the first two commands. However, if you get a permissions conflict on either, you should proceed like this: docker-compose.yml $ sudo mv ~/IOTstack.old/docker-compose.yml ~/IOTstack $ sudo chown pi:pi ~/IOTstack/docker-compose.yml services $ sudo mv ~/IOTstack.old/services ~/IOTstack $ sudo chown -R pi:pi ~/IOTstack/services There is no need to migrate the backups directory. You are better off creating it by hand: $ mkdir ~/IOTstack/backups","title":"Merge old into new"},{"location":"Basic_setup/Updates/gcgarner-migration/#step-4-choose-your-menu","text":"If you have reached this point, you have migrated to SensorsIot/IOTstack where you are on the \"master\" branch. This implies \"new menu\". The choice of menu is entirely up to you. Differences include: New menu takes a lot more screen real-estate than old menu. If you do a fair bit of work on small screens (eg iPad) you might find it hard to work with new menu. New menu creates a large number of internal Docker networks whereas old menu has one internal network to rule them all . The practical consequence is that most users see error messages for networks being defined but not used, and occasionally run into problems where two containers can't talk to each other without tinkering with the networks. Neither of those happen under old menu. See Issue 245 if you want more information on this. New menu has moved the definition of environment variables into docker-compose.yml . Old menu keeps environment variables in \"environment files\" in ~/IOTstack/services . There is no \"right\" or \"better\" about either approach. It's just something to be aware of. Under new menu, the service.yml files in ~/IOTstack/.templates have all been left-shifted by two spaces. That means you can no longer use copy and paste to test containers - you're stuck with the extra work of re-adding the spaces. Again, this doesn't matter but you do need to be aware of it. What you give up when you choose old menu is summarised in the following. If a container appears on the right hand side but not the left then it is only available in new menu. old-menu master (new menu) \u251c\u2500\u2500 adminer \u251c\u2500\u2500 adminer \u251c\u2500\u2500 blynk_server \u251c\u2500\u2500 blynk_server \u251c\u2500\u2500 dashmachine \u251c\u2500\u2500 dashmachine \u251c\u2500\u2500 deconz \u251c\u2500\u2500 deconz \u251c\u2500\u2500 diyhue \u251c\u2500\u2500 diyhue \u251c\u2500\u2500 domoticz \u251c\u2500\u2500 domoticz \u251c\u2500\u2500 dozzle \u251c\u2500\u2500 dozzle \u251c\u2500\u2500 espruinohub \u251c\u2500\u2500 espruinohub > \u251c\u2500\u2500 example_template \u251c\u2500\u2500 gitea \u251c\u2500\u2500 gitea \u251c\u2500\u2500 grafana \u251c\u2500\u2500 grafana \u251c\u2500\u2500 heimdall \u251c\u2500\u2500 heimdall > \u251c\u2500\u2500 home_assistant \u251c\u2500\u2500 homebridge \u251c\u2500\u2500 homebridge \u251c\u2500\u2500 homer \u251c\u2500\u2500 homer \u251c\u2500\u2500 influxdb \u251c\u2500\u2500 influxdb \u251c\u2500\u2500 mariadb \u251c\u2500\u2500 mariadb \u251c\u2500\u2500 mosquitto \u251c\u2500\u2500 mosquitto \u251c\u2500\u2500 motioneye \u251c\u2500\u2500 motioneye \u251c\u2500\u2500 nextcloud \u251c\u2500\u2500 nextcloud \u251c\u2500\u2500 nodered \u251c\u2500\u2500 nodered \u251c\u2500\u2500 openhab \u251c\u2500\u2500 openhab \u251c\u2500\u2500 pihole \u251c\u2500\u2500 pihole \u251c\u2500\u2500 plex \u251c\u2500\u2500 plex \u251c\u2500\u2500 portainer \u251c\u2500\u2500 portainer \u251c\u2500\u2500 portainer_agent \u251c\u2500\u2500 portainer_agent \u251c\u2500\u2500 portainer-ce \u251c\u2500\u2500 portainer-ce \u251c\u2500\u2500 postgres \u251c\u2500\u2500 postgres \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 python \u251c\u2500\u2500 python \u251c\u2500\u2500 qbittorrent \u251c\u2500\u2500 qbittorrent \u251c\u2500\u2500 rtl_433 \u251c\u2500\u2500 rtl_433 \u251c\u2500\u2500 tasmoadmin \u251c\u2500\u2500 tasmoadmin \u251c\u2500\u2500 telegraf \u251c\u2500\u2500 telegraf \u251c\u2500\u2500 timescaledb \u251c\u2500\u2500 timescaledb \u251c\u2500\u2500 transmission \u251c\u2500\u2500 transmission \u251c\u2500\u2500 webthings_gateway \u251c\u2500\u2500 webthings_gateway \u251c\u2500\u2500 wireguard \u251c\u2500\u2500 wireguard \u2514\u2500\u2500 zigbee2mqtt \u251c\u2500\u2500 zigbee2mqtt > \u2514\u2500\u2500 zigbee2mqtt_assistant You also give up the compose-override.yml functionality. On the other hand, Docker has its own docker-compose.override.yml which works with both menus. If you want to switch to the old menu: $ git checkout old-menu Any time you want to switch back to the new menu: $ git checkout master You can switch back and forth as much as you like and as often as you like. It's no harm, no foul. The branch you are on just governs what you see when you run: $ ./menu.sh Although you can freely change branches, it's probably not a good idea to try to mix-and-match your menus. Pick one menu and stick to it. Even so, nothing will change until you run your chosen menu to completion and allow it to generate a new docker-compose.yml .","title":"Step 4 \u2013 Choose your menu"},{"location":"Basic_setup/Updates/gcgarner-migration/#step-5-bring-up-your-stack","text":"Unless you have gotten ahead of yourself and have already run the menu (old or new) then nothing will have changed in the parts of your ~/IOTstack folder that define your IOTstack implementation. You can safely: $ docker-compose up -d","title":"Step 5 \u2013 Bring up your stack"},{"location":"Basic_setup/Updates/gcgarner-migration/#see-also","text":"There is another gist Installing Docker for IOTstack which explains how to overcome problems with outdated Docker and Docker-Compose installations. Depending on the age of your gcgarner installation, you may run into problems which will be cured by working through that gist.","title":"See also"},{"location":"Containers/AdGuardHome/","text":"AdGuard Home \u00b6 References \u00b6 AdGuard Home GitHub AdGuard Home DockerHub Either AdGuard Home or PiHole , but not both \u00b6 AdGuard Home and PiHole perform similar functions. They use the same ports so you can not run both at the same time. You must choose one or the other. Quick Start \u00b6 When you first install AdGuard Home: Use a web browser to connect to it using port 3001. For example: http://raspberrypi.local:3001 Click \"Getting Started\". Change the port number for the Admin Web Interface to be \"8089\". Leave the other settings on the page at their defaults and click \"Next\". Enter a username and password and click \"Next\". Click \"Open Dashboard\". This redirects to port 8089. After the initial setup, you connect to AdGuard Home via port 8089: http://raspberrypi.local:8089 About port 8089 \u00b6 Port 8089 is the default administrative user interface for AdGuard Home running under IOTstack. Port 8089 is not active until you have completed the Quick Start procedure. You must start by connecting to port 3001. Because of AdGuard Home limitations, you must take special precautions if you decide to change to a different port number: The internal and external ports must be the same; and You must convince AdGuard Home that it is a first-time installation: $ cd ~/IOTstack $ docker-compose stop adguardhome $ docker-compose rm -f adguardhome $ sudo rm -rf ./volumes/adguardhome $ docker-compose up -d adguardhome Repeat the Quick Start procedure, this time substituting the new Admin Web Interface port where you see \"8089\". About port 3001:3000 \u00b6 Port 3001 (external, 3000 internal) is only used during Quick Start procedure. Once port 8089 becomes active, port 3001 ceases to be active. In other words, you need to keep port 3001 reserved even though it is only ever used to set up port 8089. About Host Mode \u00b6 If you want to run AdGuard Home as your DHCP server, you need to put the container into \"host mode\". You need edit the AdGuard Home service definition in docker-compose.yml to: add the line: network_mode: host remove the ports: directive and all of the port mappings. Note: It is not really a good idea to offer DHCP services from a container. This is because containers generally start far too late in a boot process to be useful. If you want to use AdGuard Home for DHCP, you should probably consider a native installation.","title":"AdGuard Home"},{"location":"Containers/AdGuardHome/#adguard-home","text":"","title":"AdGuard Home"},{"location":"Containers/AdGuardHome/#references","text":"AdGuard Home GitHub AdGuard Home DockerHub","title":"References"},{"location":"Containers/AdGuardHome/#either-adguard-home-or-pihole-but-not-both","text":"AdGuard Home and PiHole perform similar functions. They use the same ports so you can not run both at the same time. You must choose one or the other.","title":"Either AdGuard Home or PiHole, but not both"},{"location":"Containers/AdGuardHome/#quick-start","text":"When you first install AdGuard Home: Use a web browser to connect to it using port 3001. For example: http://raspberrypi.local:3001 Click \"Getting Started\". Change the port number for the Admin Web Interface to be \"8089\". Leave the other settings on the page at their defaults and click \"Next\". Enter a username and password and click \"Next\". Click \"Open Dashboard\". This redirects to port 8089. After the initial setup, you connect to AdGuard Home via port 8089: http://raspberrypi.local:8089","title":"Quick Start"},{"location":"Containers/AdGuardHome/#about-port-8089","text":"Port 8089 is the default administrative user interface for AdGuard Home running under IOTstack. Port 8089 is not active until you have completed the Quick Start procedure. You must start by connecting to port 3001. Because of AdGuard Home limitations, you must take special precautions if you decide to change to a different port number: The internal and external ports must be the same; and You must convince AdGuard Home that it is a first-time installation: $ cd ~/IOTstack $ docker-compose stop adguardhome $ docker-compose rm -f adguardhome $ sudo rm -rf ./volumes/adguardhome $ docker-compose up -d adguardhome Repeat the Quick Start procedure, this time substituting the new Admin Web Interface port where you see \"8089\".","title":"About port 8089"},{"location":"Containers/AdGuardHome/#about-port-30013000","text":"Port 3001 (external, 3000 internal) is only used during Quick Start procedure. Once port 8089 becomes active, port 3001 ceases to be active. In other words, you need to keep port 3001 reserved even though it is only ever used to set up port 8089.","title":"About port 3001:3000"},{"location":"Containers/AdGuardHome/#about-host-mode","text":"If you want to run AdGuard Home as your DHCP server, you need to put the container into \"host mode\". You need edit the AdGuard Home service definition in docker-compose.yml to: add the line: network_mode: host remove the ports: directive and all of the port mappings. Note: It is not really a good idea to offer DHCP services from a container. This is because containers generally start far too late in a boot process to be useful. If you want to use AdGuard Home for DHCP, you should probably consider a native installation.","title":"About Host Mode"},{"location":"Containers/Adminer/","text":"Adminer \u00b6 References \u00b6 Docker Website About \u00b6 This is a nice tool for managing databases. Web interface has moved to port 9080. There was an issue where openHAB and Adminer were using the same ports. If you have an port conflict edit the docker-compose.yml and under the adminer service change the line to read: ports: - 9080:8080","title":"Adminer"},{"location":"Containers/Adminer/#adminer","text":"","title":"Adminer"},{"location":"Containers/Adminer/#references","text":"Docker Website","title":"References"},{"location":"Containers/Adminer/#about","text":"This is a nice tool for managing databases. Web interface has moved to port 9080. There was an issue where openHAB and Adminer were using the same ports. If you have an port conflict edit the docker-compose.yml and under the adminer service change the line to read: ports: - 9080:8080","title":"About"},{"location":"Containers/Blynk_server/","text":"Blynk server \u00b6 This document discusses an IOTstack-specific version of Blynk-Server. It is built on top of an Ubuntu base image using a Dockerfile . References \u00b6 Ubuntu base image at DockerHub Peter Knight Blynk-Server fork at GitHub (includes documentation) Peter Knight Blynk-Server releases at GitHub Blynk home page at blynk.io Blynk documentation at blynk.io Blynk community forum at community.blynk.cc Interesting post by Peter Knight on MQTT/Node Red flows at community.blynk.cc Blynk flow examples at GitHub Acknowledgement: Original writeup from @877dev Significant directories and files \u00b6 ~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 blynk_server \u2502 \u251c\u2500\u2500 Dockerfile \u2776 \u2502 \u251c\u2500\u2500 docker-entrypoint.sh \u2777 \u2502 \u251c\u2500\u2500 iotstack_defaults \u2778 \u2502 \u2502 \u251c\u2500\u2500 mail.properties \u2502 \u2502 \u2514\u2500\u2500 server.properties \u2502 \u2514\u2500\u2500 service.yml \u2779 \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 blynk_server \u2502 \u2514\u2500\u2500 service.yml \u277a \u251c\u2500\u2500 docker-compose.yml \u277b \u2514\u2500\u2500 volumes \u2514\u2500\u2500 blynk_server \u277c \u251c\u2500\u2500 config \u277d \u2502 \u251c\u2500\u2500 mail.properties \u2502 \u2514\u2500\u2500 server.properties \u2514\u2500\u2500 data The Dockerfile used to construct Blynk Server on top of Ubuntu. A start-up script designed to handle container self-repair. A folder holding the default versions of the configuration files. The template service definition . The working service definition (only relevant to old-menu, copied from \u2779). The Compose file (includes \u2779). The persistent storage area for the blynk_server container. Working copies of the configuration files (copied from \u2778). Everything in \u277d: will be replaced if it is not present when the container starts; but will never be overwritten if altered by you. How Blynk Server gets built for IOTstack \u00b6 GitHub Updates \u00b6 Periodically, the source code is updated and a new version is released. You can check for the latest version at the releases page . IOTstack menu \u00b6 When you select Blynk Server in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used. IOTstack first run \u00b6 On a first install of IOTstack, you run the menu, choose your containers, and are told to do this: $ cd ~/IOTstack $ docker-compose up -d docker-compose reads the Compose file. When it arrives at the blynk_server fragment, it finds: blynk_server: build: context: ./.templates/blynk_server/. args: - BLYNK_SERVER_VERSION=0.41.16 The build statement tells docker-compose to look for: ~/IOTstack/.templates/blynk_server/Dockerfile The BLYNK_SERVER_VERSION argument is passed into the build process. This implicitly pins each build to the version number in the Compose file (eg 0.41.16). If you need to update to a The Dockerfile is in the .templates directory because it is intended to be a common build for all IOTstack users. This is different to the arrangement for Node-RED where the Dockerfile is in the services directory because it is how each individual IOTstack user's version of Node-RED is customised. The Dockerfile begins with: FROM ubuntu The FROM statement tells the build process to pull down the base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The customisations are: Add packages to satisfy dependencies. Add the default versions of the configuration files so that the container can perform self-repair each time it is launched. Download an install the Java package that implements the Blynk Server. The local image is instantiated to become your running container. When you run the docker images command after Blynk Server has been built, you may see two rows that are relevant: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_blynk_server latest 3cd6445f8a7e 3 hours ago 652MB ubuntu latest 897590a6c564 7 days ago 49 .8MB ubuntu is the base image ; and iotstack_blynk_server is the local image . You may see the same pattern in Portainer , which reports the base image as \"unused\". You should not remove the base image, even though it appears to be unused. Whether you see one or two rows depends on the version of docker-compose you are using and how your version of docker-compose builds local images. Logging \u00b6 You can inspect Blynk Server's log by: $ docker logs blynk_server Changing Blynk Server's configuration \u00b6 The first time you launch the blynk_server container, the following structure will be created in the persistent storage area: ~/IOTstack/volumes/blynk_server \u251c\u2500\u2500 [drwxr-xr-x pi ] config \u2502 \u251c\u2500\u2500 [-rw-r--r-- pi ] mail.properties \u2502 \u2514\u2500\u2500 [-rw-r--r-- pi ] server.properties \u2514\u2500\u2500 [drwxr-xr-x root ] data The two .properties files can be used to alter Blynk Server's configuration. When you make change to these files, you activate then by restarting the container: $ cd ~/IOTstack $ docker-compose restart blynk_server Getting a clean slate \u00b6 Erasing Blynk Server's persistent storage area triggers self-healing and restores known defaults: $ cd ~/IOTstack $ docker-compose rm --force --stop -v blynk_server $ sudo rm -rf ./volumes/blynk_server $ docker-compose up -d blynk_server Note: You can also remove individual configuration files and then trigger self-healing. For example, if you decide to edit server.properties and make a mess, you can restore the original default version like this: $ cd ~/IOTstack $ rm volumes/blynk_server/config/server.properties $ docker-compose restart blynk_server Upgrading Blynk Server \u00b6 To find out when a new version has been released, you need to visit the Blynk-Server releases page at GitHub. At the time of writing, version 0.41.16 was the most up-to-date. Suppose that version 0.41.17 has been released and that you decide to upgrade: Edit your Compose file to change the version nuumber: blynk_server: build: context: ./.templates/blynk_server/. args: - BLYNK_SERVER_VERSION=0.41.17 Note: You can use this method to pin Blynk Server to any available version. You then have two options: If you only want to reconstruct the local image: $ cd ~/IOTstack $ docker-compose up --build -d blynk_server $ docker system prune -f If you want to update the Ubuntu base image at the same time: $ cd ~/IOTstack $ docker-compose build --no-cache --pull blynk_server $ docker-compose up -d blynk_server $ docker system prune -f $ docker system prune -f The second prune will only be needed if there is an old base image and that, in turn, depends on the version of docker-compose you are using and how your version of docker-compose builds local images. Using Blynk Server \u00b6 See the References for documentation links. Connecting to the administrative UI \u00b6 To connect to the administrative interface, navigate to: https://<your pis IP>:9443/admin You may encounter browser security warnings which you will have to acknowledge in order to be able to connect to the page. The default credentials are: username = admin@blynk.cc password = admin Change username and password \u00b6 Click on Users > \"email address\" and edit email, name and password. Save changes. Restart the container using either Portainer or the command line: $ cd ~/IOTstack $ docker-compose restart blynk_server Setup gmail \u00b6 Optional step, useful for getting the auth token emailed to you. (To be added once confirmed working....) iOS/Android app setup \u00b6 When setting up the application on your mobile be sure to select \"custom\" setup see . Press \"New Project\" Give it a name, choose device \"Raspberry Pi 3 B\" so you have plenty of virtual pins available, and lastly select WiFi. Create project and the auth token will be emailed to you (if emails configured). You can also find the token in app under the phone app settings, or in the admin web interface by clicking Users>\"email address\" and scroll down to token. Quick usage guide for app \u00b6 Press on the empty page, the widgets will appear from the right. Select your widget, let's say a button. It appears on the page, press on it to configure. Give it a name and colour if you want. Press on PIN, and select virtual. Choose any pin i.e. V0 Press ok. To start the project running, press top right Play button. You will get an offline message, because no devices are connected to your project via the token. Enter Node-Red..... Node-RED \u00b6 Install node-red-contrib-blynk-ws from Manage Palette. Drag a \"write event\" node into your flow, and connect to a debug node Configure the Blynk node for the first time: URL: wss://youripaddress:9443/websockets There is more information here . 4. Enter your auth token from before and save/exit. 5. When you deploy the flow, notice the app shows connected message, as does the Blynk node. 6. Press the button on the app, you will notice the payload is sent to the debug node.","title":"Blynk server"},{"location":"Containers/Blynk_server/#blynk-server","text":"This document discusses an IOTstack-specific version of Blynk-Server. It is built on top of an Ubuntu base image using a Dockerfile .","title":"Blynk server"},{"location":"Containers/Blynk_server/#references","text":"Ubuntu base image at DockerHub Peter Knight Blynk-Server fork at GitHub (includes documentation) Peter Knight Blynk-Server releases at GitHub Blynk home page at blynk.io Blynk documentation at blynk.io Blynk community forum at community.blynk.cc Interesting post by Peter Knight on MQTT/Node Red flows at community.blynk.cc Blynk flow examples at GitHub Acknowledgement: Original writeup from @877dev","title":"References"},{"location":"Containers/Blynk_server/#significant-directories-and-files","text":"~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 blynk_server \u2502 \u251c\u2500\u2500 Dockerfile \u2776 \u2502 \u251c\u2500\u2500 docker-entrypoint.sh \u2777 \u2502 \u251c\u2500\u2500 iotstack_defaults \u2778 \u2502 \u2502 \u251c\u2500\u2500 mail.properties \u2502 \u2502 \u2514\u2500\u2500 server.properties \u2502 \u2514\u2500\u2500 service.yml \u2779 \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 blynk_server \u2502 \u2514\u2500\u2500 service.yml \u277a \u251c\u2500\u2500 docker-compose.yml \u277b \u2514\u2500\u2500 volumes \u2514\u2500\u2500 blynk_server \u277c \u251c\u2500\u2500 config \u277d \u2502 \u251c\u2500\u2500 mail.properties \u2502 \u2514\u2500\u2500 server.properties \u2514\u2500\u2500 data The Dockerfile used to construct Blynk Server on top of Ubuntu. A start-up script designed to handle container self-repair. A folder holding the default versions of the configuration files. The template service definition . The working service definition (only relevant to old-menu, copied from \u2779). The Compose file (includes \u2779). The persistent storage area for the blynk_server container. Working copies of the configuration files (copied from \u2778). Everything in \u277d: will be replaced if it is not present when the container starts; but will never be overwritten if altered by you.","title":"Significant directories and files"},{"location":"Containers/Blynk_server/#how-blynk-server-gets-built-for-iotstack","text":"","title":"How Blynk Server gets built for IOTstack"},{"location":"Containers/Blynk_server/#github-updates","text":"Periodically, the source code is updated and a new version is released. You can check for the latest version at the releases page .","title":"GitHub Updates"},{"location":"Containers/Blynk_server/#iotstack-menu","text":"When you select Blynk Server in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used.","title":"IOTstack menu"},{"location":"Containers/Blynk_server/#iotstack-first-run","text":"On a first install of IOTstack, you run the menu, choose your containers, and are told to do this: $ cd ~/IOTstack $ docker-compose up -d docker-compose reads the Compose file. When it arrives at the blynk_server fragment, it finds: blynk_server: build: context: ./.templates/blynk_server/. args: - BLYNK_SERVER_VERSION=0.41.16 The build statement tells docker-compose to look for: ~/IOTstack/.templates/blynk_server/Dockerfile The BLYNK_SERVER_VERSION argument is passed into the build process. This implicitly pins each build to the version number in the Compose file (eg 0.41.16). If you need to update to a The Dockerfile is in the .templates directory because it is intended to be a common build for all IOTstack users. This is different to the arrangement for Node-RED where the Dockerfile is in the services directory because it is how each individual IOTstack user's version of Node-RED is customised. The Dockerfile begins with: FROM ubuntu The FROM statement tells the build process to pull down the base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The customisations are: Add packages to satisfy dependencies. Add the default versions of the configuration files so that the container can perform self-repair each time it is launched. Download an install the Java package that implements the Blynk Server. The local image is instantiated to become your running container. When you run the docker images command after Blynk Server has been built, you may see two rows that are relevant: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_blynk_server latest 3cd6445f8a7e 3 hours ago 652MB ubuntu latest 897590a6c564 7 days ago 49 .8MB ubuntu is the base image ; and iotstack_blynk_server is the local image . You may see the same pattern in Portainer , which reports the base image as \"unused\". You should not remove the base image, even though it appears to be unused. Whether you see one or two rows depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"IOTstack first run"},{"location":"Containers/Blynk_server/#logging","text":"You can inspect Blynk Server's log by: $ docker logs blynk_server","title":"Logging"},{"location":"Containers/Blynk_server/#changing-blynk-servers-configuration","text":"The first time you launch the blynk_server container, the following structure will be created in the persistent storage area: ~/IOTstack/volumes/blynk_server \u251c\u2500\u2500 [drwxr-xr-x pi ] config \u2502 \u251c\u2500\u2500 [-rw-r--r-- pi ] mail.properties \u2502 \u2514\u2500\u2500 [-rw-r--r-- pi ] server.properties \u2514\u2500\u2500 [drwxr-xr-x root ] data The two .properties files can be used to alter Blynk Server's configuration. When you make change to these files, you activate then by restarting the container: $ cd ~/IOTstack $ docker-compose restart blynk_server","title":"Changing Blynk Server's configuration"},{"location":"Containers/Blynk_server/#getting-a-clean-slate","text":"Erasing Blynk Server's persistent storage area triggers self-healing and restores known defaults: $ cd ~/IOTstack $ docker-compose rm --force --stop -v blynk_server $ sudo rm -rf ./volumes/blynk_server $ docker-compose up -d blynk_server Note: You can also remove individual configuration files and then trigger self-healing. For example, if you decide to edit server.properties and make a mess, you can restore the original default version like this: $ cd ~/IOTstack $ rm volumes/blynk_server/config/server.properties $ docker-compose restart blynk_server","title":"Getting a clean slate"},{"location":"Containers/Blynk_server/#upgrading-blynk-server","text":"To find out when a new version has been released, you need to visit the Blynk-Server releases page at GitHub. At the time of writing, version 0.41.16 was the most up-to-date. Suppose that version 0.41.17 has been released and that you decide to upgrade: Edit your Compose file to change the version nuumber: blynk_server: build: context: ./.templates/blynk_server/. args: - BLYNK_SERVER_VERSION=0.41.17 Note: You can use this method to pin Blynk Server to any available version. You then have two options: If you only want to reconstruct the local image: $ cd ~/IOTstack $ docker-compose up --build -d blynk_server $ docker system prune -f If you want to update the Ubuntu base image at the same time: $ cd ~/IOTstack $ docker-compose build --no-cache --pull blynk_server $ docker-compose up -d blynk_server $ docker system prune -f $ docker system prune -f The second prune will only be needed if there is an old base image and that, in turn, depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"Upgrading Blynk Server"},{"location":"Containers/Blynk_server/#using-blynk-server","text":"See the References for documentation links.","title":"Using Blynk Server"},{"location":"Containers/Blynk_server/#connecting-to-the-administrative-ui","text":"To connect to the administrative interface, navigate to: https://<your pis IP>:9443/admin You may encounter browser security warnings which you will have to acknowledge in order to be able to connect to the page. The default credentials are: username = admin@blynk.cc password = admin","title":"Connecting to the administrative UI"},{"location":"Containers/Blynk_server/#change-username-and-password","text":"Click on Users > \"email address\" and edit email, name and password. Save changes. Restart the container using either Portainer or the command line: $ cd ~/IOTstack $ docker-compose restart blynk_server","title":"Change username and password"},{"location":"Containers/Blynk_server/#setup-gmail","text":"Optional step, useful for getting the auth token emailed to you. (To be added once confirmed working....)","title":"Setup gmail"},{"location":"Containers/Blynk_server/#iosandroid-app-setup","text":"When setting up the application on your mobile be sure to select \"custom\" setup see . Press \"New Project\" Give it a name, choose device \"Raspberry Pi 3 B\" so you have plenty of virtual pins available, and lastly select WiFi. Create project and the auth token will be emailed to you (if emails configured). You can also find the token in app under the phone app settings, or in the admin web interface by clicking Users>\"email address\" and scroll down to token.","title":"iOS/Android app setup"},{"location":"Containers/Blynk_server/#quick-usage-guide-for-app","text":"Press on the empty page, the widgets will appear from the right. Select your widget, let's say a button. It appears on the page, press on it to configure. Give it a name and colour if you want. Press on PIN, and select virtual. Choose any pin i.e. V0 Press ok. To start the project running, press top right Play button. You will get an offline message, because no devices are connected to your project via the token. Enter Node-Red.....","title":"Quick usage guide for app"},{"location":"Containers/Blynk_server/#node-red","text":"Install node-red-contrib-blynk-ws from Manage Palette. Drag a \"write event\" node into your flow, and connect to a debug node Configure the Blynk node for the first time: URL: wss://youripaddress:9443/websockets There is more information here . 4. Enter your auth token from before and save/exit. 5. When you deploy the flow, notice the app shows connected message, as does the Blynk node. 6. Press the button on the app, you will notice the payload is sent to the debug node.","title":"Node-RED"},{"location":"Containers/Chronograf/","text":"Chronograf \u00b6 References \u00b6 influxdata Chronograf documentation GitHub : influxdata/influxdata-docker/chronograf DockerHub : influxdata Chronograf Kapacitor integration \u00b6 If you selected Kapacitor in the menu and want Chronograf to be able to interact with it, you need to edit docker-compose.yml to un-comment the lines which are commented-out in the following: chronograf : \u2026 environment : \u2026 # - KAPACITOR_URL=http://kapacitor:9092 depends_on : \u2026 # - kapacitor If the Chronograf container is already running when you make this change, run: $ cd ~IOTstack $ docker-compose up -d chronograf Upgrading Chronograf \u00b6 You can update the container via: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. Chronograf version pinning \u00b6 If you need to pin to a particular version: Use your favourite text editor to open docker-compose.yml . Find the line: image: chronograf:latest Replace latest with the version you wish to pin to. For example, to pin to version 1.9.0: image: chronograf:1.9.0 Save the file and tell docker-compose to bring up the container: $ cd ~/IOTstack $ docker-compose up -d chronograf $ docker system prune","title":"Chronograf"},{"location":"Containers/Chronograf/#chronograf","text":"","title":"Chronograf"},{"location":"Containers/Chronograf/#references","text":"influxdata Chronograf documentation GitHub : influxdata/influxdata-docker/chronograf DockerHub : influxdata Chronograf","title":"References"},{"location":"Containers/Chronograf/#kapacitor-integration","text":"If you selected Kapacitor in the menu and want Chronograf to be able to interact with it, you need to edit docker-compose.yml to un-comment the lines which are commented-out in the following: chronograf : \u2026 environment : \u2026 # - KAPACITOR_URL=http://kapacitor:9092 depends_on : \u2026 # - kapacitor If the Chronograf container is already running when you make this change, run: $ cd ~IOTstack $ docker-compose up -d chronograf","title":"Kapacitor integration"},{"location":"Containers/Chronograf/#upgrading-chronograf","text":"You can update the container via: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images.","title":"Upgrading Chronograf"},{"location":"Containers/Chronograf/#chronograf-version-pinning","text":"If you need to pin to a particular version: Use your favourite text editor to open docker-compose.yml . Find the line: image: chronograf:latest Replace latest with the version you wish to pin to. For example, to pin to version 1.9.0: image: chronograf:1.9.0 Save the file and tell docker-compose to bring up the container: $ cd ~/IOTstack $ docker-compose up -d chronograf $ docker system prune","title":"Chronograf version pinning"},{"location":"Containers/DashMachine/","text":"DashMachine \u00b6 References \u00b6 Homepage Docker Web Interface \u00b6 The web UI can be found on \"your_ip\":5000 . The default credentials are: * User: admin * Password: admin About DashMachine \u00b6 DashMachine is a web application bookmark dashboard. It allows you to have all your application bookmarks available in one place, grouped and organized how you want to see them. Within the context of IOTstack, DashMachine can help you organize your deployed services.","title":"DashMachine"},{"location":"Containers/DashMachine/#dashmachine","text":"","title":"DashMachine"},{"location":"Containers/DashMachine/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/DashMachine/#web-interface","text":"The web UI can be found on \"your_ip\":5000 . The default credentials are: * User: admin * Password: admin","title":"Web Interface"},{"location":"Containers/DashMachine/#about-dashmachine","text":"DashMachine is a web application bookmark dashboard. It allows you to have all your application bookmarks available in one place, grouped and organized how you want to see them. Within the context of IOTstack, DashMachine can help you organize your deployed services.","title":"About DashMachine"},{"location":"Containers/Deconz/","text":"deCONZ \u00b6 References \u00b6 Docker Website Troubleshooting \u00b6 Make sure your Conbee/Conbee II/RaspBee gateway is connected. If your gateway is not detected, or no lights can be paired, try moving the device to another usb port, reboot your computer and build the stack from the menu again cd ~/IOTstack && bash ./menu.sh (select \"Pull full service from template\" if prompted). The gateway must be plugged in when the deCONZ Docker container is being built. Before running docker-compose up -d , make sure your Linux user is part of the dialout group, which allows the user access to serial devices (i.e. Conbee/Conbee II/RaspBee). If you are not certain, simply add your user to the dialout group by running the following command (username \"pi\" being used as an example): sudo usermod -a -G dialout pi Now run docker-compose up -d to build the stack. If you are still experiencing issues, run docker-compose down to remove all containers from the stack and then docker-compose up -d to build them again. Use a 0.5-1m usb extension cable with ConBee (II) to avoid wifi and bluetooth noise/interference from your Raspberry Pi (recommended by the manufacturer and often the solution to poor performance). Accessing the Phoscon UI \u00b6 The Phoscon UI is available using port 8090 (http://your.local.ip.address:8090/) Viewing the deCONZ Zigbee mesh \u00b6 The Zigbee mesh can be viewed using VNC on port 5901. The default VNC password is \"changeme\". Connecting deCONZ and Node-RED \u00b6 Install node-red-contrib-deconz via the \"Manage palette\" menu in Node-RED (if not already installed) and follow these 2 simple steps (also shown in the video below): Step 1: In the Phoscon UI, Go to Settings > Gateway > Advanced and click \"Authenticate app\". Step 2: In Node-RED, open a deCONZ node, select \"Add new deonz-server\", insert your ip adress and port 8090 and click \"Get settings\". Click \"Add\", \"Done\" and \"Deploy\". Your device list will not be updated before deploying.","title":"deCONZ"},{"location":"Containers/Deconz/#deconz","text":"","title":"deCONZ"},{"location":"Containers/Deconz/#references","text":"Docker Website","title":"References"},{"location":"Containers/Deconz/#troubleshooting","text":"Make sure your Conbee/Conbee II/RaspBee gateway is connected. If your gateway is not detected, or no lights can be paired, try moving the device to another usb port, reboot your computer and build the stack from the menu again cd ~/IOTstack && bash ./menu.sh (select \"Pull full service from template\" if prompted). The gateway must be plugged in when the deCONZ Docker container is being built. Before running docker-compose up -d , make sure your Linux user is part of the dialout group, which allows the user access to serial devices (i.e. Conbee/Conbee II/RaspBee). If you are not certain, simply add your user to the dialout group by running the following command (username \"pi\" being used as an example): sudo usermod -a -G dialout pi Now run docker-compose up -d to build the stack. If you are still experiencing issues, run docker-compose down to remove all containers from the stack and then docker-compose up -d to build them again. Use a 0.5-1m usb extension cable with ConBee (II) to avoid wifi and bluetooth noise/interference from your Raspberry Pi (recommended by the manufacturer and often the solution to poor performance).","title":"Troubleshooting"},{"location":"Containers/Deconz/#accessing-the-phoscon-ui","text":"The Phoscon UI is available using port 8090 (http://your.local.ip.address:8090/)","title":"Accessing the Phoscon UI"},{"location":"Containers/Deconz/#viewing-the-deconz-zigbee-mesh","text":"The Zigbee mesh can be viewed using VNC on port 5901. The default VNC password is \"changeme\".","title":"Viewing the deCONZ Zigbee mesh"},{"location":"Containers/Deconz/#connecting-deconz-and-node-red","text":"Install node-red-contrib-deconz via the \"Manage palette\" menu in Node-RED (if not already installed) and follow these 2 simple steps (also shown in the video below): Step 1: In the Phoscon UI, Go to Settings > Gateway > Advanced and click \"Authenticate app\". Step 2: In Node-RED, open a deCONZ node, select \"Add new deonz-server\", insert your ip adress and port 8090 and click \"Get settings\". Click \"Add\", \"Done\" and \"Deploy\". Your device list will not be updated before deploying.","title":"Connecting deCONZ and Node-RED"},{"location":"Containers/DiyHue/","text":"DIY hue \u00b6 website About \u00b6 diyHue is a utility to contol the lights in your home Setup \u00b6 Before you start diyHue you will need to get your IP and MAC addresses. Run ip addr in the terminal Enter these values into the ./services/diyhue/diyhue.env file The default username and password it Hue and Hue respectively Usage \u00b6 The web interface is available on port 8070","title":"DIY hue"},{"location":"Containers/DiyHue/#diy-hue","text":"website","title":"DIY hue"},{"location":"Containers/DiyHue/#about","text":"diyHue is a utility to contol the lights in your home","title":"About"},{"location":"Containers/DiyHue/#setup","text":"Before you start diyHue you will need to get your IP and MAC addresses. Run ip addr in the terminal Enter these values into the ./services/diyhue/diyhue.env file The default username and password it Hue and Hue respectively","title":"Setup"},{"location":"Containers/DiyHue/#usage","text":"The web interface is available on port 8070","title":"Usage"},{"location":"Containers/ESPHome/","text":"ESPHome \u00b6 ESPHome is a system to control your ESP8266/ESP32 by simple yet powerful configuration files and control them remotely through Home Automation systems. Web UI is at: http://raspberrypi.local:6052/ Login username is admin and the password is automatically generated and can be found and changed in your docker-compose.yml . USB serial programming from a docker-container \u00b6 To support flashing an ESP device directly from your RPi, /dev/ttyUSB0 is made available to the container. This file usually auto-created when an USB-to-serial-converter is plugged in. Docker needs the file to exist when the container is started, even if an ESP is not plugged in at that time. Files in created in /dev aren't persisted upon reboot. Thus a service is needed to create it at startup. This service is usually added by install.sh or menu.sh. If it somehow missing, just add the service manually: bash .templates/esphome/create-systemd-ttyUSB0-service.sh","title":"ESPHome"},{"location":"Containers/ESPHome/#esphome","text":"ESPHome is a system to control your ESP8266/ESP32 by simple yet powerful configuration files and control them remotely through Home Automation systems. Web UI is at: http://raspberrypi.local:6052/ Login username is admin and the password is automatically generated and can be found and changed in your docker-compose.yml .","title":"ESPHome"},{"location":"Containers/ESPHome/#usb-serial-programming-from-a-docker-container","text":"To support flashing an ESP device directly from your RPi, /dev/ttyUSB0 is made available to the container. This file usually auto-created when an USB-to-serial-converter is plugged in. Docker needs the file to exist when the container is started, even if an ESP is not plugged in at that time. Files in created in /dev aren't persisted upon reboot. Thus a service is needed to create it at startup. This service is usually added by install.sh or menu.sh. If it somehow missing, just add the service manually: bash .templates/esphome/create-systemd-ttyUSB0-service.sh","title":"USB serial programming from a docker-container"},{"location":"Containers/EspruinoHub/","text":"Espruinohub \u00b6 This is a testing container I tried it however the container keeps restarting docker logs espruinohub I get \"BLE Broken?\" but could just be i dont have any BLE devices nearby web interface is on \"{your_Pis_IP}:1888\" see EspruinoHub#status--websocket-mqtt--espruino-web-ide for other details. there were no recommendations for persistent data volumes. so docker-compose down may destroy all you configurations so use docker-compose stop in stead Please check existing issues if you encounter a problem, and then open a new issue if your problem has not been reported.","title":"Espruinohub"},{"location":"Containers/EspruinoHub/#espruinohub","text":"This is a testing container I tried it however the container keeps restarting docker logs espruinohub I get \"BLE Broken?\" but could just be i dont have any BLE devices nearby web interface is on \"{your_Pis_IP}:1888\" see EspruinoHub#status--websocket-mqtt--espruino-web-ide for other details. there were no recommendations for persistent data volumes. so docker-compose down may destroy all you configurations so use docker-compose stop in stead Please check existing issues if you encounter a problem, and then open a new issue if your problem has not been reported.","title":"Espruinohub"},{"location":"Containers/Grafana/","text":"Grafana \u00b6 References \u00b6 Docker Website Setting your time-zone \u00b6 The default ~/IOTstack/services/grafana/grafana.env contains this line: #TZ=Africa/Johannesburg Uncomment that line and change the right hand side to your own timezone . Adding InfluxDB datasource \u00b6 Select Data Sources -> Add data source -> InfluxDB. Set options: HTTP / URL: http://influxdb:8086 InfluxDB Details / Database: telegraf InfluxDB Details / User: nodered InfluxDB Details / Password: nodered Security \u00b6 If Grafana has just been installed but has never been launched then the following will be true: The folder ~/IOTstack/volumes/grafana will not exist; and The file ~/IOTstack/services/grafana/grafana.env will contain these lines: #GF_SECURITY_ADMIN_USER=admin #GF_SECURITY_ADMIN_PASSWORD=admin You should see those lines as documentation rather than something you are being invited to edit. It is telling you that the default administative user for Grafana is \"admin\" and that the default password for that user is \"admin\". If you do not change anything then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will: Expect you to login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, you will login as \"admin\" with whatever password you chose. You can change the administrator's password as often as you like via the web UI ( profile button, change password tab). This method (of not touching these two keys in grafana.env ) is the recommended approach. Please try to resist the temptation to fiddle! I want a different admin username (not recommended) \u00b6 If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=jack #GF_SECURITY_ADMIN_PASSWORD=admin then, when you bring up the stack and connect on port 3000, Grafana will: Expect you to login as user \"jack\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, \"jack\" will be the Grafana administrator and you will login with the password you chose, until you decide to change the password to something else via the web UI. Don't think you can come back later and tweak the Grafana administrator name in the environment variables. It doesn't work that way. It's a one-shot. I want a different default admin password (not recommended) \u00b6 Well, first off, the two methods above both make you set a different password on first login so there probably isn't much point to this. But, if you really insist\u2026 If, before you bring up the stack for the first time, you do this: #GF_SECURITY_ADMIN_USER=admin GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"admin\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and change the password in the environment variables. It doesn't work that way. It's a one-shot. I want to change everything (not recommended) \u00b6 If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=bill GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"bill\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and tweak either the username or password in the environment variables. It doesn't work that way. It's a one-shot. Distilling it down \u00b6 Before Grafana is launched for the first time: GF_SECURITY_ADMIN_USER has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. Whatever option you choose then that's the account name of Grafana's administrative user. But choosing any value other than \"admin\" is probably a bad idea. GF_SECURITY_ADMIN_PASSWORD has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. If its value is \"admin\" then you will be forced to change it the first time you login to Grafana. If its value is something other than \"admin\" then that will be the password until you change it via the web UI. These two environment keys only work for the first launch of Grafana. Once Grafana has been launched, you can never change either the username or the password by editing grafana.env . For this reason, it is better to leave grafana.env in its shrink-wrapped state. Your first login is as \"admin/admin\" and then you set the password you actually want when Grafana prompts you to change it. HELP \u2013 I forgot my Grafana admin password! \u00b6 Assuming your IOTstack is up, the magic incantation is: $ docker exec grafana grafana-cli --homepath \"/usr/share/grafana\" admin reset-admin-password \"admin\" Then, use a browser to connect to your Raspberry Pi on port 3000. Grafana will: Expect you login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. This magic incantation assumes that your administrative username is \"admin\". If you ignored the advice above and changed the administator username to something else then all bets are off. It might work anyway but we haven't tested it. Sorry. But that's why we said changing the username was not recommended. Overriding Grafana settings \u00b6 Grafana documentation contains a list of settings . Settings are described in terms of how they appear in \".ini\" files. An example of the sort of thing you might want to do is to enable anonymous access to your Grafana dashboards. The Grafana documentation describes this in \".ini\" format as: [auth.anonymous] enabled = true # Organization name that should be used for unauthenticated users org_name = Main Org. # Role for unauthenticated users, other valid values are `Editor` and `Admin` org_role = Viewer \".ini\" format is not really appropriate in a Docker context. Instead, you use environment variables to override Docker's settings. Environment variables are placed in ~/IOTstack/services/grafana/grafana.env . You need to convert \".ini\" format to environment variable syntax. The rules are: Start with \"GF_\", then Append the [section name], replacing any periods with underscores, then Append the section key \"as is\", then Append an \"=\", then Append the right hand side in quotes. Applying those rules gets you: GF_AUTH_ANONYMOUS_ENABLED=\"true\" GF_AUTH_ANONYMOUS_ORG_NAME=\"Main Org.\" GF_AUTH_ANONYMOUS_ORG_ROLE=\"Viewer\" It is not strictly necessary to encapsulate every right hand side value in quotes. In the above, both \"true\" and \"Viewer\" would work without quotes, whereas \"Main Org.\" needs quotes because of the embedded space. After you have changed ~/IOTstack/services/grafana/grafana.env , you need to propagate the changes into the Grafana container: $ cd ~/IOTstack $ docker-compose stop grafana $ docker-compose up -d In theory, the second command could be omitted, or both the second and third commands could be replaced with \"docker-compose restart grafana\" but experience suggests stopping the container is more reliable. A slightly more real-world example would involve choosing a different default organisation name for anonymous access. This example uses \"ChezMoi\". First, the environment key needs to be set to that value: GF_AUTH_ANONYMOUS_ORG_NAME=ChezMoi Then that change needs to be propagated into the Grafana container as explained above. Next, Grafana needs to be told that \"ChezMoi\" is the default organisation: Use your browser to login to Grafana as an administrator. From the \"Server Admin\" slide-out menu on the left hand side, choose \"Orgs\". In the list that appears, click on \"Main Org\". This opens an editing panel. Change the \"Name\" field to \"ChezMoi\" and click \"Update\". Sign-out of Grafana. You will be taken back to the login screen. Your URL bar will look something like this: http://myhost.mydomain.com:3000/login 6. Edit the URL to remove the \"login\" suffix and press return. If all your changes were applied successfully, you will have anonymous access and the URL will look something like this: http://myhost.mydomain.com:3000/?orgId=1 HELP \u2013 I made a mess! \u00b6 \"I made a bit of a mess with Grafana. First time user. Steep learning curve. False starts, many. Mistakes, unavoidable. Been there, done that. But now I really need to start from a clean slate. And, yes, I understand there is no undo for this.\" Begin by stopping Grafana: $ cd ~/IOTstack $ docker-compose stop grafana You have two options: Destroy your settings and dashboards but retain any plugins you may have installed: $ sudo rm ~/IOTstack/volumes/grafana/data/grafana.db Nuke everything (triple-check this command before you hit return): $ sudo rm -rf ~/IOTstack/volumes/grafana/data This is where you should edit ~/IOTstack/services/grafana/grafana.env to correct any problems (such as choosing an administrative username other than \"admin\"). When you are ready, bring Grafana back up again: $ cd ~/IOTstack $ docker-compose up -d Grafana will automatically recreate everything it needs. You will be able to login as \"admin/admin\".","title":"Grafana"},{"location":"Containers/Grafana/#grafana","text":"","title":"Grafana"},{"location":"Containers/Grafana/#references","text":"Docker Website","title":"References"},{"location":"Containers/Grafana/#setting-your-time-zone","text":"The default ~/IOTstack/services/grafana/grafana.env contains this line: #TZ=Africa/Johannesburg Uncomment that line and change the right hand side to your own timezone .","title":"Setting your time-zone"},{"location":"Containers/Grafana/#adding-influxdb-datasource","text":"Select Data Sources -> Add data source -> InfluxDB. Set options: HTTP / URL: http://influxdb:8086 InfluxDB Details / Database: telegraf InfluxDB Details / User: nodered InfluxDB Details / Password: nodered","title":"Adding InfluxDB datasource"},{"location":"Containers/Grafana/#security","text":"If Grafana has just been installed but has never been launched then the following will be true: The folder ~/IOTstack/volumes/grafana will not exist; and The file ~/IOTstack/services/grafana/grafana.env will contain these lines: #GF_SECURITY_ADMIN_USER=admin #GF_SECURITY_ADMIN_PASSWORD=admin You should see those lines as documentation rather than something you are being invited to edit. It is telling you that the default administative user for Grafana is \"admin\" and that the default password for that user is \"admin\". If you do not change anything then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will: Expect you to login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, you will login as \"admin\" with whatever password you chose. You can change the administrator's password as often as you like via the web UI ( profile button, change password tab). This method (of not touching these two keys in grafana.env ) is the recommended approach. Please try to resist the temptation to fiddle!","title":"Security"},{"location":"Containers/Grafana/#i-want-a-different-admin-username-not-recommended","text":"If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=jack #GF_SECURITY_ADMIN_PASSWORD=admin then, when you bring up the stack and connect on port 3000, Grafana will: Expect you to login as user \"jack\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, \"jack\" will be the Grafana administrator and you will login with the password you chose, until you decide to change the password to something else via the web UI. Don't think you can come back later and tweak the Grafana administrator name in the environment variables. It doesn't work that way. It's a one-shot.","title":"I want a different admin username (not recommended)"},{"location":"Containers/Grafana/#i-want-a-different-default-admin-password-not-recommended","text":"Well, first off, the two methods above both make you set a different password on first login so there probably isn't much point to this. But, if you really insist\u2026 If, before you bring up the stack for the first time, you do this: #GF_SECURITY_ADMIN_USER=admin GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"admin\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and change the password in the environment variables. It doesn't work that way. It's a one-shot.","title":"I want a different default admin password (not recommended)"},{"location":"Containers/Grafana/#i-want-to-change-everything-not-recommended","text":"If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=bill GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"bill\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and tweak either the username or password in the environment variables. It doesn't work that way. It's a one-shot.","title":"I want to change everything (not recommended)"},{"location":"Containers/Grafana/#distilling-it-down","text":"Before Grafana is launched for the first time: GF_SECURITY_ADMIN_USER has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. Whatever option you choose then that's the account name of Grafana's administrative user. But choosing any value other than \"admin\" is probably a bad idea. GF_SECURITY_ADMIN_PASSWORD has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. If its value is \"admin\" then you will be forced to change it the first time you login to Grafana. If its value is something other than \"admin\" then that will be the password until you change it via the web UI. These two environment keys only work for the first launch of Grafana. Once Grafana has been launched, you can never change either the username or the password by editing grafana.env . For this reason, it is better to leave grafana.env in its shrink-wrapped state. Your first login is as \"admin/admin\" and then you set the password you actually want when Grafana prompts you to change it.","title":"Distilling it down"},{"location":"Containers/Grafana/#help-i-forgot-my-grafana-admin-password","text":"Assuming your IOTstack is up, the magic incantation is: $ docker exec grafana grafana-cli --homepath \"/usr/share/grafana\" admin reset-admin-password \"admin\" Then, use a browser to connect to your Raspberry Pi on port 3000. Grafana will: Expect you login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. This magic incantation assumes that your administrative username is \"admin\". If you ignored the advice above and changed the administator username to something else then all bets are off. It might work anyway but we haven't tested it. Sorry. But that's why we said changing the username was not recommended.","title":"HELP \u2013 I forgot my Grafana admin password!"},{"location":"Containers/Grafana/#overriding-grafana-settings","text":"Grafana documentation contains a list of settings . Settings are described in terms of how they appear in \".ini\" files. An example of the sort of thing you might want to do is to enable anonymous access to your Grafana dashboards. The Grafana documentation describes this in \".ini\" format as: [auth.anonymous] enabled = true # Organization name that should be used for unauthenticated users org_name = Main Org. # Role for unauthenticated users, other valid values are `Editor` and `Admin` org_role = Viewer \".ini\" format is not really appropriate in a Docker context. Instead, you use environment variables to override Docker's settings. Environment variables are placed in ~/IOTstack/services/grafana/grafana.env . You need to convert \".ini\" format to environment variable syntax. The rules are: Start with \"GF_\", then Append the [section name], replacing any periods with underscores, then Append the section key \"as is\", then Append an \"=\", then Append the right hand side in quotes. Applying those rules gets you: GF_AUTH_ANONYMOUS_ENABLED=\"true\" GF_AUTH_ANONYMOUS_ORG_NAME=\"Main Org.\" GF_AUTH_ANONYMOUS_ORG_ROLE=\"Viewer\" It is not strictly necessary to encapsulate every right hand side value in quotes. In the above, both \"true\" and \"Viewer\" would work without quotes, whereas \"Main Org.\" needs quotes because of the embedded space. After you have changed ~/IOTstack/services/grafana/grafana.env , you need to propagate the changes into the Grafana container: $ cd ~/IOTstack $ docker-compose stop grafana $ docker-compose up -d In theory, the second command could be omitted, or both the second and third commands could be replaced with \"docker-compose restart grafana\" but experience suggests stopping the container is more reliable. A slightly more real-world example would involve choosing a different default organisation name for anonymous access. This example uses \"ChezMoi\". First, the environment key needs to be set to that value: GF_AUTH_ANONYMOUS_ORG_NAME=ChezMoi Then that change needs to be propagated into the Grafana container as explained above. Next, Grafana needs to be told that \"ChezMoi\" is the default organisation: Use your browser to login to Grafana as an administrator. From the \"Server Admin\" slide-out menu on the left hand side, choose \"Orgs\". In the list that appears, click on \"Main Org\". This opens an editing panel. Change the \"Name\" field to \"ChezMoi\" and click \"Update\". Sign-out of Grafana. You will be taken back to the login screen. Your URL bar will look something like this: http://myhost.mydomain.com:3000/login 6. Edit the URL to remove the \"login\" suffix and press return. If all your changes were applied successfully, you will have anonymous access and the URL will look something like this: http://myhost.mydomain.com:3000/?orgId=1","title":"Overriding Grafana settings"},{"location":"Containers/Grafana/#help-i-made-a-mess","text":"\"I made a bit of a mess with Grafana. First time user. Steep learning curve. False starts, many. Mistakes, unavoidable. Been there, done that. But now I really need to start from a clean slate. And, yes, I understand there is no undo for this.\" Begin by stopping Grafana: $ cd ~/IOTstack $ docker-compose stop grafana You have two options: Destroy your settings and dashboards but retain any plugins you may have installed: $ sudo rm ~/IOTstack/volumes/grafana/data/grafana.db Nuke everything (triple-check this command before you hit return): $ sudo rm -rf ~/IOTstack/volumes/grafana/data This is where you should edit ~/IOTstack/services/grafana/grafana.env to correct any problems (such as choosing an administrative username other than \"admin\"). When you are ready, bring Grafana back up again: $ cd ~/IOTstack $ docker-compose up -d Grafana will automatically recreate everything it needs. You will be able to login as \"admin/admin\".","title":"HELP \u2013 I made a mess!"},{"location":"Containers/Heimdall/","text":"Heimdall \u00b6 References \u00b6 Homepage Docker Web Interface \u00b6 The web UI can be found on \"your_ip\":8880 About Heimdall \u00b6 From the Heimdall website : Heimdall Application Dashboard is a dashboard for all your web applications. It doesn't need to be limited to applications though, you can add links to anything you like. There are no iframes here, no apps within apps, no abstraction of APIs. if you think something should work a certain way, it probably does. Within the context of IOTstack, the Heimdall Application Dashboard can help you organize your deployed services.","title":"Heimdall"},{"location":"Containers/Heimdall/#heimdall","text":"","title":"Heimdall"},{"location":"Containers/Heimdall/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/Heimdall/#web-interface","text":"The web UI can be found on \"your_ip\":8880","title":"Web Interface"},{"location":"Containers/Heimdall/#about-heimdall","text":"From the Heimdall website : Heimdall Application Dashboard is a dashboard for all your web applications. It doesn't need to be limited to applications though, you can add links to anything you like. There are no iframes here, no apps within apps, no abstraction of APIs. if you think something should work a certain way, it probably does. Within the context of IOTstack, the Heimdall Application Dashboard can help you organize your deployed services.","title":"About Heimdall"},{"location":"Containers/Home-Assistant/","text":"Home Assistant \u00b6 Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at your home and offer a platform for automating control. References \u00b6 Home Assistant home page Raspberry Pi installation General installation (may be useful if you are trying to run on other hardware). GitHub repository DockerHub Home Assistant: two versions \u00b6 There are two versions of Home Assistant: Home Assistant Container; and Supervised Home Assistant (also known as both \"Hass.io\" and \"Home Assistant Core\"). Each version: provides a web-based management interface on port 8123; and runs in \"host mode\" in order to discover devices on your LAN, including devices communicating via multicast traffic. Home Assistant Container runs as a single Docker container, and doesn't support all the features that Supervised Home Assistant does (such as add-ons). Supervised Home Assistant runs as a collection of Docker containers under its own orchestration. Technically, both versions of Home Assistant can be installed on your Raspberry Pi but you can't run both at the same time. Each version runs in \"host mode\" and binds to port 8123 so, in practice, the first version to start will claim the port and the second will then be blocked. IOTstack used to offer a menu entry leading to a convenience script that could install Supervised Home Assistant but that stopped working when Home Assistant changed their approach. Now, the only method supported by IOTstack is Home Assistant Container. Installing Home Assistant Container \u00b6 Home Assistant (Container) can be found in the Build Stack menu. Selecting it in this menu results in a service definition being added to: ~/IOTstack/docker-compose.yml When you choose \"Home Assistant\", the service definition added to your docker-compose.yml includes the following: image : ghcr.io/home-assistant/home-assistant:stable #image: ghcr.io/home-assistant/raspberrypi3-homeassistant:stable #image: ghcr.io/home-assistant/raspberrypi4-homeassistant:stable The active image is generic in the sense that it should work on any platform. You may wish to edit your docker-compose.yml to deactivate the generic image in favour of an image tailored to your hardware. The normal IOTstack commands apply to Home Assistant Container such as: $ cd ~/IOTstack $ docker-compose up -d Installing Supervised Home Assistant \u00b6 The direction being taken by the Home Assistant folks is to supply a ready-to-run image for your Raspberry Pi. That effectively dedicates your Raspberry Pi to Home Assistant and precludes the possibility of running alongside IOTstack and containers like Mosquitto, InfluxDB, Node-RED, Grafana, PiHole and WireGuard. It is possible to run Supervised Home Assistant on the same Raspberry Pi as IOTstack. The recommended approach is to start from a clean slate and use PiBuilder . When you visit the PiBuilder link you may well have a reaction like \"all far too complicated\" but you should try to get past that. PiBuilder has two main use-cases: Getting a Raspberry Pi built for IOTstack (and, optionally, Supervised Home Assistant) with the least fuss. Letting you record all your own customisations so that you can rebuild your Pis quickly with all your customisations already in place (the \"magic smoke\" scenario). It's the second use-case that produces most of the apparent complexity you see when you read the PiBuilder README for the first time. The first time you use PiBuilder, the process boils down to: Clone the PiBuilder repo onto your support host (Mac, Windows, etc). Customise two files within the PiBuilder scope: wpa_supplicant.conf options.sh where, among other things, you will enable: HOME_ASSISTANT_SUPERVISED_INSTALL=true Choose a Raspbian image and transfer it to your installation media (SD/SSD). The imaging tools typically finish by ejecting the installation media. Re-mount the installation media on your support host and either: Run the supplied setup_boot_volume.sh script (if your support host is macOS or Unix); or Just drag the contents of the PiBuilder \"boot\" folder into the top level of the \"/boot\" partition on your installation media (if your support host is Windows). Move the installation media to your Raspberry Pi and apply power. Run the scripts in order: Step Command run on support host Command run on Raspberry Pi 1 ssh -4 pi@raspberrypi.local 2 /boot/scripts/01_setup.sh \u00abname\u00bb 3 ssh-keygen -R raspberrypi.local 4 ssh -4 pi@\u00abname\u00bb.local 5 /boot/scripts/02_setup.sh 6 ssh pi@\u00abname\u00bb.local 7 /boot/scripts/03_setup.sh 8 ssh pi@\u00abname\u00bb.local 9 /boot/scripts/04_setup.sh 10 ssh pi@\u00abname\u00bb.local 11 /boot/scripts/05_setup.sh where \u00abname\u00bb is the name you give to your Raspberry Pi (eg \"iot-hub\"). After step 9, Supervised Home Assistant will be running. The 04_setup.sh script also deals with the random MACs problem. After step 11, you'll be able to either: Restore a backup; or Run the IOTstack menu and choose your containers. Why random MACs are such a hassle \u00b6 This material was originally posted as part of Issue 312 . It was moved here following a suggestion by lole-elol . When you connect to a Raspberry Pi via SSH (Secure Shell), that's a layer 7 protocol that is riding on top of TCP/IP. TCP (Transmission Control Protocol) is a layer 4 connection-oriented protocol which rides on IP (Internet Protocol) which is a layer 3 protocol. So far, so good. But you also need to know what happens at layers 2 and 1. When your SSH client (eg Mac or PC or another Unix box) opens its SSH connection, at layer 3 the IP stack applies the subnet mask against the IP addresses of both the source device (your Mac, PC, etc) and destination device (Raspberry Pi) to split them into \"network portion\" (on the left) and \"host portion\" on the right. It then compares the two network portions and, if they are the same, it says \"local network\". To complete the picture, if they do not compare the same, then IP substitutes the so-called \"default gateway\" address (ie your router) and repeats the mask-and-compare process which, unless something is seriously mis-configured, will result in those comparing the same and being \"local network\". This is why data-comms gurus sometimes say, \"all networking is local\". What happens next depends on the data communications media but we'll assume Ethernet and WiFi seeing as they are pretty much interchangeable for our purposes. The source machine (Mac, PC, etc) issues an ARP (address resolution protocol). It is a broadcast frame (we talk about \"frames\" rather than \"packets\" at Layer 2) asking the question, \"who has this destination IP address?\" The Raspberry Pi responds with a unicast packet saying, \"that's me\" and part of that includes the MAC (media access control) address of the Raspberry Pi. The source machine only does this once (and this is a key point). It assumes the relationship between IP address and MAC address will not change and it adds the relationship to its \"ARP cache\". You can see the cache on any Unix computer with: $ arp -a The Raspberry Pi makes the same assumption: it has learned both the IP and MAC address of the source machine (Mac, PC, etc) from the ARP request and has added that to its own ARP cache. In addition, every layer two switch (got one of those in your home?) has been snooping on this traffic and has learned, for each of its ports, which MAC address(es) are on those ports. Not \"MAC and IP\". A switch works at Layer 2. All it sees are frames. It only caches MAC addresses! When the switch saw the \"who has?\" ARP broadcast, it replicated that out of all of its ports but when the \"that's me\" came back from the Raspberry Pi as a unicast response, it only went out on the switch port where the source machine (Mac, PC, etc) was attached. After that, it's all caching. The Mac or PC has a packet to send to the Pi. It finds the hit in its ARP cache, wraps the packet in a frame and sends it out its Ethernet or WiFi interface. Any switches receive the frame, consult their own tables, and send the frame out the port on the next hop to the destination device. It doesn't matter whether you have one switch or several in a cascade, they have all learned the \"next hop\" to each destination MAC address they have seen. Ditto when the Pi sends back any reply packets. ARP. Switch. Mac/PC. All cached. The same basic principles apply, irrespective of whether the \"switching function\" is wired (Ethernet) or WiFi, so it doesn't really matter if your home arrangement is as straightforward as Mac or PC and Pi, both WiFi, via a local WiFi \"hub\" which is either standalone or part of your router. If something is capable of learning where a MAC is, it does. Still so far so good. Now comes the problem. You have established an SSH session connected to the Pi over its WiFi interface. You install Network Manager. As part of its setup, Network Manager discards the fixed MAC address which is burned into the Pi's WiFi interface and substitutes a randomly generated MAC address. It doesn't ask for permission to do that. It doesn't warn you it's about to do it. It just does it. When the WiFi interface comes up, it almost certainly \"speaks\" straight away via DHCP to ask for an IP address. The DHCP server looks in its own table of MAC-to-IP associations (fixed or dynamic, doesn't matter) and says \"never seen that MAC before - here's a brand new IP address lease\". The DHCP request is broadcast so all the switches will have learned the new MAC but they'll also still have the old MAC (until it times out). The Mac/PC will receive the DHCP broadcast but, unless it's the DHCP server, will discard it. Either way, it has no means of knowing that this new random MAC belongs to the Pi so it can't do anything sensible with the information. Meanwhile, SSH is trying to keep the session alive. It still thinks \"old IP address\" and its ARP cache still thinks old IP belongs to old MAC. Switches know where the frames are meant to go but even if a frame does get somewhere near the Pi, the Pi's NIC (network interface card) ignores it because it's now the wrong destination MAC. The upshot is that SSH looks like the session has frozen and it will eventually time-out with a broken pipe. To summarise: Network Manager has changed the MAC without so much as a by-your-leave and, unless you have assigned static IP addresses in the Raspberry Pi it's quite likely that the Pi will have a different IP address as well. But even a static IP can't save you from the machinations of Network Manager! The Pi is as happy as the proverbial Larry. It goes on, blissfully unaware that it has just confused the heck out of everything else. You can speed-up some of the activities that need to happen before everything gets going again. You can do things like clear the old entry from the ARP cache on the Mac/PC. You can try to force a multicast DNS daemon restart so that the \"raspberrypi.local\" address gets updated more quickly but mDNS is a distributed database so it can be hit and miss (and can sometimes lead to complaints about two devices trying to use the same name). Usually, the most effective thing you can do is pull power from the Pi, reboot your Mac/PC (easiest way to clear its ARP cache) and then apply power to the Pi so that it announces its mDNS address at the right time for the newly-booted Mac/PC to hear it and update its mDNS records. That's why the installation advice says words to the effect of: whatever else you do, don't try to install Network Manager while you're connected over WiFi. If SSH is how you're going to do it, you're in for a world of pain if you don't run an Ethernet cable for at least that part of the process. And it does get worse, of course. Installing Network Manager turns on random WiFi MAC. You can turn it off and go back to the fixed MAC. But then, when you install Docker, it happens again. It may also be that other packages come along in future and say, \"hey, look, Network Manager is installed - let's take advantage of that\" and it happens again when you least expect it. Devices changing their MACs at random is becoming reasonably common. If you have a mobile device running a reasonably current OS, it is probably changing its MAC all the time. The idea is to make it hard for Fred's Corner Store to track you and conclude, \"Hey, Alex is back in the shop again.\" Random MACs are not a problem for a client device like a phone, tablet or laptop. But they are definitely a serious problem for a server device. In TCP/IP any device can be a client or a server for any protocol. The distinction here is about typical use. A mobile device is not usually set up to offer services like MQTT or Node-RED. It typically initiates connections with servers like Docker containers running on a Raspberry Pi. It is not just configuration-time SSH sessions that break. If you decide to leave Raspberry Pi random Wifi MAC active and you have other clients (eq IoT devices) communicating with the Pi over WiFi, you will wrong-foot those clients each time the Raspberry Pi reboots. Data communications services from those clients will be impacted until those client devices time-out and catch up.","title":"Home Assistant"},{"location":"Containers/Home-Assistant/#home-assistant","text":"Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at your home and offer a platform for automating control.","title":"Home Assistant"},{"location":"Containers/Home-Assistant/#references","text":"Home Assistant home page Raspberry Pi installation General installation (may be useful if you are trying to run on other hardware). GitHub repository DockerHub","title":"References"},{"location":"Containers/Home-Assistant/#home-assistant-two-versions","text":"There are two versions of Home Assistant: Home Assistant Container; and Supervised Home Assistant (also known as both \"Hass.io\" and \"Home Assistant Core\"). Each version: provides a web-based management interface on port 8123; and runs in \"host mode\" in order to discover devices on your LAN, including devices communicating via multicast traffic. Home Assistant Container runs as a single Docker container, and doesn't support all the features that Supervised Home Assistant does (such as add-ons). Supervised Home Assistant runs as a collection of Docker containers under its own orchestration. Technically, both versions of Home Assistant can be installed on your Raspberry Pi but you can't run both at the same time. Each version runs in \"host mode\" and binds to port 8123 so, in practice, the first version to start will claim the port and the second will then be blocked. IOTstack used to offer a menu entry leading to a convenience script that could install Supervised Home Assistant but that stopped working when Home Assistant changed their approach. Now, the only method supported by IOTstack is Home Assistant Container.","title":"Home Assistant: two versions"},{"location":"Containers/Home-Assistant/#installing-home-assistant-container","text":"Home Assistant (Container) can be found in the Build Stack menu. Selecting it in this menu results in a service definition being added to: ~/IOTstack/docker-compose.yml When you choose \"Home Assistant\", the service definition added to your docker-compose.yml includes the following: image : ghcr.io/home-assistant/home-assistant:stable #image: ghcr.io/home-assistant/raspberrypi3-homeassistant:stable #image: ghcr.io/home-assistant/raspberrypi4-homeassistant:stable The active image is generic in the sense that it should work on any platform. You may wish to edit your docker-compose.yml to deactivate the generic image in favour of an image tailored to your hardware. The normal IOTstack commands apply to Home Assistant Container such as: $ cd ~/IOTstack $ docker-compose up -d","title":"Installing Home Assistant Container"},{"location":"Containers/Home-Assistant/#installing-supervised-home-assistant","text":"The direction being taken by the Home Assistant folks is to supply a ready-to-run image for your Raspberry Pi. That effectively dedicates your Raspberry Pi to Home Assistant and precludes the possibility of running alongside IOTstack and containers like Mosquitto, InfluxDB, Node-RED, Grafana, PiHole and WireGuard. It is possible to run Supervised Home Assistant on the same Raspberry Pi as IOTstack. The recommended approach is to start from a clean slate and use PiBuilder . When you visit the PiBuilder link you may well have a reaction like \"all far too complicated\" but you should try to get past that. PiBuilder has two main use-cases: Getting a Raspberry Pi built for IOTstack (and, optionally, Supervised Home Assistant) with the least fuss. Letting you record all your own customisations so that you can rebuild your Pis quickly with all your customisations already in place (the \"magic smoke\" scenario). It's the second use-case that produces most of the apparent complexity you see when you read the PiBuilder README for the first time. The first time you use PiBuilder, the process boils down to: Clone the PiBuilder repo onto your support host (Mac, Windows, etc). Customise two files within the PiBuilder scope: wpa_supplicant.conf options.sh where, among other things, you will enable: HOME_ASSISTANT_SUPERVISED_INSTALL=true Choose a Raspbian image and transfer it to your installation media (SD/SSD). The imaging tools typically finish by ejecting the installation media. Re-mount the installation media on your support host and either: Run the supplied setup_boot_volume.sh script (if your support host is macOS or Unix); or Just drag the contents of the PiBuilder \"boot\" folder into the top level of the \"/boot\" partition on your installation media (if your support host is Windows). Move the installation media to your Raspberry Pi and apply power. Run the scripts in order: Step Command run on support host Command run on Raspberry Pi 1 ssh -4 pi@raspberrypi.local 2 /boot/scripts/01_setup.sh \u00abname\u00bb 3 ssh-keygen -R raspberrypi.local 4 ssh -4 pi@\u00abname\u00bb.local 5 /boot/scripts/02_setup.sh 6 ssh pi@\u00abname\u00bb.local 7 /boot/scripts/03_setup.sh 8 ssh pi@\u00abname\u00bb.local 9 /boot/scripts/04_setup.sh 10 ssh pi@\u00abname\u00bb.local 11 /boot/scripts/05_setup.sh where \u00abname\u00bb is the name you give to your Raspberry Pi (eg \"iot-hub\"). After step 9, Supervised Home Assistant will be running. The 04_setup.sh script also deals with the random MACs problem. After step 11, you'll be able to either: Restore a backup; or Run the IOTstack menu and choose your containers.","title":"Installing Supervised Home Assistant"},{"location":"Containers/Home-Assistant/#why-random-macs-are-such-a-hassle","text":"This material was originally posted as part of Issue 312 . It was moved here following a suggestion by lole-elol . When you connect to a Raspberry Pi via SSH (Secure Shell), that's a layer 7 protocol that is riding on top of TCP/IP. TCP (Transmission Control Protocol) is a layer 4 connection-oriented protocol which rides on IP (Internet Protocol) which is a layer 3 protocol. So far, so good. But you also need to know what happens at layers 2 and 1. When your SSH client (eg Mac or PC or another Unix box) opens its SSH connection, at layer 3 the IP stack applies the subnet mask against the IP addresses of both the source device (your Mac, PC, etc) and destination device (Raspberry Pi) to split them into \"network portion\" (on the left) and \"host portion\" on the right. It then compares the two network portions and, if they are the same, it says \"local network\". To complete the picture, if they do not compare the same, then IP substitutes the so-called \"default gateway\" address (ie your router) and repeats the mask-and-compare process which, unless something is seriously mis-configured, will result in those comparing the same and being \"local network\". This is why data-comms gurus sometimes say, \"all networking is local\". What happens next depends on the data communications media but we'll assume Ethernet and WiFi seeing as they are pretty much interchangeable for our purposes. The source machine (Mac, PC, etc) issues an ARP (address resolution protocol). It is a broadcast frame (we talk about \"frames\" rather than \"packets\" at Layer 2) asking the question, \"who has this destination IP address?\" The Raspberry Pi responds with a unicast packet saying, \"that's me\" and part of that includes the MAC (media access control) address of the Raspberry Pi. The source machine only does this once (and this is a key point). It assumes the relationship between IP address and MAC address will not change and it adds the relationship to its \"ARP cache\". You can see the cache on any Unix computer with: $ arp -a The Raspberry Pi makes the same assumption: it has learned both the IP and MAC address of the source machine (Mac, PC, etc) from the ARP request and has added that to its own ARP cache. In addition, every layer two switch (got one of those in your home?) has been snooping on this traffic and has learned, for each of its ports, which MAC address(es) are on those ports. Not \"MAC and IP\". A switch works at Layer 2. All it sees are frames. It only caches MAC addresses! When the switch saw the \"who has?\" ARP broadcast, it replicated that out of all of its ports but when the \"that's me\" came back from the Raspberry Pi as a unicast response, it only went out on the switch port where the source machine (Mac, PC, etc) was attached. After that, it's all caching. The Mac or PC has a packet to send to the Pi. It finds the hit in its ARP cache, wraps the packet in a frame and sends it out its Ethernet or WiFi interface. Any switches receive the frame, consult their own tables, and send the frame out the port on the next hop to the destination device. It doesn't matter whether you have one switch or several in a cascade, they have all learned the \"next hop\" to each destination MAC address they have seen. Ditto when the Pi sends back any reply packets. ARP. Switch. Mac/PC. All cached. The same basic principles apply, irrespective of whether the \"switching function\" is wired (Ethernet) or WiFi, so it doesn't really matter if your home arrangement is as straightforward as Mac or PC and Pi, both WiFi, via a local WiFi \"hub\" which is either standalone or part of your router. If something is capable of learning where a MAC is, it does. Still so far so good. Now comes the problem. You have established an SSH session connected to the Pi over its WiFi interface. You install Network Manager. As part of its setup, Network Manager discards the fixed MAC address which is burned into the Pi's WiFi interface and substitutes a randomly generated MAC address. It doesn't ask for permission to do that. It doesn't warn you it's about to do it. It just does it. When the WiFi interface comes up, it almost certainly \"speaks\" straight away via DHCP to ask for an IP address. The DHCP server looks in its own table of MAC-to-IP associations (fixed or dynamic, doesn't matter) and says \"never seen that MAC before - here's a brand new IP address lease\". The DHCP request is broadcast so all the switches will have learned the new MAC but they'll also still have the old MAC (until it times out). The Mac/PC will receive the DHCP broadcast but, unless it's the DHCP server, will discard it. Either way, it has no means of knowing that this new random MAC belongs to the Pi so it can't do anything sensible with the information. Meanwhile, SSH is trying to keep the session alive. It still thinks \"old IP address\" and its ARP cache still thinks old IP belongs to old MAC. Switches know where the frames are meant to go but even if a frame does get somewhere near the Pi, the Pi's NIC (network interface card) ignores it because it's now the wrong destination MAC. The upshot is that SSH looks like the session has frozen and it will eventually time-out with a broken pipe. To summarise: Network Manager has changed the MAC without so much as a by-your-leave and, unless you have assigned static IP addresses in the Raspberry Pi it's quite likely that the Pi will have a different IP address as well. But even a static IP can't save you from the machinations of Network Manager! The Pi is as happy as the proverbial Larry. It goes on, blissfully unaware that it has just confused the heck out of everything else. You can speed-up some of the activities that need to happen before everything gets going again. You can do things like clear the old entry from the ARP cache on the Mac/PC. You can try to force a multicast DNS daemon restart so that the \"raspberrypi.local\" address gets updated more quickly but mDNS is a distributed database so it can be hit and miss (and can sometimes lead to complaints about two devices trying to use the same name). Usually, the most effective thing you can do is pull power from the Pi, reboot your Mac/PC (easiest way to clear its ARP cache) and then apply power to the Pi so that it announces its mDNS address at the right time for the newly-booted Mac/PC to hear it and update its mDNS records. That's why the installation advice says words to the effect of: whatever else you do, don't try to install Network Manager while you're connected over WiFi. If SSH is how you're going to do it, you're in for a world of pain if you don't run an Ethernet cable for at least that part of the process. And it does get worse, of course. Installing Network Manager turns on random WiFi MAC. You can turn it off and go back to the fixed MAC. But then, when you install Docker, it happens again. It may also be that other packages come along in future and say, \"hey, look, Network Manager is installed - let's take advantage of that\" and it happens again when you least expect it. Devices changing their MACs at random is becoming reasonably common. If you have a mobile device running a reasonably current OS, it is probably changing its MAC all the time. The idea is to make it hard for Fred's Corner Store to track you and conclude, \"Hey, Alex is back in the shop again.\" Random MACs are not a problem for a client device like a phone, tablet or laptop. But they are definitely a serious problem for a server device. In TCP/IP any device can be a client or a server for any protocol. The distinction here is about typical use. A mobile device is not usually set up to offer services like MQTT or Node-RED. It typically initiates connections with servers like Docker containers running on a Raspberry Pi. It is not just configuration-time SSH sessions that break. If you decide to leave Raspberry Pi random Wifi MAC active and you have other clients (eq IoT devices) communicating with the Pi over WiFi, you will wrong-foot those clients each time the Raspberry Pi reboots. Data communications services from those clients will be impacted until those client devices time-out and catch up.","title":"Why random MACs are such a hassle"},{"location":"Containers/Homer/","text":"Homer \u00b6 References \u00b6 Homepage Docker Web Interface \u00b6 The web UI can be found on \"your_ip\":8881 About Homer \u00b6 From the Homer README : A dead simple static HOMepage for your servER to keep your services on hand, from a simple yaml configuration file. You can find an example of the config.yml file here . Within the context of IOTstack, Homer can help you organize your deployed services.","title":"Homer"},{"location":"Containers/Homer/#homer","text":"","title":"Homer"},{"location":"Containers/Homer/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/Homer/#web-interface","text":"The web UI can be found on \"your_ip\":8881","title":"Web Interface"},{"location":"Containers/Homer/#about-homer","text":"From the Homer README : A dead simple static HOMepage for your servER to keep your services on hand, from a simple yaml configuration file. You can find an example of the config.yml file here . Within the context of IOTstack, Homer can help you organize your deployed services.","title":"About Homer"},{"location":"Containers/InfluxDB/","text":"InfluxDB \u00b6 A time series database. InfluxDB has configurable aggregation and retention policies allowing measurement resolution reduction, storing all added data points for recent data and only aggregated values for older data. To connect use: Field Default User nodered Password nodered URL (from other services) http://influxdb:8086 URL (on the host machine) http://localhost:8086 References \u00b6 Docker Website Setup \u00b6 To access the influx console, show current databases and database measurements: pi@raspberrypi:~/IOTstack $ docker-compose exec influxdb bash root@6bca535a945f:/# influx Connected to http://localhost:8086 version 1.8.10 InfluxDB shell version: 1.8.10 > show databases name: databases name ---- _internal telegraf > use telegraf Using database telegraf > show measurements name: measurements name ---- cpu cpu_temperature disk diskio etc... To create a new database and set a limited retention policy, here for instance any data older than 52 weeks is deleted: > create database mydb > show retention policies on mydb name duration shardGroupDuration replicaN default ---- -------- ------------------ -------- ------- autogen 0s 168h0m0s 1 true > alter retention policy \"autogen\" on \"mydb\" duration 52w shard duration 1w replication 1 default > show retention policies on mydb name duration shardGroupDuration replicaN default ---- -------- ------------------ -------- ------- autogen 8736h0m0s 168h0m0s 1 true Aggregation, on the other hand, is where you keep your relevant statistics, but decrease their time-resolution and lose individual data-points. This is a much more complicated topic and harder to configure. As such it is well outside the scope of this guide. Reducing flash wear-out \u00b6 SSD-drives have pretty good controllers spreading out writes, so this isn't a this isn't really a concern for them. But if you store data on a SD-card, flash wear may cause the card to fail prematurely. Flash memory has a limited number of erase-write cycles per physical block. These blocks may be multiple megabytes. You can use sudo lsblk -D to see how big the erase granularity is on your card. The goal is to avoid writing lots of small changes targeting the same physical blocks. Here are some tips to mitigate SD-card wear: Don't use short retention policies. This may mask heavy disk IO without increasing disk space usage. Depending on the file system used, new data may be written to the same flash blocks that were freed by expiration, wearing them out. Take care not to add measurements too often. If possible no more often than once a minute. Add all measurements in one operation. Adding measurements directly to Influxdb will cause a write on every operation. If your client code can't aggregate multiple measurements into one write, consider routing them via Telegraf. It has the flush_interval -option, which will combine the measurements into one write. All InfluxDB queries are logged by default and logs are written to the SD-card. To disable this, add to docker-compose.yml, next to the other INFLUXDB_* entries: - INFLUXDB_DATA_QUERY_LOG_ENABLED=false - INFLUXDB_HTTP_LOG_ENABLED=false This is especially important if you plan on having Grafana or Chronograf displaying up-to-date data on a dashboard. Old-menu branch \u00b6 The credentials and default database name for influxdb are stored in the file called influxdb/influx.env . The default username and password is set to \"nodered\" for both. It is HIGHLY recommended that you change them. The environment file contains several commented out options allowing you to set several access options such as default admin user credentials as well as the default database name. Any change to the environment file will require a restart of the service. To access the terminal for influxdb execute ./services/influxdb/terminal.sh . Here you can set additional parameters or create other databases.","title":"InfluxDB"},{"location":"Containers/InfluxDB/#influxdb","text":"A time series database. InfluxDB has configurable aggregation and retention policies allowing measurement resolution reduction, storing all added data points for recent data and only aggregated values for older data. To connect use: Field Default User nodered Password nodered URL (from other services) http://influxdb:8086 URL (on the host machine) http://localhost:8086","title":"InfluxDB"},{"location":"Containers/InfluxDB/#references","text":"Docker Website","title":"References"},{"location":"Containers/InfluxDB/#setup","text":"To access the influx console, show current databases and database measurements: pi@raspberrypi:~/IOTstack $ docker-compose exec influxdb bash root@6bca535a945f:/# influx Connected to http://localhost:8086 version 1.8.10 InfluxDB shell version: 1.8.10 > show databases name: databases name ---- _internal telegraf > use telegraf Using database telegraf > show measurements name: measurements name ---- cpu cpu_temperature disk diskio etc... To create a new database and set a limited retention policy, here for instance any data older than 52 weeks is deleted: > create database mydb > show retention policies on mydb name duration shardGroupDuration replicaN default ---- -------- ------------------ -------- ------- autogen 0s 168h0m0s 1 true > alter retention policy \"autogen\" on \"mydb\" duration 52w shard duration 1w replication 1 default > show retention policies on mydb name duration shardGroupDuration replicaN default ---- -------- ------------------ -------- ------- autogen 8736h0m0s 168h0m0s 1 true Aggregation, on the other hand, is where you keep your relevant statistics, but decrease their time-resolution and lose individual data-points. This is a much more complicated topic and harder to configure. As such it is well outside the scope of this guide.","title":"Setup"},{"location":"Containers/InfluxDB/#reducing-flash-wear-out","text":"SSD-drives have pretty good controllers spreading out writes, so this isn't a this isn't really a concern for them. But if you store data on a SD-card, flash wear may cause the card to fail prematurely. Flash memory has a limited number of erase-write cycles per physical block. These blocks may be multiple megabytes. You can use sudo lsblk -D to see how big the erase granularity is on your card. The goal is to avoid writing lots of small changes targeting the same physical blocks. Here are some tips to mitigate SD-card wear: Don't use short retention policies. This may mask heavy disk IO without increasing disk space usage. Depending on the file system used, new data may be written to the same flash blocks that were freed by expiration, wearing them out. Take care not to add measurements too often. If possible no more often than once a minute. Add all measurements in one operation. Adding measurements directly to Influxdb will cause a write on every operation. If your client code can't aggregate multiple measurements into one write, consider routing them via Telegraf. It has the flush_interval -option, which will combine the measurements into one write. All InfluxDB queries are logged by default and logs are written to the SD-card. To disable this, add to docker-compose.yml, next to the other INFLUXDB_* entries: - INFLUXDB_DATA_QUERY_LOG_ENABLED=false - INFLUXDB_HTTP_LOG_ENABLED=false This is especially important if you plan on having Grafana or Chronograf displaying up-to-date data on a dashboard.","title":"Reducing flash wear-out"},{"location":"Containers/InfluxDB/#old-menu-branch","text":"The credentials and default database name for influxdb are stored in the file called influxdb/influx.env . The default username and password is set to \"nodered\" for both. It is HIGHLY recommended that you change them. The environment file contains several commented out options allowing you to set several access options such as default admin user credentials as well as the default database name. Any change to the environment file will require a restart of the service. To access the terminal for influxdb execute ./services/influxdb/terminal.sh . Here you can set additional parameters or create other databases.","title":"Old-menu branch"},{"location":"Containers/Kapacitor/","text":"Kapacitor \u00b6 References \u00b6 influxdata Kapacitor documentation GitHub : influxdata/influxdata-docker/kapacitor DockerHub : influxdata Kapacitor Upgrading Kapacitor \u00b6 You can update the container via: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. Kapacitor version pinning \u00b6 If you need to pin to a particular version: Use your favourite text editor to open docker-compose.yml . Find the line: image: kapacitor:1.5 Replace 1.5 with the version you wish to pin to. For example, to pin to version 1.5.9: image: kapacitor:1.5.9 Note: Be cautious about using the latest tag. At the time of writing, there was no linux/arm/v7 architecture support. Save the file and tell docker-compose to bring up the container: $ cd ~/IOTstack $ docker-compose up -d kapacitor $ docker system prune","title":"Kapacitor"},{"location":"Containers/Kapacitor/#kapacitor","text":"","title":"Kapacitor"},{"location":"Containers/Kapacitor/#references","text":"influxdata Kapacitor documentation GitHub : influxdata/influxdata-docker/kapacitor DockerHub : influxdata Kapacitor","title":"References"},{"location":"Containers/Kapacitor/#upgrading-kapacitor","text":"You can update the container via: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images.","title":"Upgrading Kapacitor"},{"location":"Containers/Kapacitor/#kapacitor-version-pinning","text":"If you need to pin to a particular version: Use your favourite text editor to open docker-compose.yml . Find the line: image: kapacitor:1.5 Replace 1.5 with the version you wish to pin to. For example, to pin to version 1.5.9: image: kapacitor:1.5.9 Note: Be cautious about using the latest tag. At the time of writing, there was no linux/arm/v7 architecture support. Save the file and tell docker-compose to bring up the container: $ cd ~/IOTstack $ docker-compose up -d kapacitor $ docker system prune","title":"Kapacitor version pinning"},{"location":"Containers/MariaDB/","text":"MariaDB \u00b6 Source \u00b6 Docker hub Webpage About \u00b6 MariaDB is a fork of MySQL. This is an unofficial image provided by linuxserver.io because there is no official image for arm. Connecting to the DB \u00b6 The port is 3306. It exists inside the docker network so you can connect via mariadb:3306 for internal connections. For external connections use <your Pis IP>:3306 Setup \u00b6 Before starting the stack, edit the docker-compose.yml file and check your environment variables. In particular: environment: - TZ=Etc/UTC - MYSQL_ROOT_PASSWORD= - MYSQL_DATABASE=default - MYSQL_USER=mariadbuser - MYSQL_PASSWORD= If you are running old-menu, you will have to set both passwords. Under new-menu, the menu may have allocated random passwords for you but you can change them if you like. You only get the opportunity to change the MQSL_ prefixed environment variables before you bring up the container for the first time. If you decide to change these values after initialisation, you will either have to: Erase the persistent storage area and start again. There are three steps: Stop the container and remove the persistent storage area: $ cd ~/IOTstack $ docker-compose rm --force --stop -v mariadb $ sudo rm -rf ./volumes/mariadb Edit docker-compose.yml and change the variables. Bring up the container: $ docker-compose up -d mariadb Open a terminal window within the container (see below) and change the values by hand. The how-to is beyond the scope of this documentation. Google is your friend! Terminal \u00b6 You can open a terminal session within the mariadb container via: $ docker exec -it mariadb bash To connect to the database: mysql -uroot -p To close the terminal session, either: type \"exit\" and press return ; or press control + d . Container health check \u00b6 theory of operation \u00b6 A script , or \"agent\", to assess the health of the MariaDB container has been added to the local image via the Dockerfile . In other words, the script is specific to IOTstack. The agent is invoked 30 seconds after the container starts, and every 30 seconds thereafter. The agent: Runs the command: mysqladmin ping -h localhost If that command succeeds, the agent compares the response returned by the command with the expected response: mysqld is alive If the command returned the expected response, the agent tests the responsiveness of the TCP port the mysqld daemon should be listening on (see customising health-check ). If all of those steps succeed, the agent concludes that MariaDB is functioning properly and returns \"healthy\". monitoring health-check \u00b6 Portainer's Containers display contains a Status column which shows health-check results for all containers that support the feature. You can also use the docker ps command to monitor health-check results. The following command narrows the focus to mariadb: $ docker ps --format \"table {{.Names}}\\t{{.Status}}\" --filter name = mariadb Possible reply patterns are: The container is starting and has not yet run the health-check agent: NAMES STATUS mariadb Up 5 seconds (health: starting) The container has been running for at least 30 seconds and the health-check agent has returned a positive result within the last 30 seconds: NAMES STATUS mariadb Up 33 seconds (healthy) The container has been running for more than 90 seconds but has failed the last three successive health-check tests: NAMES STATUS mariadb Up About a minute (unhealthy) customising health-check \u00b6 You can customise the operation of the health-check agent by editing the mariadb service definition in your Compose file: By default, the mysqld daemon listens to internal port 3306. If you need change that port, you also need to inform the health-check agent via an environment variable. For example, suppose you changed the internal port to 12345: environment : - MYSQL_TCP_PORT=12345 Notes: The MYSQL_TCP_PORT variable is defined by MariaDB , not IOTstack, so changing this variable affects more than just the health-check agent. If you are running \"old menu\", this change should be made in the file: ~/IOTstack/services/mariadb/mariadb.env The mysqladmin ping command relies on the root password supplied via the MYSQL_ROOT_PASSWORD environment variable in the Compose file. The command will not succeed if the root password is not correct, and the agent will return \"unhealthy\". If the health-check agent misbehaves in your environment, or if you simply don't want it to be active, you can disable all health-checking for the container by adding the following lines to its service definition: healthcheck : disable : true Note: The mere presence of a healthcheck: clause in the mariadb service definition overrides the supplied agent. In other words, the following can't be used to re-enable the supplied agent: healthcheck : disable : false You must remove the entire healthcheck: clause. Keeping MariaDB up-to-date \u00b6 To update the mariadb container: $ cd ~/IOTstack $ docker-compose build --no-cache --pull mariadb $ docker-compose up -d mariadb $ docker system prune $ docker system prune The first \"prune\" removes the old local image, the second removes the old base image.","title":"MariaDB"},{"location":"Containers/MariaDB/#mariadb","text":"","title":"MariaDB"},{"location":"Containers/MariaDB/#source","text":"Docker hub Webpage","title":"Source"},{"location":"Containers/MariaDB/#about","text":"MariaDB is a fork of MySQL. This is an unofficial image provided by linuxserver.io because there is no official image for arm.","title":"About"},{"location":"Containers/MariaDB/#connecting-to-the-db","text":"The port is 3306. It exists inside the docker network so you can connect via mariadb:3306 for internal connections. For external connections use <your Pis IP>:3306","title":"Connecting to the DB"},{"location":"Containers/MariaDB/#setup","text":"Before starting the stack, edit the docker-compose.yml file and check your environment variables. In particular: environment: - TZ=Etc/UTC - MYSQL_ROOT_PASSWORD= - MYSQL_DATABASE=default - MYSQL_USER=mariadbuser - MYSQL_PASSWORD= If you are running old-menu, you will have to set both passwords. Under new-menu, the menu may have allocated random passwords for you but you can change them if you like. You only get the opportunity to change the MQSL_ prefixed environment variables before you bring up the container for the first time. If you decide to change these values after initialisation, you will either have to: Erase the persistent storage area and start again. There are three steps: Stop the container and remove the persistent storage area: $ cd ~/IOTstack $ docker-compose rm --force --stop -v mariadb $ sudo rm -rf ./volumes/mariadb Edit docker-compose.yml and change the variables. Bring up the container: $ docker-compose up -d mariadb Open a terminal window within the container (see below) and change the values by hand. The how-to is beyond the scope of this documentation. Google is your friend!","title":"Setup"},{"location":"Containers/MariaDB/#terminal","text":"You can open a terminal session within the mariadb container via: $ docker exec -it mariadb bash To connect to the database: mysql -uroot -p To close the terminal session, either: type \"exit\" and press return ; or press control + d .","title":"Terminal"},{"location":"Containers/MariaDB/#container-health-check","text":"","title":"Container health check"},{"location":"Containers/MariaDB/#theory-of-operation","text":"A script , or \"agent\", to assess the health of the MariaDB container has been added to the local image via the Dockerfile . In other words, the script is specific to IOTstack. The agent is invoked 30 seconds after the container starts, and every 30 seconds thereafter. The agent: Runs the command: mysqladmin ping -h localhost If that command succeeds, the agent compares the response returned by the command with the expected response: mysqld is alive If the command returned the expected response, the agent tests the responsiveness of the TCP port the mysqld daemon should be listening on (see customising health-check ). If all of those steps succeed, the agent concludes that MariaDB is functioning properly and returns \"healthy\".","title":"theory of operation"},{"location":"Containers/MariaDB/#monitoring-health-check","text":"Portainer's Containers display contains a Status column which shows health-check results for all containers that support the feature. You can also use the docker ps command to monitor health-check results. The following command narrows the focus to mariadb: $ docker ps --format \"table {{.Names}}\\t{{.Status}}\" --filter name = mariadb Possible reply patterns are: The container is starting and has not yet run the health-check agent: NAMES STATUS mariadb Up 5 seconds (health: starting) The container has been running for at least 30 seconds and the health-check agent has returned a positive result within the last 30 seconds: NAMES STATUS mariadb Up 33 seconds (healthy) The container has been running for more than 90 seconds but has failed the last three successive health-check tests: NAMES STATUS mariadb Up About a minute (unhealthy)","title":"monitoring health-check"},{"location":"Containers/MariaDB/#customising-health-check","text":"You can customise the operation of the health-check agent by editing the mariadb service definition in your Compose file: By default, the mysqld daemon listens to internal port 3306. If you need change that port, you also need to inform the health-check agent via an environment variable. For example, suppose you changed the internal port to 12345: environment : - MYSQL_TCP_PORT=12345 Notes: The MYSQL_TCP_PORT variable is defined by MariaDB , not IOTstack, so changing this variable affects more than just the health-check agent. If you are running \"old menu\", this change should be made in the file: ~/IOTstack/services/mariadb/mariadb.env The mysqladmin ping command relies on the root password supplied via the MYSQL_ROOT_PASSWORD environment variable in the Compose file. The command will not succeed if the root password is not correct, and the agent will return \"unhealthy\". If the health-check agent misbehaves in your environment, or if you simply don't want it to be active, you can disable all health-checking for the container by adding the following lines to its service definition: healthcheck : disable : true Note: The mere presence of a healthcheck: clause in the mariadb service definition overrides the supplied agent. In other words, the following can't be used to re-enable the supplied agent: healthcheck : disable : false You must remove the entire healthcheck: clause.","title":"customising health-check"},{"location":"Containers/MariaDB/#keeping-mariadb-up-to-date","text":"To update the mariadb container: $ cd ~/IOTstack $ docker-compose build --no-cache --pull mariadb $ docker-compose up -d mariadb $ docker system prune $ docker system prune The first \"prune\" removes the old local image, the second removes the old base image.","title":"Keeping MariaDB up-to-date"},{"location":"Containers/Mosquitto/","text":"Mosquitto \u00b6 This document discusses an IOTstack-specific version of Mosquitto built on top of Eclipse/Mosquitto using a Dockerfile . If you want the documentation for the original implementation of Mosquitto (just \"as it comes\" from DockerHub ) please see Mosquitto.md on the old-menu branch. References \u00b6 Eclipse Mosquitto home GitHub : eclipse/mosquitto DockerHub : eclipse-mosquitto Setting up passwords (video) Tutorial: from MQTT to InfluxDB via Node-Red Significant directories and files \u00b6 ~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 mosquitto \u2502 \u251c\u2500\u2500 service.yml \u2776 \u2502 \u251c\u2500\u2500 Dockerfile \u2777 \u2502 \u251c\u2500\u2500 docker-entrypoint.sh \u2778 \u2502 \u2514\u2500\u2500 iotstack_defaults \u2779 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u251c\u2500\u2500 filter.acl \u2502 \u2502 \u2514\u2500\u2500 mosquitto.conf \u2502 \u2514\u2500\u2500 pwfile \u2502 \u2514\u2500\u2500 pwfile \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 mosquitto \u2502 \u2514\u2500\u2500 service.yml \u277a \u251c\u2500\u2500 docker-compose.yml \u277b \u2514\u2500\u2500 volumes \u2514\u2500\u2500 mosquitto \u277c \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 filter.acl \u2502 \u2514\u2500\u2500 mosquitto.conf \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 mosquitto.db \u251c\u2500\u2500 log \u2514\u2500\u2500 pwfile \u2514\u2500\u2500 pwfile The template service definition . The Dockerfile used to customise Mosquitto for IOTstack. A replacement for the Eclipse-Mosquitto script of the same name, extended to handle container self-repair. A standard set of defaults for IOTstack (used to initialise defaults on first run, and for container self-repair). The working service definition (only relevant to old-menu, copied from \u2776). The Compose file (includes \u2776). The persistent storage area : Directories and files in \u277c are owned by userID 1883. This is enforced each time Mosquitto starts. You will normally need sudo to make changes in this area. Each time Mosquitto starts, it automatically replaces anything originating in \u2779 that has gone missing from \u277c. This \"self-repair\" function is intended to provide reasonable assurance that Mosquitto will at least start instead of going into a restart loop. How Mosquitto gets built for IOTstack \u00b6 Mosquitto source code ( GitHub ) \u00b6 The source code for Mosquitto lives at GitHub eclipse/mosquitto . Mosquitto images ( DockerHub ) \u00b6 Periodically, the source code is recompiled and the resulting image is pushed to eclipse-mosquitto on DockerHub . IOTstack menu \u00b6 When you select Mosquitto in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used. IOTstack first run \u00b6 On a first install of IOTstack, you run the menu, choose Mosquitto as one of your containers, and are told to do this: $ cd ~/IOTstack $ docker-compose up -d See also the Migration considerations (below). docker-compose reads the Compose file. When it arrives at the mosquitto fragment, it finds: mosquitto: container_name: mosquitto build: ./.templates/mosquitto/. \u2026 The build statement tells docker-compose to look for: ~/IOTstack/.templates/mosquitto/Dockerfile The Dockerfile is in the .templates directory because it is intended to be a common build for all IOTstack users. This is different to the arrangement for Node-RED where the Dockerfile is in the services directory because it is how each individual IOTstack user's version of Node-RED is customised. The Dockerfile begins with: FROM eclipse-mosquitto:latest If you need to pin to a particular version of Mosquitto, the Dockerfile is the place to do it. See Mosquitto version pinning . The FROM statement tells the build process to pull down the base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The customisations are: Add the rsync and tzdata packages. rsync helps the container perform self-repair; while tzdata enables Mosquitto to respect the \"TZ\" environment variable. Add a standard set of configuration defaults appropriate for IOTstack. Replace docker-entrypoint.sh with a version which: Calls rsync to perform self-repair if configuration files go missing; and Enforces 1883:1883 ownership in ~/IOTstack/volumes/mosquitto . The local image is instantiated to become your running container. When you run the docker images command after Mosquitto has been built, you may see two rows for Mosquitto: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_mosquitto latest cf0bfe1a34d6 4 weeks ago 11 .6MB eclipse-mosquitto latest 46ad1893f049 4 weeks ago 8 .31MB eclipse-mosquitto is the base image ; and iotstack_mosquitto is the local image . You may see the same pattern in Portainer, which reports the base image as \"unused\". You should not remove the base image, even though it appears to be unused. Whether you see one or two rows depends on the version of docker-compose you are using and how your version of docker-compose builds local images. Migration considerations \u00b6 Under the original IOTstack implementation of Mosquitto (just \"as it comes\" from DockerHub ), the service definition expected the configuration files to be at: ~/IOTstack/services/mosquitto/mosquitto.conf ~/IOTstack/services/mosquitto/filter.acl Under this implementation of Mosquitto, the configuration files have moved to: ~/IOTstack/volumes/mosquitto/config/mosquitto.conf ~/IOTstack/volumes/mosquitto/config/filter.acl The change of location is one of the things that allows self-repair to work properly. The default versions of each configuration file are the same . Only the locations have changed. If you did not alter either file when you were running the original IOTstack implementation of Mosquitto, there will be no change in Mosquitto's behaviour when it is built from a Dockerfile . However, if you did alter either or both configuration files, then you should compare the old and new versions and decide whether you wish to retain your old settings. For example: $ cd ~/IOTstack $ diff ./services/mosquitto/mosquitto.conf ./volumes/mosquitto/config/mosquitto.conf You can also use the -y option on the diff command to see a side-by-side comparison of the two files. Using mosquitto.conf as the example, assume you wish to use your existing file instead of the default: To move your existing file into the new location: $ cd ~/IOTstack $ sudo mv ./services/mosquitto/mosquitto.conf ./volumes/mosquitto/config/mosquitto.conf The move overwrites the default. At this point, the moved file will probably be owned by user \"pi\" but that does not matter. Mosquitto will always enforce correct ownership (1883:1883) on any restart but it will not overwrite permissions. If in doubt, use mode 644 as your default for permissions: $ sudo chmod 644 ./services/mosquitto/mosquitto.conf Restart Mosquitto: $ docker-compose restart mosquitto Check your work: $ ls -l ./volumes/mosquitto/config/mosquitto.conf -rw-r--r-- 1 1883 1883 ssss mmm dd hh:mm ./volumes/mosquitto/config/mosquitto.conf If necessary, repeat these steps with filter.acl . Logging \u00b6 Mosquitto logging is controlled by mosquitto.conf . This is the default configuration: #log_dest file /mosquitto/log/mosquitto.log log_dest stdout log_timestamp_format %Y-%m-%dT%H:%M:%S When log_dest is set to stdout , you inspect Mosquitto's logs like this: $ docker logs mosquitto Logs written to stdout are ephemeral and will disappear when your Mosquitto container is rebuilt, but this type of configuration reduces wear and tear on your SD card. The alternative, which may be more appropriate if you are running on an SSD or HD, is to change mosquitto.conf to be like this: log_dest file /mosquitto/log/mosquitto.log #log_dest stdout log_timestamp_format %Y-%m-%dT%H:%M:%S and then restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto The path /mosquitto/log/mosquitto.log is an internal path. When this style of logging is active, you inspect Mosquitto's logs using the external path like this: $ sudo tail ~/IOTstack/volumes/mosquitto/log/mosquitto.log You need to use sudo because the log is owned by userID 1883 and Mosquitto creates it without \"world\" read permission. Logs written to mosquitto.log do not disappear when your IOTstack is restarted. They persist until you take action to prune the file. Security \u00b6 Configuring security \u00b6 Mosquitto security is controlled by mosquitto.conf . These are the relevant directives: #password_file /mosquitto/pwfile/pwfile allow_anonymous true Mosquitto security can be in four different states, which are summarised in the following table: password_file allow_anonymous security enforcement remark disabled true open access default disabled false all access denied not really useful enabled true credentials optional enabled false credentials required Password file management \u00b6 The password file for Mosquitto is part of a mapped volume: The internal path is /mosquitto/pwfile/pwfile The external path is ~/IOTstack/volumes/mosquitto/pwfile/pwfile A common problem with the previous version of Mosquitto for IOTstack occurred when the password_file directive was enabled but the pwfile was not present. Mosquitto went into a restart loop. The Mosquitto container performs self-repair each time the container is brought up or restarts. If pwfile is missing, an empty file is created as a placeholder. This prevents the restart loop. What happens next depends on allow_anonymous : If true then: Any MQTT request without credentials will be permitted; Any MQTT request with credentials will be rejected (because pwfile is empty so there is nothing to match on). If false then all MQTT requests will be rejected. create username and password \u00b6 To create a username and password, use the following as a template. $ docker exec mosquitto mosquitto_passwd -b /mosquitto/pwfile/pwfile \u00abusername\u00bb \u00abpassword\u00bb Replace \u00abusername\u00bb and \u00abpassword\u00bb with appropriate values, then execute the command. For example, to create the username \"hello\" with password \"world\": $ docker exec mosquitto mosquitto_passwd -b /mosquitto/pwfile/pwfile hello world Note: See also customising health-check . If you are creating usernames and passwords, you may also want to create credentials for the health-check agent. check password file \u00b6 There are two ways to verify that the password file exists and has the expected content: View the file using its external path: $ sudo cat ~/IOTstack/volumes/mosquitto/pwfile/pwfile sudo is needed because the file is neither owned nor readable by pi . View the file using its internal path: $ docker exec mosquitto cat /mosquitto/pwfile/pwfile Each credential starts with the username and occupies one line in the file: hello:$7$101$ZFOHHVJLp2bcgX+h$MdHsc4rfOAhmGG+65NpIEJkxY0beNeFUyfjNAGx1ILDmI498o4cVOaD9vDmXqlGUH9g6AgHki8RPDEgjWZMkDA== remove entry from password file \u00b6 To remove an entry from the password file: $ docker exec mosquitto mosquitto_passwd -D /mosquitto/pwfile/pwfile \u00abusername\u00bb reset the password file \u00b6 There are several ways to reset the password file. Your options are: Remove the password file and restart Mosquitto: $ cd ~/IOTstack $ sudo rm ./volumes/mosquitto/pwfile/pwfile $ docker-compose restart mosquitto The result is an empty password file. Clear all existing passwords while adding a new password: $ docker exec mosquitto mosquitto_passwd -c -b /mosquitto/pwfile/pwfile \u00abusername\u00bb \u00abpassword\u00bb The result is a password file with a single entry. Clear all existing passwords in favour of a single dummy password which is then removed: $ docker exec mosquitto mosquitto_passwd -c -b /mosquitto/pwfile/pwfile dummy dummy $ docker exec mosquitto mosquitto_passwd -D /mosquitto/pwfile/pwfile dummy The result is an empty password file. Activate Mosquitto security \u00b6 Use sudo and your favourite text editor to open the following file: ~/IOTstack/volumes/mosquitto/config/mosquitto.conf Remove the comment indicator from the following line: #password_file /mosquitto/pwfile/pwfile so that it becomes: password_file /mosquitto/pwfile/pwfile Set allow_anonymous as required: allow_anonymous true If true then: Any MQTT request without credentials will be permitted; The validity of credentials supplied with any MQTT request will be enforced. If false then: Any MQTT request without credentials will be rejected; The validity of credentials supplied with any MQTT request will be enforced. Save the modified configuration file and restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto Testing Mosquitto security \u00b6 assumptions \u00b6 You have created at least one username (\"hello\") and password (\"world\"). password_file is enabled. allow_anonymous is false . install testing tools \u00b6 If you do not have the Mosquitto clients installed on your Raspberry Pi (ie $ which mosquitto_pub does not return a path), install them using: $ sudo apt install -y mosquitto-clients test: anonymous access is prohibited \u00b6 Test without providing credentials: $ mosquitto_pub -h 127.0.0.1 -p 1883 -t \"/password/test\" -m \"up up and away\" Connection Refused: not authorised. Error: The connection was refused. Note: The error is the expected result and shows that Mosquitto will not allow anonymous access. test: access with credentials is permitted \u00b6 Test with credentials $ mosquitto_pub -h 127.0.0.1 -p 1883 -t \"/password/test\" -m \"up up and away\" -u hello -P world $ Note: The absence of any error message means the message was sent. Silence = success! test: round-trip with credentials is permitted \u00b6 Prove round-trip connectivity will succeed when credentials are provided. First, set up a subscriber as a background process. This mimics the role of a process like Node-Red: $ mosquitto_sub -v -h 127.0.0.1 -p 1883 -t \"/password/test\" -F \"%I %t %p\" -u hello -P world & [1] 25996 Repeat the earlier test: $ mosquitto_pub -h 127.0.0.1 -p 1883 -t \"/password/test\" -m \"up up and away\" -u hello -P world 2021-02-16T14:40:51+1100 /password/test up up and away Note: the second line above is coming from the mosquitto_sub running in the background. When you have finished testing you can kill the background process (press return twice after you enter the kill command): $ kill %1 $ [1]+ Terminated mosquitto_sub -v -h 127.0.0.1 -p 1883 -t \"/password/test\" -F \"%I %t %p\" -u hello -P world Container health check \u00b6 theory of operation \u00b6 A script , or \"agent\", to assess the health of the Mosquitto container has been added to the local image via the Dockerfile . In other words, the script is specific to IOTstack. The agent is invoked 30 seconds after the container starts, and every 30 seconds thereafter. The agent: Publishes a retained MQTT message to the broker running in the same container. The message payload is the current date and time, and the default topic string is: iotstack/mosquitto/healthcheck Subscribes to the same broker for the same topic for a single message event. Compares the payload sent with the payload received. If the payloads (ie time-stamps) match, the agent concludes that the Mosquitto broker (the process running inside the same container) is functioning properly for round-trip messaging. monitoring health-check \u00b6 Portainer's Containers display contains a Status column which shows health-check results for all containers that support the feature. You can also use the docker ps command to monitor health-check results. The following command narrows the focus to mosquitto: $ docker ps --format \"table {{.Names}}\\t{{.Status}}\" --filter name = mosquitto Possible reply patterns are: The container is starting and has not yet run the health-check agent: NAMES STATUS mosquitto Up 3 seconds (health: starting) The container has been running for at least 30 seconds and the health-check agent has returned a positive result within the last 30 seconds: NAMES STATUS mosquitto Up 34 seconds (healthy) The container has been running for more than 90 seconds but has failed the last three successive health-check tests: NAMES STATUS mosquitto Up About a minute (unhealthy) You can also subscribe to the same topic that the health-check agent is using to view the retained messages as they are published: $ mosquitto_sub -v -h localhost -p 1883 -t \"iotstack/mosquitto/healthcheck\" -F \"%I %t %p\" Notes: This assumes you are running the command outside container-space on the same host as your Mosquitto container. If you run this command from another host, replace localhost with the IP address or domain name of the host where your Mosquitto container is running. The -p 1883 is the external port. You will need to adjust this if you are using a different external port for your MQTT service. If you enable authentication for your Mosquitto broker, you will need to add -u \u00abuser\u00bb and -P \u00abpassword\u00bb parameters to this command. You should expect to see a new message appear approximately every 30 seconds. That indicates the health-check agent is functioning normally. Use control + c to terminate the command. customising health-check \u00b6 You can customise the operation of the health-check agent by editing the mosquitto service definition in your Compose file: By default, the mosquitto broker listens to internal port 1883. If you need change that port, you also need to inform the health-check agent via an environment variable. For example, suppose you changed the internal port to 12345: environment : - HEALTHCHECK_PORT=12345 If the default topic string used by the health-check agent causes a name-space collision, you can override it. For example, you could use a Universally-Unique Identifier (UUID): environment : - HEALTHCHECK_TOPIC=4DAA361F-288C-45D5-9540-F1275BDCAF02 Note: You will also need to use the same topic string in the mosquitto_sub command shown at monitoring health-check . If you have enabled authentication for your Mosquitto broker service, you will need to provide appropriate credentials for your health-check agent: environment : - HEALTHCHECK_USER=healthyUser - HEALTHCHECK_PASSWORD=healthyUserPassword If the health-check agent misbehaves in your environment, or if you simply don't want it to be active, you can disable all health-checking for the container by adding the following lines to its service definition: healthcheck : disable : true Notes: The directives to disable health-checking are independent of the environment variables. If you want to disable health-checking temporarily, there is no need to remove any HEALTHCHECK_ environment variables that may already be in place. Conversely, the mere presence of a healthcheck: clause in the mosquitto service definition overrides the supplied agent. In other words, the following can't be used to re-enable the supplied agent: healthcheck : disable : false You must remove the entire healthcheck: clause. Upgrading Mosquitto \u00b6 You can update most containers like this: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. This strategy doesn't work when a Dockerfile is used to build a local image on top of a base image downloaded from DockerHub . The local image is what is running so there is no way for the pull to sense when a newer version becomes available. The only way to know when an update to Mosquitto is available is to check the eclipse-mosquitto tags page on DockerHub . Once a new version appears on DockerHub , you can upgrade Mosquitto like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull mosquitto $ docker-compose up -d mosquitto $ docker system prune $ docker system prune Breaking it down into parts: build causes the named container to be rebuilt; --no-cache tells the Dockerfile process that it must not take any shortcuts. It really must rebuild the local image ; --pull tells the Dockerfile process to actually check with DockerHub to see if there is a later version of the base image and, if so, to download it before starting the build; mosquitto is the named container argument required by the build command. Your existing Mosquitto container continues to run while the rebuild proceeds. Once the freshly-built local image is ready, the up tells docker-compose to do a new-for-old swap. There is barely any downtime for your MQTT broker service. The prune is the simplest way of cleaning up. The first call removes the old local image . The second call cleans up the old base image . Whether an old base image exists depends on the version of docker-compose you are using and how your version of docker-compose builds local images. Mosquitto version pinning \u00b6 If you need to pin Mosquitto to a particular version: Use your favourite text editor to open the following file: ~/IOTstack/.templates/mosquitto/Dockerfile Find the line: FROM eclipse-mosquitto:latest Replace latest with the version you wish to pin to. For example, to pin to version 2.0.10: FROM eclipse-mosquitto:2.0.10 Save the file and tell docker-compose to rebuild the local image: $ cd ~/IOTstack $ docker-compose up -d --build mosquitto $ docker system prune The new local image is built, then the new container is instantiated based on that image. The prune deletes the old local image . Note: As well as preventing Docker from updating the base image , pinning will also block incoming updates to the Dockerfile from a git pull . Nothing will change until you decide to remove the pin. About Port 9001 \u00b6 Earlier versions of the IOTstack service definition for Mosquitto included two port mappings: ports: - \"1883:1883\" - \"9001:9001\" Issue 67 explored the topic of port 9001 and showed that: The base image for Mosquitto did not expose port 9001; and The running container was not listening to port 9001. On that basis, the mapping for port 9001 was removed from service.yml . If you have a use-case that needs port 9001, you can re-enable support by: Inserting the port mapping under the mosquitto definition in docker-compose.yml : - \"9001:9001\" Inserting the additional listener in mosquitto.conf : listener 1883 listener 9001 You need both lines. If you omit 1883 then Mosquitto will stop listening to port 1883 and will only listen to port 9001. Restarting the container: $ cd ~/IOTstack $ docker-compose restart mosquitto Please consider raising an issue to document your use-case. If you think your use-case has general application then please also consider creating a pull request to make the changes permanent.","title":"Mosquitto"},{"location":"Containers/Mosquitto/#mosquitto","text":"This document discusses an IOTstack-specific version of Mosquitto built on top of Eclipse/Mosquitto using a Dockerfile . If you want the documentation for the original implementation of Mosquitto (just \"as it comes\" from DockerHub ) please see Mosquitto.md on the old-menu branch.","title":"Mosquitto"},{"location":"Containers/Mosquitto/#references","text":"Eclipse Mosquitto home GitHub : eclipse/mosquitto DockerHub : eclipse-mosquitto Setting up passwords (video) Tutorial: from MQTT to InfluxDB via Node-Red","title":"References"},{"location":"Containers/Mosquitto/#significant-directories-and-files","text":"~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 mosquitto \u2502 \u251c\u2500\u2500 service.yml \u2776 \u2502 \u251c\u2500\u2500 Dockerfile \u2777 \u2502 \u251c\u2500\u2500 docker-entrypoint.sh \u2778 \u2502 \u2514\u2500\u2500 iotstack_defaults \u2779 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u251c\u2500\u2500 filter.acl \u2502 \u2502 \u2514\u2500\u2500 mosquitto.conf \u2502 \u2514\u2500\u2500 pwfile \u2502 \u2514\u2500\u2500 pwfile \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 mosquitto \u2502 \u2514\u2500\u2500 service.yml \u277a \u251c\u2500\u2500 docker-compose.yml \u277b \u2514\u2500\u2500 volumes \u2514\u2500\u2500 mosquitto \u277c \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 filter.acl \u2502 \u2514\u2500\u2500 mosquitto.conf \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 mosquitto.db \u251c\u2500\u2500 log \u2514\u2500\u2500 pwfile \u2514\u2500\u2500 pwfile The template service definition . The Dockerfile used to customise Mosquitto for IOTstack. A replacement for the Eclipse-Mosquitto script of the same name, extended to handle container self-repair. A standard set of defaults for IOTstack (used to initialise defaults on first run, and for container self-repair). The working service definition (only relevant to old-menu, copied from \u2776). The Compose file (includes \u2776). The persistent storage area : Directories and files in \u277c are owned by userID 1883. This is enforced each time Mosquitto starts. You will normally need sudo to make changes in this area. Each time Mosquitto starts, it automatically replaces anything originating in \u2779 that has gone missing from \u277c. This \"self-repair\" function is intended to provide reasonable assurance that Mosquitto will at least start instead of going into a restart loop.","title":"Significant directories and files"},{"location":"Containers/Mosquitto/#how-mosquitto-gets-built-for-iotstack","text":"","title":"How Mosquitto gets built for IOTstack"},{"location":"Containers/Mosquitto/#mosquitto-source-code-github","text":"The source code for Mosquitto lives at GitHub eclipse/mosquitto .","title":"Mosquitto source code (GitHub)"},{"location":"Containers/Mosquitto/#mosquitto-images-dockerhub","text":"Periodically, the source code is recompiled and the resulting image is pushed to eclipse-mosquitto on DockerHub .","title":"Mosquitto images (DockerHub)"},{"location":"Containers/Mosquitto/#iotstack-menu","text":"When you select Mosquitto in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used.","title":"IOTstack menu"},{"location":"Containers/Mosquitto/#iotstack-first-run","text":"On a first install of IOTstack, you run the menu, choose Mosquitto as one of your containers, and are told to do this: $ cd ~/IOTstack $ docker-compose up -d See also the Migration considerations (below). docker-compose reads the Compose file. When it arrives at the mosquitto fragment, it finds: mosquitto: container_name: mosquitto build: ./.templates/mosquitto/. \u2026 The build statement tells docker-compose to look for: ~/IOTstack/.templates/mosquitto/Dockerfile The Dockerfile is in the .templates directory because it is intended to be a common build for all IOTstack users. This is different to the arrangement for Node-RED where the Dockerfile is in the services directory because it is how each individual IOTstack user's version of Node-RED is customised. The Dockerfile begins with: FROM eclipse-mosquitto:latest If you need to pin to a particular version of Mosquitto, the Dockerfile is the place to do it. See Mosquitto version pinning . The FROM statement tells the build process to pull down the base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The customisations are: Add the rsync and tzdata packages. rsync helps the container perform self-repair; while tzdata enables Mosquitto to respect the \"TZ\" environment variable. Add a standard set of configuration defaults appropriate for IOTstack. Replace docker-entrypoint.sh with a version which: Calls rsync to perform self-repair if configuration files go missing; and Enforces 1883:1883 ownership in ~/IOTstack/volumes/mosquitto . The local image is instantiated to become your running container. When you run the docker images command after Mosquitto has been built, you may see two rows for Mosquitto: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_mosquitto latest cf0bfe1a34d6 4 weeks ago 11 .6MB eclipse-mosquitto latest 46ad1893f049 4 weeks ago 8 .31MB eclipse-mosquitto is the base image ; and iotstack_mosquitto is the local image . You may see the same pattern in Portainer, which reports the base image as \"unused\". You should not remove the base image, even though it appears to be unused. Whether you see one or two rows depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"IOTstack first run"},{"location":"Containers/Mosquitto/#migration-considerations","text":"Under the original IOTstack implementation of Mosquitto (just \"as it comes\" from DockerHub ), the service definition expected the configuration files to be at: ~/IOTstack/services/mosquitto/mosquitto.conf ~/IOTstack/services/mosquitto/filter.acl Under this implementation of Mosquitto, the configuration files have moved to: ~/IOTstack/volumes/mosquitto/config/mosquitto.conf ~/IOTstack/volumes/mosquitto/config/filter.acl The change of location is one of the things that allows self-repair to work properly. The default versions of each configuration file are the same . Only the locations have changed. If you did not alter either file when you were running the original IOTstack implementation of Mosquitto, there will be no change in Mosquitto's behaviour when it is built from a Dockerfile . However, if you did alter either or both configuration files, then you should compare the old and new versions and decide whether you wish to retain your old settings. For example: $ cd ~/IOTstack $ diff ./services/mosquitto/mosquitto.conf ./volumes/mosquitto/config/mosquitto.conf You can also use the -y option on the diff command to see a side-by-side comparison of the two files. Using mosquitto.conf as the example, assume you wish to use your existing file instead of the default: To move your existing file into the new location: $ cd ~/IOTstack $ sudo mv ./services/mosquitto/mosquitto.conf ./volumes/mosquitto/config/mosquitto.conf The move overwrites the default. At this point, the moved file will probably be owned by user \"pi\" but that does not matter. Mosquitto will always enforce correct ownership (1883:1883) on any restart but it will not overwrite permissions. If in doubt, use mode 644 as your default for permissions: $ sudo chmod 644 ./services/mosquitto/mosquitto.conf Restart Mosquitto: $ docker-compose restart mosquitto Check your work: $ ls -l ./volumes/mosquitto/config/mosquitto.conf -rw-r--r-- 1 1883 1883 ssss mmm dd hh:mm ./volumes/mosquitto/config/mosquitto.conf If necessary, repeat these steps with filter.acl .","title":"Migration considerations"},{"location":"Containers/Mosquitto/#logging","text":"Mosquitto logging is controlled by mosquitto.conf . This is the default configuration: #log_dest file /mosquitto/log/mosquitto.log log_dest stdout log_timestamp_format %Y-%m-%dT%H:%M:%S When log_dest is set to stdout , you inspect Mosquitto's logs like this: $ docker logs mosquitto Logs written to stdout are ephemeral and will disappear when your Mosquitto container is rebuilt, but this type of configuration reduces wear and tear on your SD card. The alternative, which may be more appropriate if you are running on an SSD or HD, is to change mosquitto.conf to be like this: log_dest file /mosquitto/log/mosquitto.log #log_dest stdout log_timestamp_format %Y-%m-%dT%H:%M:%S and then restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto The path /mosquitto/log/mosquitto.log is an internal path. When this style of logging is active, you inspect Mosquitto's logs using the external path like this: $ sudo tail ~/IOTstack/volumes/mosquitto/log/mosquitto.log You need to use sudo because the log is owned by userID 1883 and Mosquitto creates it without \"world\" read permission. Logs written to mosquitto.log do not disappear when your IOTstack is restarted. They persist until you take action to prune the file.","title":"Logging"},{"location":"Containers/Mosquitto/#security","text":"","title":"Security"},{"location":"Containers/Mosquitto/#configuring-security","text":"Mosquitto security is controlled by mosquitto.conf . These are the relevant directives: #password_file /mosquitto/pwfile/pwfile allow_anonymous true Mosquitto security can be in four different states, which are summarised in the following table: password_file allow_anonymous security enforcement remark disabled true open access default disabled false all access denied not really useful enabled true credentials optional enabled false credentials required","title":"Configuring security"},{"location":"Containers/Mosquitto/#password-file-management","text":"The password file for Mosquitto is part of a mapped volume: The internal path is /mosquitto/pwfile/pwfile The external path is ~/IOTstack/volumes/mosquitto/pwfile/pwfile A common problem with the previous version of Mosquitto for IOTstack occurred when the password_file directive was enabled but the pwfile was not present. Mosquitto went into a restart loop. The Mosquitto container performs self-repair each time the container is brought up or restarts. If pwfile is missing, an empty file is created as a placeholder. This prevents the restart loop. What happens next depends on allow_anonymous : If true then: Any MQTT request without credentials will be permitted; Any MQTT request with credentials will be rejected (because pwfile is empty so there is nothing to match on). If false then all MQTT requests will be rejected.","title":"Password file management"},{"location":"Containers/Mosquitto/#create-username-and-password","text":"To create a username and password, use the following as a template. $ docker exec mosquitto mosquitto_passwd -b /mosquitto/pwfile/pwfile \u00abusername\u00bb \u00abpassword\u00bb Replace \u00abusername\u00bb and \u00abpassword\u00bb with appropriate values, then execute the command. For example, to create the username \"hello\" with password \"world\": $ docker exec mosquitto mosquitto_passwd -b /mosquitto/pwfile/pwfile hello world Note: See also customising health-check . If you are creating usernames and passwords, you may also want to create credentials for the health-check agent.","title":"create username and password"},{"location":"Containers/Mosquitto/#check-password-file","text":"There are two ways to verify that the password file exists and has the expected content: View the file using its external path: $ sudo cat ~/IOTstack/volumes/mosquitto/pwfile/pwfile sudo is needed because the file is neither owned nor readable by pi . View the file using its internal path: $ docker exec mosquitto cat /mosquitto/pwfile/pwfile Each credential starts with the username and occupies one line in the file: hello:$7$101$ZFOHHVJLp2bcgX+h$MdHsc4rfOAhmGG+65NpIEJkxY0beNeFUyfjNAGx1ILDmI498o4cVOaD9vDmXqlGUH9g6AgHki8RPDEgjWZMkDA==","title":"check password file"},{"location":"Containers/Mosquitto/#remove-entry-from-password-file","text":"To remove an entry from the password file: $ docker exec mosquitto mosquitto_passwd -D /mosquitto/pwfile/pwfile \u00abusername\u00bb","title":"remove entry from password file"},{"location":"Containers/Mosquitto/#reset-the-password-file","text":"There are several ways to reset the password file. Your options are: Remove the password file and restart Mosquitto: $ cd ~/IOTstack $ sudo rm ./volumes/mosquitto/pwfile/pwfile $ docker-compose restart mosquitto The result is an empty password file. Clear all existing passwords while adding a new password: $ docker exec mosquitto mosquitto_passwd -c -b /mosquitto/pwfile/pwfile \u00abusername\u00bb \u00abpassword\u00bb The result is a password file with a single entry. Clear all existing passwords in favour of a single dummy password which is then removed: $ docker exec mosquitto mosquitto_passwd -c -b /mosquitto/pwfile/pwfile dummy dummy $ docker exec mosquitto mosquitto_passwd -D /mosquitto/pwfile/pwfile dummy The result is an empty password file.","title":"reset the password file"},{"location":"Containers/Mosquitto/#activate-mosquitto-security","text":"Use sudo and your favourite text editor to open the following file: ~/IOTstack/volumes/mosquitto/config/mosquitto.conf Remove the comment indicator from the following line: #password_file /mosquitto/pwfile/pwfile so that it becomes: password_file /mosquitto/pwfile/pwfile Set allow_anonymous as required: allow_anonymous true If true then: Any MQTT request without credentials will be permitted; The validity of credentials supplied with any MQTT request will be enforced. If false then: Any MQTT request without credentials will be rejected; The validity of credentials supplied with any MQTT request will be enforced. Save the modified configuration file and restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto","title":"Activate Mosquitto security"},{"location":"Containers/Mosquitto/#testing-mosquitto-security","text":"","title":"Testing Mosquitto security"},{"location":"Containers/Mosquitto/#assumptions","text":"You have created at least one username (\"hello\") and password (\"world\"). password_file is enabled. allow_anonymous is false .","title":"assumptions"},{"location":"Containers/Mosquitto/#install-testing-tools","text":"If you do not have the Mosquitto clients installed on your Raspberry Pi (ie $ which mosquitto_pub does not return a path), install them using: $ sudo apt install -y mosquitto-clients","title":"install testing tools"},{"location":"Containers/Mosquitto/#test-anonymous-access-is-prohibited","text":"Test without providing credentials: $ mosquitto_pub -h 127.0.0.1 -p 1883 -t \"/password/test\" -m \"up up and away\" Connection Refused: not authorised. Error: The connection was refused. Note: The error is the expected result and shows that Mosquitto will not allow anonymous access.","title":"test: anonymous access is prohibited"},{"location":"Containers/Mosquitto/#test-access-with-credentials-is-permitted","text":"Test with credentials $ mosquitto_pub -h 127.0.0.1 -p 1883 -t \"/password/test\" -m \"up up and away\" -u hello -P world $ Note: The absence of any error message means the message was sent. Silence = success!","title":"test: access with credentials is permitted"},{"location":"Containers/Mosquitto/#test-round-trip-with-credentials-is-permitted","text":"Prove round-trip connectivity will succeed when credentials are provided. First, set up a subscriber as a background process. This mimics the role of a process like Node-Red: $ mosquitto_sub -v -h 127.0.0.1 -p 1883 -t \"/password/test\" -F \"%I %t %p\" -u hello -P world & [1] 25996 Repeat the earlier test: $ mosquitto_pub -h 127.0.0.1 -p 1883 -t \"/password/test\" -m \"up up and away\" -u hello -P world 2021-02-16T14:40:51+1100 /password/test up up and away Note: the second line above is coming from the mosquitto_sub running in the background. When you have finished testing you can kill the background process (press return twice after you enter the kill command): $ kill %1 $ [1]+ Terminated mosquitto_sub -v -h 127.0.0.1 -p 1883 -t \"/password/test\" -F \"%I %t %p\" -u hello -P world","title":"test: round-trip with credentials is permitted"},{"location":"Containers/Mosquitto/#container-health-check","text":"","title":"Container health check"},{"location":"Containers/Mosquitto/#theory-of-operation","text":"A script , or \"agent\", to assess the health of the Mosquitto container has been added to the local image via the Dockerfile . In other words, the script is specific to IOTstack. The agent is invoked 30 seconds after the container starts, and every 30 seconds thereafter. The agent: Publishes a retained MQTT message to the broker running in the same container. The message payload is the current date and time, and the default topic string is: iotstack/mosquitto/healthcheck Subscribes to the same broker for the same topic for a single message event. Compares the payload sent with the payload received. If the payloads (ie time-stamps) match, the agent concludes that the Mosquitto broker (the process running inside the same container) is functioning properly for round-trip messaging.","title":"theory of operation"},{"location":"Containers/Mosquitto/#monitoring-health-check","text":"Portainer's Containers display contains a Status column which shows health-check results for all containers that support the feature. You can also use the docker ps command to monitor health-check results. The following command narrows the focus to mosquitto: $ docker ps --format \"table {{.Names}}\\t{{.Status}}\" --filter name = mosquitto Possible reply patterns are: The container is starting and has not yet run the health-check agent: NAMES STATUS mosquitto Up 3 seconds (health: starting) The container has been running for at least 30 seconds and the health-check agent has returned a positive result within the last 30 seconds: NAMES STATUS mosquitto Up 34 seconds (healthy) The container has been running for more than 90 seconds but has failed the last three successive health-check tests: NAMES STATUS mosquitto Up About a minute (unhealthy) You can also subscribe to the same topic that the health-check agent is using to view the retained messages as they are published: $ mosquitto_sub -v -h localhost -p 1883 -t \"iotstack/mosquitto/healthcheck\" -F \"%I %t %p\" Notes: This assumes you are running the command outside container-space on the same host as your Mosquitto container. If you run this command from another host, replace localhost with the IP address or domain name of the host where your Mosquitto container is running. The -p 1883 is the external port. You will need to adjust this if you are using a different external port for your MQTT service. If you enable authentication for your Mosquitto broker, you will need to add -u \u00abuser\u00bb and -P \u00abpassword\u00bb parameters to this command. You should expect to see a new message appear approximately every 30 seconds. That indicates the health-check agent is functioning normally. Use control + c to terminate the command.","title":"monitoring health-check"},{"location":"Containers/Mosquitto/#customising-health-check","text":"You can customise the operation of the health-check agent by editing the mosquitto service definition in your Compose file: By default, the mosquitto broker listens to internal port 1883. If you need change that port, you also need to inform the health-check agent via an environment variable. For example, suppose you changed the internal port to 12345: environment : - HEALTHCHECK_PORT=12345 If the default topic string used by the health-check agent causes a name-space collision, you can override it. For example, you could use a Universally-Unique Identifier (UUID): environment : - HEALTHCHECK_TOPIC=4DAA361F-288C-45D5-9540-F1275BDCAF02 Note: You will also need to use the same topic string in the mosquitto_sub command shown at monitoring health-check . If you have enabled authentication for your Mosquitto broker service, you will need to provide appropriate credentials for your health-check agent: environment : - HEALTHCHECK_USER=healthyUser - HEALTHCHECK_PASSWORD=healthyUserPassword If the health-check agent misbehaves in your environment, or if you simply don't want it to be active, you can disable all health-checking for the container by adding the following lines to its service definition: healthcheck : disable : true Notes: The directives to disable health-checking are independent of the environment variables. If you want to disable health-checking temporarily, there is no need to remove any HEALTHCHECK_ environment variables that may already be in place. Conversely, the mere presence of a healthcheck: clause in the mosquitto service definition overrides the supplied agent. In other words, the following can't be used to re-enable the supplied agent: healthcheck : disable : false You must remove the entire healthcheck: clause.","title":"customising health-check"},{"location":"Containers/Mosquitto/#upgrading-mosquitto","text":"You can update most containers like this: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. This strategy doesn't work when a Dockerfile is used to build a local image on top of a base image downloaded from DockerHub . The local image is what is running so there is no way for the pull to sense when a newer version becomes available. The only way to know when an update to Mosquitto is available is to check the eclipse-mosquitto tags page on DockerHub . Once a new version appears on DockerHub , you can upgrade Mosquitto like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull mosquitto $ docker-compose up -d mosquitto $ docker system prune $ docker system prune Breaking it down into parts: build causes the named container to be rebuilt; --no-cache tells the Dockerfile process that it must not take any shortcuts. It really must rebuild the local image ; --pull tells the Dockerfile process to actually check with DockerHub to see if there is a later version of the base image and, if so, to download it before starting the build; mosquitto is the named container argument required by the build command. Your existing Mosquitto container continues to run while the rebuild proceeds. Once the freshly-built local image is ready, the up tells docker-compose to do a new-for-old swap. There is barely any downtime for your MQTT broker service. The prune is the simplest way of cleaning up. The first call removes the old local image . The second call cleans up the old base image . Whether an old base image exists depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"Upgrading Mosquitto"},{"location":"Containers/Mosquitto/#mosquitto-version-pinning","text":"If you need to pin Mosquitto to a particular version: Use your favourite text editor to open the following file: ~/IOTstack/.templates/mosquitto/Dockerfile Find the line: FROM eclipse-mosquitto:latest Replace latest with the version you wish to pin to. For example, to pin to version 2.0.10: FROM eclipse-mosquitto:2.0.10 Save the file and tell docker-compose to rebuild the local image: $ cd ~/IOTstack $ docker-compose up -d --build mosquitto $ docker system prune The new local image is built, then the new container is instantiated based on that image. The prune deletes the old local image . Note: As well as preventing Docker from updating the base image , pinning will also block incoming updates to the Dockerfile from a git pull . Nothing will change until you decide to remove the pin.","title":"Mosquitto version pinning"},{"location":"Containers/Mosquitto/#about-port-9001","text":"Earlier versions of the IOTstack service definition for Mosquitto included two port mappings: ports: - \"1883:1883\" - \"9001:9001\" Issue 67 explored the topic of port 9001 and showed that: The base image for Mosquitto did not expose port 9001; and The running container was not listening to port 9001. On that basis, the mapping for port 9001 was removed from service.yml . If you have a use-case that needs port 9001, you can re-enable support by: Inserting the port mapping under the mosquitto definition in docker-compose.yml : - \"9001:9001\" Inserting the additional listener in mosquitto.conf : listener 1883 listener 9001 You need both lines. If you omit 1883 then Mosquitto will stop listening to port 1883 and will only listen to port 9001. Restarting the container: $ cd ~/IOTstack $ docker-compose restart mosquitto Please consider raising an issue to document your use-case. If you think your use-case has general application then please also consider creating a pull request to make the changes permanent.","title":"About Port 9001"},{"location":"Containers/MotionEye/","text":"MotionEye \u00b6 References \u00b6 Website About \u00b6 MotionEye is a camera/webcam package. The port is set to 8765 Config \u00b6 This is the yml entry. Notice that the devices is commented out. This is because if you don't have a camera attached then it will fail to start. Uncomment if you need to. This is for a Pi camera, you will need to add additional lines for usb cameras motioneye: image: \"ccrisan/motioneye:master-armhf\" container_name: \"motioneye\" restart: unless-stopped ports: - 8765:8765 - 8081:8081 volumes: - /etc/localtime:/etc/localtime:ro - ./volumes/motioneye/etc_motioneye:/etc/motioneye - ./volumes/motioneye/var_lib_motioneye:/var/lib/motioneye #devices: # - \"/dev/video0:/dev/video0\" Login Details \u00b6 On first login you will be asked for login details. The default user is admin (all lowercase) with no password Storage \u00b6 By default local camera data will be stored in /var/lib/motioneye/camera_name in the container which equates to the following: Remote motioneye \u00b6 If you have connected to a remote motion eye note that the directory is on that device and has nothing to do with the container.","title":"MotionEye"},{"location":"Containers/MotionEye/#motioneye","text":"","title":"MotionEye"},{"location":"Containers/MotionEye/#references","text":"Website","title":"References"},{"location":"Containers/MotionEye/#about","text":"MotionEye is a camera/webcam package. The port is set to 8765","title":"About"},{"location":"Containers/MotionEye/#config","text":"This is the yml entry. Notice that the devices is commented out. This is because if you don't have a camera attached then it will fail to start. Uncomment if you need to. This is for a Pi camera, you will need to add additional lines for usb cameras motioneye: image: \"ccrisan/motioneye:master-armhf\" container_name: \"motioneye\" restart: unless-stopped ports: - 8765:8765 - 8081:8081 volumes: - /etc/localtime:/etc/localtime:ro - ./volumes/motioneye/etc_motioneye:/etc/motioneye - ./volumes/motioneye/var_lib_motioneye:/var/lib/motioneye #devices: # - \"/dev/video0:/dev/video0\"","title":"Config"},{"location":"Containers/MotionEye/#login-details","text":"On first login you will be asked for login details. The default user is admin (all lowercase) with no password","title":"Login Details"},{"location":"Containers/MotionEye/#storage","text":"By default local camera data will be stored in /var/lib/motioneye/camera_name in the container which equates to the following:","title":"Storage"},{"location":"Containers/MotionEye/#remote-motioneye","text":"If you have connected to a remote motion eye note that the directory is on that device and has nothing to do with the container.","title":"Remote motioneye"},{"location":"Containers/NextCloud/","text":"Nextcloud \u00b6 Service definition \u00b6 This is the core of the IOTstack Nextcloud service definition: nextcloud: container_name: nextcloud image: nextcloud restart: unless-stopped environment: - MYSQL_HOST=nextcloud_db - MYSQL_PASSWORD=\u00abuser_password\u00bb - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud ports: - \"9321:80\" volumes: - ./volumes/nextcloud/html:/var/www/html depends_on: - nextcloud_db nextcloud_db: container_name: nextcloud_db build: ./.templates/mariadb/. restart: unless-stopped environment: - TZ=Etc/UTC - PUID=1000 - PGID=1000 - MYSQL_ROOT_PASSWORD=\u00abroot_password\u00bb - MYSQL_PASSWORD=\u00abuser_password\u00bb - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud volumes: - ./volumes/nextcloud/db:/config - ./volumes/nextcloud/db_backup:/backup There are two containers, one for the cloud service itself, and the other for the database. Both containers share the same persistent storage area in the volumes subdirectory so they are treated as a unit. This will not interfere with any other MariaDB containers you might wish to run. Depending on the IOTstack branch you are running, there may also be networks: directives. Other than to note that new menu dedicates a network to inter-container communications, those directives make no difference for this discussion. Under old-menu, you are responsible for setting passwords. The passwords are \"internal use only\" and it is unlikely that you will need them unless you plan to go ferreting-about in the database using SQL. The rules are: The two instances of \u00abuser_password\u00bb must be the same. The instance of \u00abroot_password\u00bb should be different from \u00abuser_password\u00bb . Under new-menu, the menu can generate random passwords for you. You can either use that feature or roll your own using the old-menu approach by replacing: Two instances of %randomMySqlPassword% (the \u00abuser_password\u00bb ) One instance of %randomPassword% (the \u00abroot_password\u00bb ) The passwords need to be set before you bring up the Nextcloud service for the first time but the following initialisation steps assume you might not have done that and always start over from a clean slate. Initialising Nextcloud \u00b6 Be in the correct directory: $ cd ~/IOTstack If the stack is running, take it down: $ docker-compose down Erase the persistent storage area for Nextcloud (double-check the command before you hit return): $ sudo rm -rf ./volumes/nextcloud This is done to force re-initialisation. In particular, it gives you assurance that the passwords in your docker-compose.yml are the ones that are actually in effect. Bring up the stack: $ docker-compose up -d Check for errors: Repeat the following command two or three times at 10-second intervals: $ docker ps You are looking for evidence that the nextcloud and nextcloud_db containers are up, stable, and not restarting. If you see any evidence of restarts, try to figure out why using: $ docker logs nextcloud If you want to be sure Nextcloud gets set up correctly, it is best to perform the remaining steps from a different computer. That means you need to decide how that other computer will refer to your Raspberry Pi running Nextcloud. Your choices are: the IP address of your Raspberry Pi \u2013 eg 192.168.203.200 your Raspberry Pi's fully-qualified domain name \u2013 eg myrpi.mydomain.com your Raspberry Pi's host name \u2013 eg myrpi Key points: You can't use a multicast domain name (eg myrpi.local ). An mDNS name will not work until Nextcloud has been initialised! Once you have picked a connection method, STICK TO IT . You are only stuck with this restriction until Nextcloud has been initialised. You can (and should) fix it later by completing the steps in \"Access through untrusted domain\" . On a computer that is not the Raspberry Pi running Nextcloud, launch a browser and point to the Raspberry Pi running Nextcloud using your chosen connection method. Examples: If you are using an IP address: http://192.168.203.200:9321 If you are using a domain name: http://myrpi.mydomain.com:9321 If you are using a host name in /etc/hosts : http://myrpi:9321 The expected result is: Create an administrator account and then click \"Finish Setup\". There is a long delay. In most cases, the \"Recommended apps\" screen appears and you can ignored the instructions in this section. However, if you see the following error: then you should: Examine the contents of your browser's URL bar. If you see this pattern: http://localhost/index.php/core/apps/recommended Edit the URL to replace localhost with what it should be, which will be one of the following patterns, depending on which method you chose to access Nextcloud: http://192.168.203.200:9321/index.php/core/apps/recommended http://myrpi.mydomain.com:9321/index.php/core/apps/recommended http://myrpi:9321/index.php/core/apps/recommended Note: This seems to be the only time Nextcloud misbehaves and forces localhost into a URL. The \"Recommended apps\" screen appears. A spinner moves down the list of apps as they are loaded: Wait for the loading to complete. Eventually, the dashboard will appear. Then the dashboard will be obscured by the \"Nextcloud Hub\" floating window: Hover your mouse to the right of the floating window and keep clicking on the right-arrow button until you reach the last screen, then click \"Start using Nextcloud\". Congratulations. Your IOTstack implementation of Nextcloud is ready to roll: \"Access through untrusted domain\" \u00b6 During Nextcloud initialisation you had to choose between an IP address, a domain name or a host name. Now that Nextcloud is running, you have the opportunity to expand your connection options. If you are reading this because you are staring at an \"access through untrusted domain\" message then you have come to the right place. Let's assume the following: You used raspi-config to give your Raspberry Pi the name \"myrpi\". Your Raspberry Pi has the fixed IP address \"192.168.203.200\" (via either a static binding in your DHCP server or a static IP address on your Raspberry Pi). Out of the box, a Raspberry Pi participates in multicast DNS so it will also have the mDNS name: \"myrpi.local\" Let's also assume you have a local Domain Name System server where your Raspberry Pi: has the canonical name (A record) \"myrpi.mydomain.com\"; plus an alias (CNAME record) of \"nextcloud.mydomain.com\". Rolling all that together, you would expect your Nextcloud service to be reachable at any of the following URLs: http://192.168.203.200:9321 http://myrpi.local:9321 http://myrpi.mydomain.com:9321 http://nextcloud.mydomain.com:9321 To tell Nextcloud that all of those URLs are valid, you need to use sudo and your favourite text editor to edit this file: ~/IOTstack/volumes/nextcloud/html/config/config.php Hint: It is a good idea to make a backup of any file before you edit it. For example: $ cd ~/IOTstack/volumes/nextcloud/html/config/ $ sudo cp config.php config.php.bak Search for \"trusted_domains\". To tell Nextcloud to trust all of the URLs above, edit the array structure like this: 'trusted_domains' => array ( 0 => '192.168.203.200:9321', 1 => 'myrpi.local:9321', 2 => 'myrpi.mydomain.com:9321', 3 => 'nextcloud.mydomain.com:9321', ), Note: all the trailing commas are intentional! Once you have finished editing the file, save your work then restart Nextcloud: $ cd ~/IOTstack $ docker-compose restart nextcloud Use docker ps to check that the container has restarted properly and hasn't gone into a restart loop. See also: Nextcloud documentation - trusted domains . Using a DNS alias for your Nextcloud service \u00b6 The examples above include using a DNS alias (a CNAME record) for your Nextcloud service. If you decide to do that, you may see this warning in the log: Could not reliably determine the server's fully qualified domain name You can silence the warning by editing the Nextcloud service definition in docker-compose.yml to add your fully-qualified DNS alias to at hostname directive. For example: hostname: nextcloud.mydomain.com Security considerations \u00b6 Nextcloud traffic is not encrypted. Do not expose it to the web by opening a port on your home router. Instead, use a VPN like Wireguard to provide secure access to your home network, and let your remote clients access Nextcloud over the VPN tunnel. Container health check \u00b6 A script , or \"agent\", to assess the health of the MariaDB container has been added to the local image via the Dockerfile . In other words, the script is specific to IOTstack. Because it is an instance of MariaDB, Nextcloud_DB inherits the health-check agent. See the IOTstack MariaDB documentation for more information. Keeping Nextcloud up-to-date \u00b6 To update the nextcloud container: $ cd ~/IOTstack $ docker-compose pull nextcloud $ docker-compose up -d nextcloud $ docker system prune To update the nextcloud_db container: $ cd ~/IOTstack $ docker-compose build --no-cache --pull nextcloud_db $ docker-compose up -d nextcloud_db $ docker system prune $ docker system prune The first \"prune\" removes the old local image, the second removes the old base image. Whether an old base image exists depends on the version of docker-compose you are using and how your version of docker-compose builds local images. Backups \u00b6 Nextcloud is currently excluded from the IOTstack-supplied backup scripts due to its potential size. This is also true for Paraphraser/IOTstackBackup . If you want to take a backup, something like the following will get the job done: $ cd ~/IOTstack $ BACKUP_TAR_GZ=$PWD/backups/$(date +\"%Y-%m-%d_%H%M\").$HOSTNAME.nextcloud-backup.tar.gz $ touch \"$BACKUP_TAR_GZ\" $ docker-compose rm --force --stop -v nextcloud nextcloud_db $ sudo tar -czf \"$BACKUP_TAR_GZ\" -C \"./volumes/nextcloud\" . $ docker-compose up -d nextcloud Notes: A baseline backup takes over 400MB and about 2 minutes. Once you start adding your own data, it will take even more time and storage. The up of the NextCloud container implies the up of the Nextcloud_DB container. To restore, you first need to identify the name of the backup file by looking in the backups directory. Then: $ cd ~/IOTstack $ RESTORE_TAR_GZ=$PWD/backups/2021-06-12_1321.sec-dev.nextcloud-backup.tar.gz $ docker-compose rm --force --stop -v nextcloud nextcloud_db $ sudo rm -rf ./volumes/nextcloud/* $ sudo tar -x --same-owner -z -f \"$RESTORE_TAR_GZ\" -C \"./volumes/nextcloud\" $ docker-compose up -d nextcloud If you are running from an SD card, it would be a good idea to mount an external drive to store the data. Something like: The external drive will have to be an ext4 formatted drive because smb, fat32 and NTFS can't handle Linux file permissions. If the permissions aren't set to \"www-data\" then the container won't be able to write to the disk. Finally, a warning: If your database gets corrupted then your Nextcloud is pretty much stuffed.","title":"Nextcloud"},{"location":"Containers/NextCloud/#nextcloud","text":"","title":"Nextcloud"},{"location":"Containers/NextCloud/#service-definition","text":"This is the core of the IOTstack Nextcloud service definition: nextcloud: container_name: nextcloud image: nextcloud restart: unless-stopped environment: - MYSQL_HOST=nextcloud_db - MYSQL_PASSWORD=\u00abuser_password\u00bb - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud ports: - \"9321:80\" volumes: - ./volumes/nextcloud/html:/var/www/html depends_on: - nextcloud_db nextcloud_db: container_name: nextcloud_db build: ./.templates/mariadb/. restart: unless-stopped environment: - TZ=Etc/UTC - PUID=1000 - PGID=1000 - MYSQL_ROOT_PASSWORD=\u00abroot_password\u00bb - MYSQL_PASSWORD=\u00abuser_password\u00bb - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud volumes: - ./volumes/nextcloud/db:/config - ./volumes/nextcloud/db_backup:/backup There are two containers, one for the cloud service itself, and the other for the database. Both containers share the same persistent storage area in the volumes subdirectory so they are treated as a unit. This will not interfere with any other MariaDB containers you might wish to run. Depending on the IOTstack branch you are running, there may also be networks: directives. Other than to note that new menu dedicates a network to inter-container communications, those directives make no difference for this discussion. Under old-menu, you are responsible for setting passwords. The passwords are \"internal use only\" and it is unlikely that you will need them unless you plan to go ferreting-about in the database using SQL. The rules are: The two instances of \u00abuser_password\u00bb must be the same. The instance of \u00abroot_password\u00bb should be different from \u00abuser_password\u00bb . Under new-menu, the menu can generate random passwords for you. You can either use that feature or roll your own using the old-menu approach by replacing: Two instances of %randomMySqlPassword% (the \u00abuser_password\u00bb ) One instance of %randomPassword% (the \u00abroot_password\u00bb ) The passwords need to be set before you bring up the Nextcloud service for the first time but the following initialisation steps assume you might not have done that and always start over from a clean slate.","title":"Service definition"},{"location":"Containers/NextCloud/#initialising-nextcloud","text":"Be in the correct directory: $ cd ~/IOTstack If the stack is running, take it down: $ docker-compose down Erase the persistent storage area for Nextcloud (double-check the command before you hit return): $ sudo rm -rf ./volumes/nextcloud This is done to force re-initialisation. In particular, it gives you assurance that the passwords in your docker-compose.yml are the ones that are actually in effect. Bring up the stack: $ docker-compose up -d Check for errors: Repeat the following command two or three times at 10-second intervals: $ docker ps You are looking for evidence that the nextcloud and nextcloud_db containers are up, stable, and not restarting. If you see any evidence of restarts, try to figure out why using: $ docker logs nextcloud If you want to be sure Nextcloud gets set up correctly, it is best to perform the remaining steps from a different computer. That means you need to decide how that other computer will refer to your Raspberry Pi running Nextcloud. Your choices are: the IP address of your Raspberry Pi \u2013 eg 192.168.203.200 your Raspberry Pi's fully-qualified domain name \u2013 eg myrpi.mydomain.com your Raspberry Pi's host name \u2013 eg myrpi Key points: You can't use a multicast domain name (eg myrpi.local ). An mDNS name will not work until Nextcloud has been initialised! Once you have picked a connection method, STICK TO IT . You are only stuck with this restriction until Nextcloud has been initialised. You can (and should) fix it later by completing the steps in \"Access through untrusted domain\" . On a computer that is not the Raspberry Pi running Nextcloud, launch a browser and point to the Raspberry Pi running Nextcloud using your chosen connection method. Examples: If you are using an IP address: http://192.168.203.200:9321 If you are using a domain name: http://myrpi.mydomain.com:9321 If you are using a host name in /etc/hosts : http://myrpi:9321 The expected result is: Create an administrator account and then click \"Finish Setup\". There is a long delay. In most cases, the \"Recommended apps\" screen appears and you can ignored the instructions in this section. However, if you see the following error: then you should: Examine the contents of your browser's URL bar. If you see this pattern: http://localhost/index.php/core/apps/recommended Edit the URL to replace localhost with what it should be, which will be one of the following patterns, depending on which method you chose to access Nextcloud: http://192.168.203.200:9321/index.php/core/apps/recommended http://myrpi.mydomain.com:9321/index.php/core/apps/recommended http://myrpi:9321/index.php/core/apps/recommended Note: This seems to be the only time Nextcloud misbehaves and forces localhost into a URL. The \"Recommended apps\" screen appears. A spinner moves down the list of apps as they are loaded: Wait for the loading to complete. Eventually, the dashboard will appear. Then the dashboard will be obscured by the \"Nextcloud Hub\" floating window: Hover your mouse to the right of the floating window and keep clicking on the right-arrow button until you reach the last screen, then click \"Start using Nextcloud\". Congratulations. Your IOTstack implementation of Nextcloud is ready to roll:","title":"Initialising Nextcloud"},{"location":"Containers/NextCloud/#access-through-untrusted-domain","text":"During Nextcloud initialisation you had to choose between an IP address, a domain name or a host name. Now that Nextcloud is running, you have the opportunity to expand your connection options. If you are reading this because you are staring at an \"access through untrusted domain\" message then you have come to the right place. Let's assume the following: You used raspi-config to give your Raspberry Pi the name \"myrpi\". Your Raspberry Pi has the fixed IP address \"192.168.203.200\" (via either a static binding in your DHCP server or a static IP address on your Raspberry Pi). Out of the box, a Raspberry Pi participates in multicast DNS so it will also have the mDNS name: \"myrpi.local\" Let's also assume you have a local Domain Name System server where your Raspberry Pi: has the canonical name (A record) \"myrpi.mydomain.com\"; plus an alias (CNAME record) of \"nextcloud.mydomain.com\". Rolling all that together, you would expect your Nextcloud service to be reachable at any of the following URLs: http://192.168.203.200:9321 http://myrpi.local:9321 http://myrpi.mydomain.com:9321 http://nextcloud.mydomain.com:9321 To tell Nextcloud that all of those URLs are valid, you need to use sudo and your favourite text editor to edit this file: ~/IOTstack/volumes/nextcloud/html/config/config.php Hint: It is a good idea to make a backup of any file before you edit it. For example: $ cd ~/IOTstack/volumes/nextcloud/html/config/ $ sudo cp config.php config.php.bak Search for \"trusted_domains\". To tell Nextcloud to trust all of the URLs above, edit the array structure like this: 'trusted_domains' => array ( 0 => '192.168.203.200:9321', 1 => 'myrpi.local:9321', 2 => 'myrpi.mydomain.com:9321', 3 => 'nextcloud.mydomain.com:9321', ), Note: all the trailing commas are intentional! Once you have finished editing the file, save your work then restart Nextcloud: $ cd ~/IOTstack $ docker-compose restart nextcloud Use docker ps to check that the container has restarted properly and hasn't gone into a restart loop. See also: Nextcloud documentation - trusted domains .","title":"\"Access through untrusted domain\""},{"location":"Containers/NextCloud/#using-a-dns-alias-for-your-nextcloud-service","text":"The examples above include using a DNS alias (a CNAME record) for your Nextcloud service. If you decide to do that, you may see this warning in the log: Could not reliably determine the server's fully qualified domain name You can silence the warning by editing the Nextcloud service definition in docker-compose.yml to add your fully-qualified DNS alias to at hostname directive. For example: hostname: nextcloud.mydomain.com","title":"Using a DNS alias for your Nextcloud service"},{"location":"Containers/NextCloud/#security-considerations","text":"Nextcloud traffic is not encrypted. Do not expose it to the web by opening a port on your home router. Instead, use a VPN like Wireguard to provide secure access to your home network, and let your remote clients access Nextcloud over the VPN tunnel.","title":" Security considerations"},{"location":"Containers/NextCloud/#container-health-check","text":"A script , or \"agent\", to assess the health of the MariaDB container has been added to the local image via the Dockerfile . In other words, the script is specific to IOTstack. Because it is an instance of MariaDB, Nextcloud_DB inherits the health-check agent. See the IOTstack MariaDB documentation for more information.","title":"Container health check"},{"location":"Containers/NextCloud/#keeping-nextcloud-up-to-date","text":"To update the nextcloud container: $ cd ~/IOTstack $ docker-compose pull nextcloud $ docker-compose up -d nextcloud $ docker system prune To update the nextcloud_db container: $ cd ~/IOTstack $ docker-compose build --no-cache --pull nextcloud_db $ docker-compose up -d nextcloud_db $ docker system prune $ docker system prune The first \"prune\" removes the old local image, the second removes the old base image. Whether an old base image exists depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"Keeping Nextcloud up-to-date"},{"location":"Containers/NextCloud/#backups","text":"Nextcloud is currently excluded from the IOTstack-supplied backup scripts due to its potential size. This is also true for Paraphraser/IOTstackBackup . If you want to take a backup, something like the following will get the job done: $ cd ~/IOTstack $ BACKUP_TAR_GZ=$PWD/backups/$(date +\"%Y-%m-%d_%H%M\").$HOSTNAME.nextcloud-backup.tar.gz $ touch \"$BACKUP_TAR_GZ\" $ docker-compose rm --force --stop -v nextcloud nextcloud_db $ sudo tar -czf \"$BACKUP_TAR_GZ\" -C \"./volumes/nextcloud\" . $ docker-compose up -d nextcloud Notes: A baseline backup takes over 400MB and about 2 minutes. Once you start adding your own data, it will take even more time and storage. The up of the NextCloud container implies the up of the Nextcloud_DB container. To restore, you first need to identify the name of the backup file by looking in the backups directory. Then: $ cd ~/IOTstack $ RESTORE_TAR_GZ=$PWD/backups/2021-06-12_1321.sec-dev.nextcloud-backup.tar.gz $ docker-compose rm --force --stop -v nextcloud nextcloud_db $ sudo rm -rf ./volumes/nextcloud/* $ sudo tar -x --same-owner -z -f \"$RESTORE_TAR_GZ\" -C \"./volumes/nextcloud\" $ docker-compose up -d nextcloud If you are running from an SD card, it would be a good idea to mount an external drive to store the data. Something like: The external drive will have to be an ext4 formatted drive because smb, fat32 and NTFS can't handle Linux file permissions. If the permissions aren't set to \"www-data\" then the container won't be able to write to the disk. Finally, a warning: If your database gets corrupted then your Nextcloud is pretty much stuffed.","title":"Backups"},{"location":"Containers/Node-RED/","text":"Node-RED \u00b6 References \u00b6 nodered.org home GitHub: node-red/node-red-docker DockerHub: nodered/node-red Tutorial: from MQTT to InfluxDB via Node-Red Significant files \u00b6 ~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 nodered \u2502 \u2514\u2500\u2500 service.yml \u2776 \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 nodered \u2502 \u251c\u2500\u2500 Dockerfile \u2777 \u2502 \u2514\u2500\u2500 service.yml \u2778 \u251c\u2500\u2500 docker-compose.yml \u2779 \u2514\u2500\u2500 volumes \u2514\u2500\u2500 nodered \u277a \u251c\u2500\u2500 data \u277b \u2514\u2500\u2500 ssh \u277c Template service definition . The Dockerfile . Working service definition (old-menu only, copied from \u2776). The Compose file (includes \u2776) Persistent storage area. Data directory (mapped volume). SSH directory (mapped volume). How Node-RED gets built for IOTstack \u00b6 Node-RED source code ( GitHub ) \u00b6 The source code for Node-RED lives at GitHub node-red/node-red-docker . Node-RED images ( DockerHub ) \u00b6 Periodically, the source code is recompiled and pushed to nodered/node-red on DockerHub . There's a lot of stuff at that page but it boils down to variations on two basic themes: images with a suffix like -10 , -12 or -14 ; and images without a numeric suffix. The suffixes refer to the version of \"Node.js\" installed when the image was built. In the case of images without a numeric suffix: If the version of Node-RED is 1.3.x then a suffix of -10 is implied. In other words, Node.js defaults to version 10. If the version of Node-RED is 2.x.x then a suffix of -14 is implied. In other words, Node.js defaults to version 14. As you will see a bit further down, the current default for IOTstack is an image tag of latest-12 which means Node-RED 2.x.x with Node.js version 12. IOTstack menu \u00b6 When you select Node-RED in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used. You choose add-on nodes from a supplementary menu. We recommend accepting the default nodes, and adding others that you think you are likely to need. Node-RED will not build if you do not select at least one add-on node. Key points: Under new menu, you must press the right arrow to access the supplementary menu. Under old menu, the list of add-on nodes is displayed automatically. Do not be concerned if you can't find an add-on node you need in the list. You can also add nodes via Manage Palette once Node-RED is running. See adding extra nodes . Choosing add-on nodes in the menu causes the Dockerfile to be created. IOTstack first run \u00b6 On a first install of IOTstack, you are told to do this: $ cd ~/IOTstack $ docker-compose up -d docker-compose reads the Compose file. When it arrives at the nodered fragment, it finds: nodered: container_name: nodered build: ./services/nodered/. \u2026 The build statement tells docker-compose to look for: ~/IOTstack/services/nodered/Dockerfile The Dockerfile begins with: FROM nodered/node-red:latest-12 Note: IOTstack switched to the -12 suffix in March 2021. Existing IOTstack installations will not update unless you re-create the service from its template, or hand-edit the Dockerfile . The FROM statement tells the build process to pull down the latest base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The local image is instantiated to become your running container. Notes: During the build you may see warnings and deprecation notices. You may also see messages about \"vulnerabilities\" along with recommendations to run npm audit fix . You should ignore all such messages. There is no need to take any action. If SQLite is in your list of nodes, be aware that it needs to be compiled from its source code. It takes a long time, outputs an astonishing number of warnings and, from time to time, will look as if it has gotten stuck. Be patient. Acknowledgement: Successful installation of the SQLite node is thanks to @fragolinux. When you run the docker images command after Node-RED has been built, you may see two rows for Node-RED: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_nodered latest b0b21a97b8bb 4 days ago 462MB nodered/node-red latest deb99584fa75 5 days ago 385MB nodered/node-red is the base image ; and iotstack_nodered is the local image . The local image is the one that is instantiated to become the running container. You may see the same pattern in Portainer, which reports the base image as \"unused\": You should not remove the base image, even though it appears to be unused. Whether you see one or two rows depends on the version of docker-compose you are using and how your version of docker-compose builds local images. Securing Node-RED \u00b6 Setting an encryption key for your credentials \u00b6 After you install Node-RED, you should set an encryption key. Completing this step will silence the warning you will see when you run: $ docker logs nodered \u2026 --------------------------------------------------------------------- Your flow credentials file is encrypted using a system-generated key. If the system-generated key is lost for any reason, your credentials file will not be recoverable, you will have to delete it and re-enter your credentials. You should set your own key using the 'credentialSecret' option in your settings file. Node-RED will then re-encrypt your credentials file using your chosen key the next time you deploy a change. --------------------------------------------------------------------- \u2026 Setting an encryption key also means that any credentials you create will be portable , in the sense that you can backup Node-RED on one machine and restore it on another. The encryption key can be any string. For example, if you have UUID support installed ( sudo apt install -y uuid-runtime ), you could generate a UUID as your key: $ uuidgen 2deb50d4-38f5-4ab3-a97e-d59741802e2d Once you have defined your encryption key, use sudo and your favourite text editor to open this file: ~/IOTstack/volumes/nodered/data/settings.js Search for credentialSecret : //credentialSecret: \"a-secret-key\", Un-comment the line and replace a-secret-key with your chosen key. Do not remove the comma at the end of the line. The result should look something like this: credentialSecret: \"2deb50d4-38f5-4ab3-a97e-d59741802e2d\", Save the file and then restart Node-RED: $ cd ~/IOTstack $ docker-compose restart nodered Setting a username and password for Node-RED \u00b6 To secure Node-RED you need a password hash. Run the following command, replacing PASSWORD with your own password: $ docker exec nodered node -e \"console.log(require('bcryptjs').hashSync(process.argv[1], 8));\" PASSWORD You will get an answer that looks something like this: $2a$08$gTdx7SkckJVCw1U98o4r0O7b8P.gd5/LAPlZI6geg5LRg4AUKuDhS Copy that text to your clipboard, then follow the instructions at Node-RED User Guide - Securing Node-RED - Username & Password-based authentication . Referring to other containers \u00b6 Node-RED can run in two modes. By default, it runs in \"non-host mode\" but you can also move the container to \"host mode\" by editing the Node-RED service definition in your Compose file to: Add the following directive: network_mode: host Remove the ports directive and the mapping of port 1880. When Node-RED is not in host mode \u00b6 Most examples on the web assume Node-RED and other services in the MING (Mosquitto, InfluxDB, Node-RED, Grafana) stack have been installed natively, rather than in Docker containers. Those examples typically include the loopback address + port syntax, like this: 127.0.0.1:1883 The loopback address will not work when Node-RED is in non-host mode. This is because each container behaves like a self-contained computer. The loopback address means \"this container\". It does not mean \"this Raspberry Pi\". You refer to other containers by their container name. For example, a flow subscribing to an MQTT feed provided by the mosquitto container uses: mosquitto:1883 Similarly, if a flow writes to an InfluxDB database maintained by the influxdb container, the flow uses: influxdb:8086 Behind the scenes, Docker maintains a table similar to an /etc/hosts file mapping container names to the IP addresses on the internal bridged network that are assigned, dynamically, by Docker when it spins up each container. When Node-RED is in host mode \u00b6 This is where you use loopback+port syntax, such as the following to communicate with Mosquitto: 127.0.0.1:1883 What actually occurs is that Docker is listening to external port 1883 on behalf of Mosquitto. It receives the packet and routes it (layer three) to the internal bridged network, performing network address translation (NAT) along the way to map the external port to the internal port. Then the packet is delivered to Mosquitto. The reverse happens when Mosquitto replies. It works but is less efficient than when all containers are in non-host mode. GPIO Access \u00b6 To communicate with your Raspberry Pi's GPIO you need to do the following: Install dependencies: $ sudo apt update $ sudo apt install pigpio python-pigpio python3-pigpio Install the node-red-node-pi-gpiod node. See Adding extra nodes . It allows you to connect to multiple Pis from the same Node-RED service. Make sure that the pigpdiod daemon is running. The recommended method is listed here . In essence, you need to: Use sudo to edit /etc/rc.local ; Before the exit 0 statement, insert the line: /usr/bin/pigpiod Reboot. You can also pass parameters to pigpiod to secure the service. See the writeup for further instructions. Drag a gpio node onto the canvas and configure it using the IP address of your Raspberry Pi (eg 192.168.1.123). Don't try to use 127.0.0.1 because that is the loopback address of the Node-RED container. Sharing files between Node-RED and the Raspberry Pi \u00b6 Containers run in a sandboxed environment. A process running inside a container can't see the Raspberry Pi's file system. Neither can a process running outside a container access files inside the container. This presents a problem if you want write to a file outside a container, then read from it inside the container, or vice-versa. IOTstack containers have been set up with shared volume mappings. Each volume mapping associates a specific directory in the Raspberry Pi file system with a specific directory inside the container. If you write to files in a shared directory (or one of its sub-directories), both the host and the container can see the same sub-directories and files. Key point: Files and directories in the shared volume are persistent between restarts. If you save your data anywhere else inside the container, it will be lost when the container is rebuilt. The Node-RED service definition in the Compose file includes the following: volumes: - ./volumes/nodered/data:/data That decomposes into: external path = ./volumes/nodered/data internal path = /data The leading \".\" on the external path implies \"the folder containing the Compose file so it actually means: external path = ~/IOTstack/volumes/nodered/data internal path = /data If you write to the internal path from inside the Node-RED container, the Raspberry Pi will see the results at the external path, and vice versa. Example: $ docker exec -it nodered bash # echo \"The time now is $(date)\" >/data/example.txt # cat /data/example.txt The time now is Thu Apr 1 11 :25:56 AEDT 2021 # exit $ cat ~/IOTstack/volumes/nodered/data/example.txt The time now is Thu Apr 1 11 :25:56 AEDT 2021 $ sudo rm ~/IOTstack/volumes/nodered/data/example.txt In words: Open a shell into the Node-RED container. Two things happen: You are now inside the container. Any commands you execute while in this shell are run inside the container; and The prompt changes to a \"#\" indicating that you are running as the \"root\" user, meaning you don't need sudo for anything. Use the echo command to create a small file which embeds the current timestamp. The path is in the /data directory which is mapped to the Raspberry Pi's file system. Show that the file has been created inside the container. Exit the shell: You can either type the exit command and press return, or press Control+D. Exiting the shell drops you out of the container so the \"$\" prompt returns, indicating that you are outside the Node-Red container, running as a non-root user (\"pi\"). Show that the same file can be seen from outside the container. Tidy-up by removing the file. You need sudo to do that because the persistent storage area at the external path is owned by root, and you are running as user \"pi\". You can do the same thing from within a Node-RED flow. The flow comprises: An Inject node, wired to a Template node. When an Inject node's input tab is clicked, it sets the message payload to the number of seconds since 1/1/1970 UTC and triggers the flow. A Template node, wired to both a Debug node and a File node. The template field is set to: The time at the moment is {{payload}} seconds since 1/1/1970 UTC ! When this node runs, it replaces {{payload}} with the seconds value supplied by the Inject node. A Debug node. When this node runs, it displays the payload in the debug window on the right hand side of the Node-RED GUI. A File node. The \"Filename\" field of the node is set to write to the path: /data/flow-example.txt When this node runs, it writes the payload to the specified file. Remember that /data is an internal path within the Node-RED container. Deploying the flow and clicking on the Inject node results in the debug message shown on the right hand side of the screen shot. The embedded terminal window shows that the same information is accessible from outside the container. You can reverse this process. Any file you place within the path ~/IOTstack/volumes/nodered/data can be read by a \"File in\" node. Executing commands outside the Node-RED container \u00b6 A reasonably common requirement in a Node-RED flow is the ability to execute a command on the host system. The standard tool for this is an \"exec\" node. An \"exec\" node works as expected when Node-RED is running as a native service but not when Node-RED is running in a container. That's because the command spawned by the \"exec\" node runs inside the container. To help you understand the difference, consider this command: $ grep \"^PRETTY_NAME=\" /etc/os-release When you run that command on a Raspberry Pi outside container-space, the answer is: PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" If you run the same command inside a Node-RED container, the output will be: PRETTY_NAME=\"Alpine Linux v3.11\" The same thing will happen if a Node-RED \"exec\" node executes that grep command when Node-RED is running in a container. It will see the \"Alpine Linux\" answer. Docker doesn't provide any mechanism for a container to execute an arbitrary command outside of its container. A workaround is to utilise SSH. This remainder of this section explains how to set up the SSH scaffolding so that \"exec\" nodes running in a Node-RED container can invoke arbitrary commands outside container-space. Task Goal \u00b6 Be able to use a Node-RED exec node to perform the equivalent of: $ ssh \u00abHOSTNAME\u00bb \u00abCOMMAND\u00bb where: \u00abHOSTNAME\u00bb is any host under your control (not just the Raspberry Pi running IOTstack); and \u00abCOMMAND\u00bb is any command known to the target host. Assumptions \u00b6 SensorsIot/IOTstack is installed on your Raspberry Pi. The Node-RED container is running. These instructions are specific to IOTstack but the underlying concepts should apply to any installation of Node-RED in a Docker container. Executing commands \"inside\" a container \u00b6 These instructions make frequent use of the ability to run commands \"inside\" the Node-RED container. For example, suppose you want to execute: $ grep \"^PRETTY_NAME=\" /etc/os-release You have several options: You can do it from the normal Raspberry Pi command line using a Docker command. The basic syntax is: $ docker exec { -it } \u00abcontainerName\u00bb \u00abcommand and parameters\u00bb The actual command you would need would be: $ docker exec nodered grep \"^PRETTY_NAME=\" /etc/os-release Note: The -it flag is optional . It means \"interactive terminal\". Its presence tells Docker that the command may need user interaction, such as entering a password or typing \"yes\" to a question. You can open a shell into the container, run as many commands as you like inside the container, and then exit. For example: $ docker exec -it nodered bash # grep \"^PRETTY_NAME=\" /etc/os-release # whoami # exit $ In words: Run the bash shell inside the Node-RED container. You need to be able to interact with the shell to type commands so the -it flag is required. The \"#\" prompt is coming from bash running inside the container. It also signals that you are running as the root user inside the container. You run the grep , whoami and any other commands. You finish with the exit command (or Control+D). The \"$\" prompt means you have left the container and are back at the normal Raspberry Pi command line. Run the command from Portainer by selecting the container, then clicking the \">_ console\" link. This is identical to opening a shell. Variable definitions \u00b6 You will need to have a few concepts clear in your mind before you can set up SSH successfully. I use double-angle quote marks (guillemets) to mean \"substitute the appropriate value here\". \u00abHOSTNAME\u00bb (required) \u00b6 The name of your Raspberry Pi. When you first booted your RPi, it had the name \"raspberrypi\" but you probably changed it using raspi-config . Example: iot-dev \u00abHOSTADDR\u00bb (required) \u00b6 Either or both of the following: \u00abHOSTFQDN\u00bb (optional) If you have a local Domain Name System server, you may have defined a fully-qualified domain name (FQDN) for your Raspberry Pi. Example: iot-dev.mydomain.com Note: Docker's internal networks do not support multicast traffic. You can't use a multicast DNS name (eg \"raspberrypi.local\") as a substitute for a fully-qualified domain name. \u00abHOSTIP\u00bb (required) Even if you don't have a fully-qualified domain name, you will still have an IP address for your Raspberry Pi. Example: 192.168.132.9 Keep in mind that a Raspberry Pi running IOTstack is operating as a server . A dynamic DHCP address is not appropriate for a server. The server's IP address needs to be fixed. The two standard approaches are: a static DHCP assignment configured on your DHCP server (eg your router) which always returns the same IP address for a given MAC address; or a static IP address configured on your Raspberry Pi. \u00abUSERID\u00bb (required) \u00b6 The user ID of the account on \u00abHOSTNAME\u00bb where you want Node-RED flows to be able to run commands. Example: pi Step 1: Generate SSH key-pair for Node-RED (one time) \u00b6 Create a key-pair for Node-RED. This is done by executing the ssh-keygen command inside the container: $ docker exec -it nodered ssh-keygen -q -t ed25519 -C \"Node-RED container key-pair\" -N \"\" Notes: The \"ed25519\" elliptic curve algorithm is recommended (generally described as quicker and more secure than RSA) but you can use the default RSA algorithm if you prefer. Respond to the \"Enter file in which to save the key\" prompt by pressing return to accept the default location. If ssh-keygen displays an \"Overwrite (y/n)?\" message, it implies that a key-pair already exists. You will need to decide what to do: press \"y\" to overwrite (and lose the old keys) press \"n\" to terminate the command, after which you can investigate why a key-pair already exists. Step 2: Exchange keys with target hosts (once per target host) \u00b6 Node-RED's public key needs to be copied to the user account on each target machine where you want a Node-RED \"exec\" node to be able to execute commands. At the same time, the Node-RED container needs to learn the public host key of the target machine. The ssh-copy-id command does both steps. The required syntax is: $ docker exec -it nodered ssh-copy-id \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb Examples: $ docker exec -it nodered ssh-copy-id pi@iot-dev.mydomain.com $ docker exec -it nodered ssh-copy-id pi@192.168.132.9 The output will be something similar to the following: /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/root/.ssh/id_ed25519.pub\" The authenticity of host 'iot-dev.mydomain.com (192.168.132.9)' can't be established. ED25519 key fingerprint is SHA256:HVoeowZ1WTSG0qggNsnGwDA6acCd/JfVLZsNUv4hjNg. Are you sure you want to continue connecting (yes/no/[fingerprint])? Respond to the prompt by typing \"yes\" and press return. The output continues: /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed expr: warning: '^ERROR: ': using '^' as the first character of a basic regular expression is not portable; it is ignored /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys pi@iot-dev.mydomain.com's password: The response may look like it contains errors but those can be ignored. Enter the password you use to login as \u00abUSERID\u00bb on \u00abHOSTADDR\u00bb and press return. Normal completion looks similar to this: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh 'pi@iot-dev.mydomain.com'\" and check to make sure that only the key(s) you wanted were added. If you do not see an indication that a key has been added, you may need to retrace your steps. Step 3: Perform the recommended test \u00b6 The output above recommends a test. The test needs to be run inside the Node-RED container so the syntax is: $ docker exec -it nodered ssh \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb ls -1 /home/pi/IOTstack Examples: $ docker exec -it nodered ssh pi@iot-dev.mydomain.com ls -1 /home/pi/IOTstack $ docker exec -it nodered ssh pi@192.168.132.9 ls -1 /home/pi/IOTstack You should not be prompted for a password. If you are, you may need to retrace your steps. If everything works as expected, you should see a list of the files in your IOTstack folder. Assuming success, think about what just happened? You told SSH inside the Node-RED container to run the ls command outside the container on your Raspberry Pi. You broke through the containerisation. Understanding what's where and what each file does \u00b6 What files are where \u00b6 Six files are relevant to Node-RED's ability to execute commands outside of container-space: in /etc/ssh : ssh_host_ed25519_key is the Raspberry Pi's private host key ssh_host_ed25519_key.pub is the Raspberry Pi's public host key Those keys were created when your Raspberry Pi was initialised. They are unique to the host. Unless you take precautions, those keys will change whenever your Raspberry Pi is rebuilt from scratch and that will stop SSH from working. You can recover by re-running ssh-copy-id . in ~/IOTstack/volumes/nodered/ssh : id_ed25519 is the Node-RED container's private key id_ed25519.pub is the Node-RED container's public key Those keys were created when you generated the SSH key-pair for Node-RED. They are unique to Node-RED but will follow the container in backups and will work on the same machine, or other machines, if you restore the backup. It does not matter if the Node-RED container is rebuilt or if a new version of Node-RED comes down from DockerHub. These keys will remain valid until lost or overwritten. If you lose or destroy these keys, SSH will stop working. You can recover by generating new keys and then re-running ssh-copy-id . known_hosts The known_hosts file contains a copy of the Raspberry Pi's public host key. It was put there by ssh-copy-id . If you lose this file or it gets overwritten, SSH will still work but will re-prompt for authorisation to connect. This works when you are running commands from docker exec -it but not when running commands from an exec node. Note that authorising the connection at the command line (\"Are you sure you want to continue connecting?\") will auto-repair the known_hosts file. in ~/.ssh/ : authorized_keys That file contains a copy of the Node-RED container's public key. It was put there by ssh-copy-id . Pay attention to the path. It implies that there is one authorized_keys file per user, per target host. If you lose this file or it gets overwritten, SSH will still work but will ask for the password for \u00abUSERID\u00bb. This works when you are running commands from docker exec -it but not when running commands from an exec node. Note that providing the correct password at the command line will auto-repair the authorized_keys file. What each file does \u00b6 SSH running inside the Node-RED container uses the Node-RED container's private key to provide assurance to SSH running outside the container that it (the Node-RED container) is who it claims to be. SSH running outside container-space verifies that assurance by using its copy of the Node-RED container's public key in authorized_keys . SSH running outside container-space uses the Raspberry Pi's private host key to provide assurance to SSH running inside the Node-RED container that it (the RPi) is who it claims to be. SSH running inside the Node-RED container verifies that assurance by using its copy of the Raspberry Pi's public host key stored in known_hosts . Config file (optional) \u00b6 You don't have to do this step but it will simplify your exec node commands and reduce your maintenance problems if you do. At this point, SSH commands can be executed from inside the container using this syntax: # ssh \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb \u00abCOMMAND\u00bb A config file is needed to achieve the task goal of the simpler syntax: # ssh \u00abHOSTNAME\u00bb \u00abCOMMAND\u00bb A config file does not just simplify connection commands. It provides isolation between the \u00abHOSTNAME\u00bb and \u00abHOSTADDR\u00bb such that you only have a single file to change if your \u00abHOSTADDR\u00bb changes (eg new IP address or fully qualified domain name). It also exposes less about your network infrastructure when you share your flows. The goal is to set up this file: -rw-r--r-- 1 root root ~/IOTstack/volumes/nodered/ssh/config The file needs the ownership and permissions shown. There are several ways of going about this and you are free to choose the one that works for you. The method described here creates the file first, then sets correct ownership and permissions, and then moves the file into place. Start in a directory where you can create a file without needing sudo . The IOTstack folder is just as good as anywhere else: $ cd ~/IOTstack $ touch config Select the following text, copy it to the clipboard. host \u00abHOSTNAME\u00bb hostname \u00abHOSTADDR\u00bb user \u00abUSERID\u00bb IdentitiesOnly yes IdentityFile /root/.ssh/id_ed25519 Open ~/IOTstack/config in your favourite text editor and paste the contents of the clipboard. Replace the \u00abdelimited\u00bb keys. Completed examples: If you are using the \u00abHOSTFQDN\u00bb form: host iot-dev hostname iot-dev.mydomain.com user pi IdentitiesOnly yes IdentityFile /root/.ssh/id_ed25519 If you are using the \u00abHOSTIP\u00bb form: host iot-dev hostname 192.168.132.9 user pi IdentitiesOnly yes IdentityFile /root/.ssh/id_ed25519 Save the file. Change the config file's ownership and permissions, and move it into the correct directory: $ chmod 644 config $ sudo chown root:root config $ sudo mv config ./volumes/nodered/ssh Re-test with config file in place \u00b6 The previous test used this syntax: $ docker exec nodered ssh \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb ls -1 /home/pi/IOTstack Now that the config file is in place, the syntax changes to: $ docker exec nodered ssh \u00abHOSTNAME\u00bb ls -1 /home/pi/IOTstack Example: $ docker exec nodered ssh iot-dev ls -1 /home/pi/IOTstack The result should be the same as the earlier test. A test flow \u00b6 In the Node-RED GUI: Click the \"+\" to create a new, empty flow. Drag the following nodes onto the canvas: One \"inject\" node Two \"exec\" nodes Two \"debug\" nodes Wire the outlet of the \"inject\" node to the inlet of both \"exec\" nodes. Wire the uppermost \"stdout\" outlet of the first \"exec\" node to the inlet of the first \"debug\" node. Repeat step 4 with the other \"exec\" and \"debug\" node. Open the first \"exec\" node and: set the \"command\" field to: grep \"^PRETTY_NAME=\" /etc/os-release - turn off the \"append msg.payload\" checkbox - set the timeout to a reasonable value (eg 10 seconds) - click \"Done\". 7. Repeat step 6 with the other \"exec\" node, with one difference: - set the \"command\" field to: ssh iot-dev grep \"^PRETTY_NAME=\" /etc/os-release Click the Deploy button. Set the right hand panel to display debug messages. Click the touch panel of the \"inject\" node to trigger the flow. Inspect the result in the debug panel. You should see payload differences similar to the following: PRETTY_NAME=\"Alpine Linux v3.11\" PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" The first line is the result of running the command inside the Node-RED container. The second line is the result of running the same command outside the Node-RED container on the Raspberry Pi. Suppose you want to add another \u00abHOSTNAME\u00bb \u00b6 Exchange keys with the new target host using: $ docker exec -it nodered ssh-copy-id \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb Edit the config file at the path: ~/IOTstack/volumes/nodered/ssh/config to define the new host. Remember to use sudo to edit the file. There is no need to restart Node-RED or recreate the container. Upgrading Node-RED \u00b6 You can update most containers like this: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. This strategy doesn't work for Node-RED. The local image ( iotstack_nodered ) does not exist on DockerHub so there is no way for the pull to sense when a newer version becomes available. The only way to know when an update to Node-RED is available is to check the nodered/node-red tags page on DockerHub . Once a new version appears on DockerHub , you can upgrade Node-RED like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull nodered $ docker-compose up -d nodered $ docker system prune Breaking it down into parts: build causes the named container to be rebuilt; --no-cache tells the Dockerfile process that it must not take any shortcuts. It really must rebuild the local image ; --pull tells the Dockerfile process to actually check with DockerHub to see if there is a later version of the base image and, if so, to download it before starting the build; nodered is the named container argument required by the build command. Your existing Node-RED container continues to run while the rebuild proceeds. Once the freshly-built local image is ready, the up tells docker-compose to do a new-for-old swap. There is barely any downtime for your Node-RED service. The prune is the simplest way of cleaning up old images. Sometimes you need to run this twice, the first time to clean up the old local image, the second time for the old base image. Whether an old base image exists depends on the version of docker-compose you are using and how your version of docker-compose builds local images. Customising Node-RED \u00b6 You customise your local image of Node-RED by making changes to: ~/IOTstack/services/nodered/Dockerfile You apply a Dockerfile change like this: $ cd ~/IOTstack $ docker-compose up --build -d nodered $ docker system prune The --build option on the up command (as distinct from a docker-compose build command) works in this situation because you've made a substantive change to your Dockerfile . Node.js version \u00b6 Out of the box, IOTstack starts the Node-RED Dockerfile with: FROM nodered/node-red:latest-12 The -12 suffix means \" node.js is pinned at version 12.x.x\". That's the latest version of node.js that Node-RED currently supports. If you want to check which version of node.js is installed on your system, you can do it like this: $ docker exec nodered node --version In the unlikely event that you need to run an add-on node that needs version 10 of node.js , you can pin to version 10.x.x by changing the first line of your Dockerfile like this: FROM nodered/node-red:latest-10 Once you have made that change, follow the steps at apply Dockerfile changes . Adding extra packages \u00b6 As well as providing the Node-RED service, the nodered container is an excellent testbed. Installing the DNS tools, Mosquitto clients and tcpdump will help you to figure out what is going on inside container-space. There are two ways to add extra packages. The first method is to add them to the running container. For example, to add the Mosquitto clients: $ docker exec nodered apk add --no-cache mosquitto-clients The \"apk\" implies that the Node-RED container is based on Alpine Linux. Keep that in mind when you search for instructions on installing packages. Packages installed this way will persist until the container is re-created (eg a down and up of the stack, or a reboot of your Raspberry Pi). This is a good choice if you only want to run a quick experiment. The second method changes the Dockerfile to add the packages permanently to your build. You just append the packages to the end of the existing apk add : RUN apk update && apk add --no-cache eudev-dev mosquitto-clients bind-tools tcpdump You can add as many extra packages as you like. They will persist until you change the Dockerfile again. Once you have made this change, follow the steps at apply Dockerfile changes . Adding extra nodes \u00b6 You can install nodes by: Adding nodes to the Dockerfile and then following the steps at apply Dockerfile changes . This is also what will happen if you re-run the menu and change the selected nodes, except that the menu will also blow away any other customisations you may have made to your Dockerfile . Adding, removing or updating nodes in Manage Palette. Node-RED will remind you to restart Node-RED and that is something you have to do by hand: $ cd ~/IOTstack $ docker-compose restart nodered Note: Some users have reported misbehaviour from Node-RED if they do too many iterations of: [ make a single change in Manage Palette ] $ docker-compose restart nodered [ make a single change in Manage Palette ] $ docker-compose restart nodered \u2026 It is better to: [ do ALL your Manage Palette changes ] $ docker-compose restart nodered Installing nodes inside the container via npm: $ docker exec -it nodered bash # cd /data # npm install \u00abnode-name\u00bb /data # exit $ cd ~/IOTstack $ docker-compose restart nodered Note: You must put the /data onto the end of the npm install command. Any formula you find on the web will not include this. You have to remember to do it yourself! See also the note above about restarting too frequently. You can use this approach if you need to force the installation of a specific version (which you don't appear to be able to do in Manage Palette). For example, to install version 4.0.0 of the \"moment\" node: # npm install node-red-contrib-moment@4.0.0 /data There is no real difference between the methods. Some nodes (eg \"node-red-contrib-generic-ble\" and \"node-red-node-sqlite\" must be installed by Dockerfile but the only way of finding out if a node must be installed via Dockerfile is to try Manage Palette and find that it doesn't work. Aside from the exception cases that require Dockerfile or where you need to force a specific version, it is quicker to install nodes via Manage Palette and applying updates is a bit easier too. But it's really up to you. If you're wondering about \"backup\", nodes installed via: Dockerfile \u2013 implicitly backed up when the Dockerfile is backed-up. Manage Palette or npm install \u2013 explicitly backed up when the ~/IOTstack/volumes directory is backed-up. Basically, if you're running IOTstack backups then your add-on nodes will be backed-up. Node precedence \u00b6 Add-on nodes that are installed via Dockerfile wind up at the internal path: /usr/src/node-red/node_modules Add-on nodes installed via Manage Palette wind up at the external path: ~/IOTstack/volumes/nodered/data/node_modules The Compose file volumes mapping: ./volumes/nodered/data:/data implies that add-on nodes installed via Manage Palette are made available to Node-RED at the internal path: /data/node_modules Because there are two places, this invites the question of what happens if a given node is installed in both? The answer is that add-ons installed at: /data/node_modules take precedence over those installed at: /usr/src/node-red/node_modules Or, to put it more simply: in any contest, Manage Palette prevails over Dockerfile . Resolving node duplication \u00b6 Sometimes, even when you are 100% certain that you didn't do it, an add-on node will turn up in both places. There is probably some logical reason for this but I don't know what it is. The problem this creates is that a later version of an add-on node installed via Dockerfile will be blocked by the presence of an older version of that node in: ~/IOTstack/volumes/nodered/data/node_modules The nodered_list_installed_nodes.sh script helps discover when this situation exists. For example: $ ~/IOTstack/scripts/nodered_list_installed_nodes.sh Nodes installed by Dockerfile INSIDE the container at /usr/src/node-red/node_modules ACTIVE: node-red-admin ACTIVE: node-red-configurable-ping ACTIVE: node-red-contrib-boolean-logic ACTIVE: node-red-contrib-generic-ble ACTIVE: node-red-contrib-influxdb ACTIVE: node-red-dashboard BLOCKED: node-red-node-email ACTIVE: node-red-node-pi-gpiod ACTIVE: node-red-node-rbe ACTIVE: node-red-node-sqlite ACTIVE: node-red-node-tail Nodes installed by \u00abManage Palette\u00bb OUTSIDE the container at /home/pi/IOTstack/volumes/nodered/data/node_modules node-red-contrib-boolean-logic-ultimate node-red-contrib-chartjs node-red-node-email node-red-contrib-md5 node-red-contrib-moment node-red-contrib-pushsafer Notice how node-red-node-email appears in both lists. To fix this problem: Move into the correct external directory: $ cd ~/IOTstack/volumes/nodered/data/node_modules Create a sub-directory to be the equivalent of a local trash can: $ sudo mkdir duplicates Move each duplicate node into the duplicates directory. For example, to move node-red-node-email you would: $ sudo mv node-red-node-email duplicates Tell Node-RED to restart. This causes it to forget about the nodes which have just been moved out of the way: $ docker-compose -f ~/IOTstack/docker-compose.yml restart nodered Finish off by erasing the duplicates folder: $ sudo rm -rf duplicates Always be extremely careful with any rm -rf , particularly when it is coupled with a sudo . Double-check your work before you press return.","title":"Node-RED"},{"location":"Containers/Node-RED/#node-red","text":"","title":"Node-RED"},{"location":"Containers/Node-RED/#references","text":"nodered.org home GitHub: node-red/node-red-docker DockerHub: nodered/node-red Tutorial: from MQTT to InfluxDB via Node-Red","title":"References"},{"location":"Containers/Node-RED/#significant-files","text":"~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 nodered \u2502 \u2514\u2500\u2500 service.yml \u2776 \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 nodered \u2502 \u251c\u2500\u2500 Dockerfile \u2777 \u2502 \u2514\u2500\u2500 service.yml \u2778 \u251c\u2500\u2500 docker-compose.yml \u2779 \u2514\u2500\u2500 volumes \u2514\u2500\u2500 nodered \u277a \u251c\u2500\u2500 data \u277b \u2514\u2500\u2500 ssh \u277c Template service definition . The Dockerfile . Working service definition (old-menu only, copied from \u2776). The Compose file (includes \u2776) Persistent storage area. Data directory (mapped volume). SSH directory (mapped volume).","title":"Significant files"},{"location":"Containers/Node-RED/#how-node-red-gets-built-for-iotstack","text":"","title":"How Node-RED gets built for IOTstack"},{"location":"Containers/Node-RED/#node-red-source-code-github","text":"The source code for Node-RED lives at GitHub node-red/node-red-docker .","title":"Node-RED source code (GitHub)"},{"location":"Containers/Node-RED/#node-red-images-dockerhub","text":"Periodically, the source code is recompiled and pushed to nodered/node-red on DockerHub . There's a lot of stuff at that page but it boils down to variations on two basic themes: images with a suffix like -10 , -12 or -14 ; and images without a numeric suffix. The suffixes refer to the version of \"Node.js\" installed when the image was built. In the case of images without a numeric suffix: If the version of Node-RED is 1.3.x then a suffix of -10 is implied. In other words, Node.js defaults to version 10. If the version of Node-RED is 2.x.x then a suffix of -14 is implied. In other words, Node.js defaults to version 14. As you will see a bit further down, the current default for IOTstack is an image tag of latest-12 which means Node-RED 2.x.x with Node.js version 12.","title":"Node-RED images (DockerHub)"},{"location":"Containers/Node-RED/#iotstack-menu","text":"When you select Node-RED in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used. You choose add-on nodes from a supplementary menu. We recommend accepting the default nodes, and adding others that you think you are likely to need. Node-RED will not build if you do not select at least one add-on node. Key points: Under new menu, you must press the right arrow to access the supplementary menu. Under old menu, the list of add-on nodes is displayed automatically. Do not be concerned if you can't find an add-on node you need in the list. You can also add nodes via Manage Palette once Node-RED is running. See adding extra nodes . Choosing add-on nodes in the menu causes the Dockerfile to be created.","title":"IOTstack menu"},{"location":"Containers/Node-RED/#iotstack-first-run","text":"On a first install of IOTstack, you are told to do this: $ cd ~/IOTstack $ docker-compose up -d docker-compose reads the Compose file. When it arrives at the nodered fragment, it finds: nodered: container_name: nodered build: ./services/nodered/. \u2026 The build statement tells docker-compose to look for: ~/IOTstack/services/nodered/Dockerfile The Dockerfile begins with: FROM nodered/node-red:latest-12 Note: IOTstack switched to the -12 suffix in March 2021. Existing IOTstack installations will not update unless you re-create the service from its template, or hand-edit the Dockerfile . The FROM statement tells the build process to pull down the latest base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The local image is instantiated to become your running container. Notes: During the build you may see warnings and deprecation notices. You may also see messages about \"vulnerabilities\" along with recommendations to run npm audit fix . You should ignore all such messages. There is no need to take any action. If SQLite is in your list of nodes, be aware that it needs to be compiled from its source code. It takes a long time, outputs an astonishing number of warnings and, from time to time, will look as if it has gotten stuck. Be patient. Acknowledgement: Successful installation of the SQLite node is thanks to @fragolinux. When you run the docker images command after Node-RED has been built, you may see two rows for Node-RED: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_nodered latest b0b21a97b8bb 4 days ago 462MB nodered/node-red latest deb99584fa75 5 days ago 385MB nodered/node-red is the base image ; and iotstack_nodered is the local image . The local image is the one that is instantiated to become the running container. You may see the same pattern in Portainer, which reports the base image as \"unused\": You should not remove the base image, even though it appears to be unused. Whether you see one or two rows depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"IOTstack first run"},{"location":"Containers/Node-RED/#securing-node-red","text":"","title":"Securing Node-RED"},{"location":"Containers/Node-RED/#setting-an-encryption-key-for-your-credentials","text":"After you install Node-RED, you should set an encryption key. Completing this step will silence the warning you will see when you run: $ docker logs nodered \u2026 --------------------------------------------------------------------- Your flow credentials file is encrypted using a system-generated key. If the system-generated key is lost for any reason, your credentials file will not be recoverable, you will have to delete it and re-enter your credentials. You should set your own key using the 'credentialSecret' option in your settings file. Node-RED will then re-encrypt your credentials file using your chosen key the next time you deploy a change. --------------------------------------------------------------------- \u2026 Setting an encryption key also means that any credentials you create will be portable , in the sense that you can backup Node-RED on one machine and restore it on another. The encryption key can be any string. For example, if you have UUID support installed ( sudo apt install -y uuid-runtime ), you could generate a UUID as your key: $ uuidgen 2deb50d4-38f5-4ab3-a97e-d59741802e2d Once you have defined your encryption key, use sudo and your favourite text editor to open this file: ~/IOTstack/volumes/nodered/data/settings.js Search for credentialSecret : //credentialSecret: \"a-secret-key\", Un-comment the line and replace a-secret-key with your chosen key. Do not remove the comma at the end of the line. The result should look something like this: credentialSecret: \"2deb50d4-38f5-4ab3-a97e-d59741802e2d\", Save the file and then restart Node-RED: $ cd ~/IOTstack $ docker-compose restart nodered","title":"Setting an encryption key for your credentials"},{"location":"Containers/Node-RED/#setting-a-username-and-password-for-node-red","text":"To secure Node-RED you need a password hash. Run the following command, replacing PASSWORD with your own password: $ docker exec nodered node -e \"console.log(require('bcryptjs').hashSync(process.argv[1], 8));\" PASSWORD You will get an answer that looks something like this: $2a$08$gTdx7SkckJVCw1U98o4r0O7b8P.gd5/LAPlZI6geg5LRg4AUKuDhS Copy that text to your clipboard, then follow the instructions at Node-RED User Guide - Securing Node-RED - Username & Password-based authentication .","title":"Setting a username and password for Node-RED"},{"location":"Containers/Node-RED/#referring-to-other-containers","text":"Node-RED can run in two modes. By default, it runs in \"non-host mode\" but you can also move the container to \"host mode\" by editing the Node-RED service definition in your Compose file to: Add the following directive: network_mode: host Remove the ports directive and the mapping of port 1880.","title":"Referring to other containers"},{"location":"Containers/Node-RED/#when-node-red-is-not-in-host-mode","text":"Most examples on the web assume Node-RED and other services in the MING (Mosquitto, InfluxDB, Node-RED, Grafana) stack have been installed natively, rather than in Docker containers. Those examples typically include the loopback address + port syntax, like this: 127.0.0.1:1883 The loopback address will not work when Node-RED is in non-host mode. This is because each container behaves like a self-contained computer. The loopback address means \"this container\". It does not mean \"this Raspberry Pi\". You refer to other containers by their container name. For example, a flow subscribing to an MQTT feed provided by the mosquitto container uses: mosquitto:1883 Similarly, if a flow writes to an InfluxDB database maintained by the influxdb container, the flow uses: influxdb:8086 Behind the scenes, Docker maintains a table similar to an /etc/hosts file mapping container names to the IP addresses on the internal bridged network that are assigned, dynamically, by Docker when it spins up each container.","title":"When Node-RED is not in host mode"},{"location":"Containers/Node-RED/#when-node-red-is-in-host-mode","text":"This is where you use loopback+port syntax, such as the following to communicate with Mosquitto: 127.0.0.1:1883 What actually occurs is that Docker is listening to external port 1883 on behalf of Mosquitto. It receives the packet and routes it (layer three) to the internal bridged network, performing network address translation (NAT) along the way to map the external port to the internal port. Then the packet is delivered to Mosquitto. The reverse happens when Mosquitto replies. It works but is less efficient than when all containers are in non-host mode.","title":"When Node-RED is in host mode"},{"location":"Containers/Node-RED/#gpio-access","text":"To communicate with your Raspberry Pi's GPIO you need to do the following: Install dependencies: $ sudo apt update $ sudo apt install pigpio python-pigpio python3-pigpio Install the node-red-node-pi-gpiod node. See Adding extra nodes . It allows you to connect to multiple Pis from the same Node-RED service. Make sure that the pigpdiod daemon is running. The recommended method is listed here . In essence, you need to: Use sudo to edit /etc/rc.local ; Before the exit 0 statement, insert the line: /usr/bin/pigpiod Reboot. You can also pass parameters to pigpiod to secure the service. See the writeup for further instructions. Drag a gpio node onto the canvas and configure it using the IP address of your Raspberry Pi (eg 192.168.1.123). Don't try to use 127.0.0.1 because that is the loopback address of the Node-RED container.","title":"GPIO Access"},{"location":"Containers/Node-RED/#sharing-files-between-node-red-and-the-raspberry-pi","text":"Containers run in a sandboxed environment. A process running inside a container can't see the Raspberry Pi's file system. Neither can a process running outside a container access files inside the container. This presents a problem if you want write to a file outside a container, then read from it inside the container, or vice-versa. IOTstack containers have been set up with shared volume mappings. Each volume mapping associates a specific directory in the Raspberry Pi file system with a specific directory inside the container. If you write to files in a shared directory (or one of its sub-directories), both the host and the container can see the same sub-directories and files. Key point: Files and directories in the shared volume are persistent between restarts. If you save your data anywhere else inside the container, it will be lost when the container is rebuilt. The Node-RED service definition in the Compose file includes the following: volumes: - ./volumes/nodered/data:/data That decomposes into: external path = ./volumes/nodered/data internal path = /data The leading \".\" on the external path implies \"the folder containing the Compose file so it actually means: external path = ~/IOTstack/volumes/nodered/data internal path = /data If you write to the internal path from inside the Node-RED container, the Raspberry Pi will see the results at the external path, and vice versa. Example: $ docker exec -it nodered bash # echo \"The time now is $(date)\" >/data/example.txt # cat /data/example.txt The time now is Thu Apr 1 11 :25:56 AEDT 2021 # exit $ cat ~/IOTstack/volumes/nodered/data/example.txt The time now is Thu Apr 1 11 :25:56 AEDT 2021 $ sudo rm ~/IOTstack/volumes/nodered/data/example.txt In words: Open a shell into the Node-RED container. Two things happen: You are now inside the container. Any commands you execute while in this shell are run inside the container; and The prompt changes to a \"#\" indicating that you are running as the \"root\" user, meaning you don't need sudo for anything. Use the echo command to create a small file which embeds the current timestamp. The path is in the /data directory which is mapped to the Raspberry Pi's file system. Show that the file has been created inside the container. Exit the shell: You can either type the exit command and press return, or press Control+D. Exiting the shell drops you out of the container so the \"$\" prompt returns, indicating that you are outside the Node-Red container, running as a non-root user (\"pi\"). Show that the same file can be seen from outside the container. Tidy-up by removing the file. You need sudo to do that because the persistent storage area at the external path is owned by root, and you are running as user \"pi\". You can do the same thing from within a Node-RED flow. The flow comprises: An Inject node, wired to a Template node. When an Inject node's input tab is clicked, it sets the message payload to the number of seconds since 1/1/1970 UTC and triggers the flow. A Template node, wired to both a Debug node and a File node. The template field is set to: The time at the moment is {{payload}} seconds since 1/1/1970 UTC ! When this node runs, it replaces {{payload}} with the seconds value supplied by the Inject node. A Debug node. When this node runs, it displays the payload in the debug window on the right hand side of the Node-RED GUI. A File node. The \"Filename\" field of the node is set to write to the path: /data/flow-example.txt When this node runs, it writes the payload to the specified file. Remember that /data is an internal path within the Node-RED container. Deploying the flow and clicking on the Inject node results in the debug message shown on the right hand side of the screen shot. The embedded terminal window shows that the same information is accessible from outside the container. You can reverse this process. Any file you place within the path ~/IOTstack/volumes/nodered/data can be read by a \"File in\" node.","title":"Sharing files between Node-RED and the Raspberry Pi"},{"location":"Containers/Node-RED/#executing-commands-outside-the-node-red-container","text":"A reasonably common requirement in a Node-RED flow is the ability to execute a command on the host system. The standard tool for this is an \"exec\" node. An \"exec\" node works as expected when Node-RED is running as a native service but not when Node-RED is running in a container. That's because the command spawned by the \"exec\" node runs inside the container. To help you understand the difference, consider this command: $ grep \"^PRETTY_NAME=\" /etc/os-release When you run that command on a Raspberry Pi outside container-space, the answer is: PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" If you run the same command inside a Node-RED container, the output will be: PRETTY_NAME=\"Alpine Linux v3.11\" The same thing will happen if a Node-RED \"exec\" node executes that grep command when Node-RED is running in a container. It will see the \"Alpine Linux\" answer. Docker doesn't provide any mechanism for a container to execute an arbitrary command outside of its container. A workaround is to utilise SSH. This remainder of this section explains how to set up the SSH scaffolding so that \"exec\" nodes running in a Node-RED container can invoke arbitrary commands outside container-space.","title":"Executing commands outside the Node-RED container"},{"location":"Containers/Node-RED/#task-goal","text":"Be able to use a Node-RED exec node to perform the equivalent of: $ ssh \u00abHOSTNAME\u00bb \u00abCOMMAND\u00bb where: \u00abHOSTNAME\u00bb is any host under your control (not just the Raspberry Pi running IOTstack); and \u00abCOMMAND\u00bb is any command known to the target host.","title":"Task Goal"},{"location":"Containers/Node-RED/#assumptions","text":"SensorsIot/IOTstack is installed on your Raspberry Pi. The Node-RED container is running. These instructions are specific to IOTstack but the underlying concepts should apply to any installation of Node-RED in a Docker container.","title":"Assumptions"},{"location":"Containers/Node-RED/#executing-commands-inside-a-container","text":"These instructions make frequent use of the ability to run commands \"inside\" the Node-RED container. For example, suppose you want to execute: $ grep \"^PRETTY_NAME=\" /etc/os-release You have several options: You can do it from the normal Raspberry Pi command line using a Docker command. The basic syntax is: $ docker exec { -it } \u00abcontainerName\u00bb \u00abcommand and parameters\u00bb The actual command you would need would be: $ docker exec nodered grep \"^PRETTY_NAME=\" /etc/os-release Note: The -it flag is optional . It means \"interactive terminal\". Its presence tells Docker that the command may need user interaction, such as entering a password or typing \"yes\" to a question. You can open a shell into the container, run as many commands as you like inside the container, and then exit. For example: $ docker exec -it nodered bash # grep \"^PRETTY_NAME=\" /etc/os-release # whoami # exit $ In words: Run the bash shell inside the Node-RED container. You need to be able to interact with the shell to type commands so the -it flag is required. The \"#\" prompt is coming from bash running inside the container. It also signals that you are running as the root user inside the container. You run the grep , whoami and any other commands. You finish with the exit command (or Control+D). The \"$\" prompt means you have left the container and are back at the normal Raspberry Pi command line. Run the command from Portainer by selecting the container, then clicking the \">_ console\" link. This is identical to opening a shell.","title":"Executing commands \"inside\" a container"},{"location":"Containers/Node-RED/#variable-definitions","text":"You will need to have a few concepts clear in your mind before you can set up SSH successfully. I use double-angle quote marks (guillemets) to mean \"substitute the appropriate value here\".","title":"Variable definitions"},{"location":"Containers/Node-RED/#hostname-required","text":"The name of your Raspberry Pi. When you first booted your RPi, it had the name \"raspberrypi\" but you probably changed it using raspi-config . Example: iot-dev","title":"\u00abHOSTNAME\u00bb (required)"},{"location":"Containers/Node-RED/#hostaddr-required","text":"Either or both of the following: \u00abHOSTFQDN\u00bb (optional) If you have a local Domain Name System server, you may have defined a fully-qualified domain name (FQDN) for your Raspberry Pi. Example: iot-dev.mydomain.com Note: Docker's internal networks do not support multicast traffic. You can't use a multicast DNS name (eg \"raspberrypi.local\") as a substitute for a fully-qualified domain name. \u00abHOSTIP\u00bb (required) Even if you don't have a fully-qualified domain name, you will still have an IP address for your Raspberry Pi. Example: 192.168.132.9 Keep in mind that a Raspberry Pi running IOTstack is operating as a server . A dynamic DHCP address is not appropriate for a server. The server's IP address needs to be fixed. The two standard approaches are: a static DHCP assignment configured on your DHCP server (eg your router) which always returns the same IP address for a given MAC address; or a static IP address configured on your Raspberry Pi.","title":"\u00abHOSTADDR\u00bb (required)"},{"location":"Containers/Node-RED/#userid-required","text":"The user ID of the account on \u00abHOSTNAME\u00bb where you want Node-RED flows to be able to run commands. Example: pi","title":"\u00abUSERID\u00bb (required)"},{"location":"Containers/Node-RED/#step-1-generate-ssh-key-pair-for-node-red-one-time","text":"Create a key-pair for Node-RED. This is done by executing the ssh-keygen command inside the container: $ docker exec -it nodered ssh-keygen -q -t ed25519 -C \"Node-RED container key-pair\" -N \"\" Notes: The \"ed25519\" elliptic curve algorithm is recommended (generally described as quicker and more secure than RSA) but you can use the default RSA algorithm if you prefer. Respond to the \"Enter file in which to save the key\" prompt by pressing return to accept the default location. If ssh-keygen displays an \"Overwrite (y/n)?\" message, it implies that a key-pair already exists. You will need to decide what to do: press \"y\" to overwrite (and lose the old keys) press \"n\" to terminate the command, after which you can investigate why a key-pair already exists.","title":"Step 1: Generate SSH key-pair for Node-RED (one time)"},{"location":"Containers/Node-RED/#step-2-exchange-keys-with-target-hosts-once-per-target-host","text":"Node-RED's public key needs to be copied to the user account on each target machine where you want a Node-RED \"exec\" node to be able to execute commands. At the same time, the Node-RED container needs to learn the public host key of the target machine. The ssh-copy-id command does both steps. The required syntax is: $ docker exec -it nodered ssh-copy-id \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb Examples: $ docker exec -it nodered ssh-copy-id pi@iot-dev.mydomain.com $ docker exec -it nodered ssh-copy-id pi@192.168.132.9 The output will be something similar to the following: /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/root/.ssh/id_ed25519.pub\" The authenticity of host 'iot-dev.mydomain.com (192.168.132.9)' can't be established. ED25519 key fingerprint is SHA256:HVoeowZ1WTSG0qggNsnGwDA6acCd/JfVLZsNUv4hjNg. Are you sure you want to continue connecting (yes/no/[fingerprint])? Respond to the prompt by typing \"yes\" and press return. The output continues: /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed expr: warning: '^ERROR: ': using '^' as the first character of a basic regular expression is not portable; it is ignored /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys pi@iot-dev.mydomain.com's password: The response may look like it contains errors but those can be ignored. Enter the password you use to login as \u00abUSERID\u00bb on \u00abHOSTADDR\u00bb and press return. Normal completion looks similar to this: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh 'pi@iot-dev.mydomain.com'\" and check to make sure that only the key(s) you wanted were added. If you do not see an indication that a key has been added, you may need to retrace your steps.","title":"Step 2: Exchange keys with target hosts (once per target host)"},{"location":"Containers/Node-RED/#step-3-perform-the-recommended-test","text":"The output above recommends a test. The test needs to be run inside the Node-RED container so the syntax is: $ docker exec -it nodered ssh \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb ls -1 /home/pi/IOTstack Examples: $ docker exec -it nodered ssh pi@iot-dev.mydomain.com ls -1 /home/pi/IOTstack $ docker exec -it nodered ssh pi@192.168.132.9 ls -1 /home/pi/IOTstack You should not be prompted for a password. If you are, you may need to retrace your steps. If everything works as expected, you should see a list of the files in your IOTstack folder. Assuming success, think about what just happened? You told SSH inside the Node-RED container to run the ls command outside the container on your Raspberry Pi. You broke through the containerisation.","title":"Step 3: Perform the recommended test"},{"location":"Containers/Node-RED/#understanding-whats-where-and-what-each-file-does","text":"","title":"Understanding what's where and what each file does"},{"location":"Containers/Node-RED/#what-files-are-where","text":"Six files are relevant to Node-RED's ability to execute commands outside of container-space: in /etc/ssh : ssh_host_ed25519_key is the Raspberry Pi's private host key ssh_host_ed25519_key.pub is the Raspberry Pi's public host key Those keys were created when your Raspberry Pi was initialised. They are unique to the host. Unless you take precautions, those keys will change whenever your Raspberry Pi is rebuilt from scratch and that will stop SSH from working. You can recover by re-running ssh-copy-id . in ~/IOTstack/volumes/nodered/ssh : id_ed25519 is the Node-RED container's private key id_ed25519.pub is the Node-RED container's public key Those keys were created when you generated the SSH key-pair for Node-RED. They are unique to Node-RED but will follow the container in backups and will work on the same machine, or other machines, if you restore the backup. It does not matter if the Node-RED container is rebuilt or if a new version of Node-RED comes down from DockerHub. These keys will remain valid until lost or overwritten. If you lose or destroy these keys, SSH will stop working. You can recover by generating new keys and then re-running ssh-copy-id . known_hosts The known_hosts file contains a copy of the Raspberry Pi's public host key. It was put there by ssh-copy-id . If you lose this file or it gets overwritten, SSH will still work but will re-prompt for authorisation to connect. This works when you are running commands from docker exec -it but not when running commands from an exec node. Note that authorising the connection at the command line (\"Are you sure you want to continue connecting?\") will auto-repair the known_hosts file. in ~/.ssh/ : authorized_keys That file contains a copy of the Node-RED container's public key. It was put there by ssh-copy-id . Pay attention to the path. It implies that there is one authorized_keys file per user, per target host. If you lose this file or it gets overwritten, SSH will still work but will ask for the password for \u00abUSERID\u00bb. This works when you are running commands from docker exec -it but not when running commands from an exec node. Note that providing the correct password at the command line will auto-repair the authorized_keys file.","title":"What files are where"},{"location":"Containers/Node-RED/#what-each-file-does","text":"SSH running inside the Node-RED container uses the Node-RED container's private key to provide assurance to SSH running outside the container that it (the Node-RED container) is who it claims to be. SSH running outside container-space verifies that assurance by using its copy of the Node-RED container's public key in authorized_keys . SSH running outside container-space uses the Raspberry Pi's private host key to provide assurance to SSH running inside the Node-RED container that it (the RPi) is who it claims to be. SSH running inside the Node-RED container verifies that assurance by using its copy of the Raspberry Pi's public host key stored in known_hosts .","title":"What each file does"},{"location":"Containers/Node-RED/#config-file-optional","text":"You don't have to do this step but it will simplify your exec node commands and reduce your maintenance problems if you do. At this point, SSH commands can be executed from inside the container using this syntax: # ssh \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb \u00abCOMMAND\u00bb A config file is needed to achieve the task goal of the simpler syntax: # ssh \u00abHOSTNAME\u00bb \u00abCOMMAND\u00bb A config file does not just simplify connection commands. It provides isolation between the \u00abHOSTNAME\u00bb and \u00abHOSTADDR\u00bb such that you only have a single file to change if your \u00abHOSTADDR\u00bb changes (eg new IP address or fully qualified domain name). It also exposes less about your network infrastructure when you share your flows. The goal is to set up this file: -rw-r--r-- 1 root root ~/IOTstack/volumes/nodered/ssh/config The file needs the ownership and permissions shown. There are several ways of going about this and you are free to choose the one that works for you. The method described here creates the file first, then sets correct ownership and permissions, and then moves the file into place. Start in a directory where you can create a file without needing sudo . The IOTstack folder is just as good as anywhere else: $ cd ~/IOTstack $ touch config Select the following text, copy it to the clipboard. host \u00abHOSTNAME\u00bb hostname \u00abHOSTADDR\u00bb user \u00abUSERID\u00bb IdentitiesOnly yes IdentityFile /root/.ssh/id_ed25519 Open ~/IOTstack/config in your favourite text editor and paste the contents of the clipboard. Replace the \u00abdelimited\u00bb keys. Completed examples: If you are using the \u00abHOSTFQDN\u00bb form: host iot-dev hostname iot-dev.mydomain.com user pi IdentitiesOnly yes IdentityFile /root/.ssh/id_ed25519 If you are using the \u00abHOSTIP\u00bb form: host iot-dev hostname 192.168.132.9 user pi IdentitiesOnly yes IdentityFile /root/.ssh/id_ed25519 Save the file. Change the config file's ownership and permissions, and move it into the correct directory: $ chmod 644 config $ sudo chown root:root config $ sudo mv config ./volumes/nodered/ssh","title":"Config file (optional)"},{"location":"Containers/Node-RED/#re-test-with-config-file-in-place","text":"The previous test used this syntax: $ docker exec nodered ssh \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb ls -1 /home/pi/IOTstack Now that the config file is in place, the syntax changes to: $ docker exec nodered ssh \u00abHOSTNAME\u00bb ls -1 /home/pi/IOTstack Example: $ docker exec nodered ssh iot-dev ls -1 /home/pi/IOTstack The result should be the same as the earlier test.","title":"Re-test with config file in place"},{"location":"Containers/Node-RED/#a-test-flow","text":"In the Node-RED GUI: Click the \"+\" to create a new, empty flow. Drag the following nodes onto the canvas: One \"inject\" node Two \"exec\" nodes Two \"debug\" nodes Wire the outlet of the \"inject\" node to the inlet of both \"exec\" nodes. Wire the uppermost \"stdout\" outlet of the first \"exec\" node to the inlet of the first \"debug\" node. Repeat step 4 with the other \"exec\" and \"debug\" node. Open the first \"exec\" node and: set the \"command\" field to: grep \"^PRETTY_NAME=\" /etc/os-release - turn off the \"append msg.payload\" checkbox - set the timeout to a reasonable value (eg 10 seconds) - click \"Done\". 7. Repeat step 6 with the other \"exec\" node, with one difference: - set the \"command\" field to: ssh iot-dev grep \"^PRETTY_NAME=\" /etc/os-release Click the Deploy button. Set the right hand panel to display debug messages. Click the touch panel of the \"inject\" node to trigger the flow. Inspect the result in the debug panel. You should see payload differences similar to the following: PRETTY_NAME=\"Alpine Linux v3.11\" PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" The first line is the result of running the command inside the Node-RED container. The second line is the result of running the same command outside the Node-RED container on the Raspberry Pi.","title":"A test flow"},{"location":"Containers/Node-RED/#suppose-you-want-to-add-another-hostname","text":"Exchange keys with the new target host using: $ docker exec -it nodered ssh-copy-id \u00abUSERID\u00bb@\u00abHOSTADDR\u00bb Edit the config file at the path: ~/IOTstack/volumes/nodered/ssh/config to define the new host. Remember to use sudo to edit the file. There is no need to restart Node-RED or recreate the container.","title":"Suppose you want to add another \u00abHOSTNAME\u00bb"},{"location":"Containers/Node-RED/#upgrading-node-red","text":"You can update most containers like this: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. This strategy doesn't work for Node-RED. The local image ( iotstack_nodered ) does not exist on DockerHub so there is no way for the pull to sense when a newer version becomes available. The only way to know when an update to Node-RED is available is to check the nodered/node-red tags page on DockerHub . Once a new version appears on DockerHub , you can upgrade Node-RED like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull nodered $ docker-compose up -d nodered $ docker system prune Breaking it down into parts: build causes the named container to be rebuilt; --no-cache tells the Dockerfile process that it must not take any shortcuts. It really must rebuild the local image ; --pull tells the Dockerfile process to actually check with DockerHub to see if there is a later version of the base image and, if so, to download it before starting the build; nodered is the named container argument required by the build command. Your existing Node-RED container continues to run while the rebuild proceeds. Once the freshly-built local image is ready, the up tells docker-compose to do a new-for-old swap. There is barely any downtime for your Node-RED service. The prune is the simplest way of cleaning up old images. Sometimes you need to run this twice, the first time to clean up the old local image, the second time for the old base image. Whether an old base image exists depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"Upgrading Node-RED"},{"location":"Containers/Node-RED/#customising-node-red","text":"You customise your local image of Node-RED by making changes to: ~/IOTstack/services/nodered/Dockerfile You apply a Dockerfile change like this: $ cd ~/IOTstack $ docker-compose up --build -d nodered $ docker system prune The --build option on the up command (as distinct from a docker-compose build command) works in this situation because you've made a substantive change to your Dockerfile .","title":"Customising Node-RED"},{"location":"Containers/Node-RED/#nodejs-version","text":"Out of the box, IOTstack starts the Node-RED Dockerfile with: FROM nodered/node-red:latest-12 The -12 suffix means \" node.js is pinned at version 12.x.x\". That's the latest version of node.js that Node-RED currently supports. If you want to check which version of node.js is installed on your system, you can do it like this: $ docker exec nodered node --version In the unlikely event that you need to run an add-on node that needs version 10 of node.js , you can pin to version 10.x.x by changing the first line of your Dockerfile like this: FROM nodered/node-red:latest-10 Once you have made that change, follow the steps at apply Dockerfile changes .","title":"Node.js version"},{"location":"Containers/Node-RED/#adding-extra-packages","text":"As well as providing the Node-RED service, the nodered container is an excellent testbed. Installing the DNS tools, Mosquitto clients and tcpdump will help you to figure out what is going on inside container-space. There are two ways to add extra packages. The first method is to add them to the running container. For example, to add the Mosquitto clients: $ docker exec nodered apk add --no-cache mosquitto-clients The \"apk\" implies that the Node-RED container is based on Alpine Linux. Keep that in mind when you search for instructions on installing packages. Packages installed this way will persist until the container is re-created (eg a down and up of the stack, or a reboot of your Raspberry Pi). This is a good choice if you only want to run a quick experiment. The second method changes the Dockerfile to add the packages permanently to your build. You just append the packages to the end of the existing apk add : RUN apk update && apk add --no-cache eudev-dev mosquitto-clients bind-tools tcpdump You can add as many extra packages as you like. They will persist until you change the Dockerfile again. Once you have made this change, follow the steps at apply Dockerfile changes .","title":"Adding extra packages"},{"location":"Containers/Node-RED/#adding-extra-nodes","text":"You can install nodes by: Adding nodes to the Dockerfile and then following the steps at apply Dockerfile changes . This is also what will happen if you re-run the menu and change the selected nodes, except that the menu will also blow away any other customisations you may have made to your Dockerfile . Adding, removing or updating nodes in Manage Palette. Node-RED will remind you to restart Node-RED and that is something you have to do by hand: $ cd ~/IOTstack $ docker-compose restart nodered Note: Some users have reported misbehaviour from Node-RED if they do too many iterations of: [ make a single change in Manage Palette ] $ docker-compose restart nodered [ make a single change in Manage Palette ] $ docker-compose restart nodered \u2026 It is better to: [ do ALL your Manage Palette changes ] $ docker-compose restart nodered Installing nodes inside the container via npm: $ docker exec -it nodered bash # cd /data # npm install \u00abnode-name\u00bb /data # exit $ cd ~/IOTstack $ docker-compose restart nodered Note: You must put the /data onto the end of the npm install command. Any formula you find on the web will not include this. You have to remember to do it yourself! See also the note above about restarting too frequently. You can use this approach if you need to force the installation of a specific version (which you don't appear to be able to do in Manage Palette). For example, to install version 4.0.0 of the \"moment\" node: # npm install node-red-contrib-moment@4.0.0 /data There is no real difference between the methods. Some nodes (eg \"node-red-contrib-generic-ble\" and \"node-red-node-sqlite\" must be installed by Dockerfile but the only way of finding out if a node must be installed via Dockerfile is to try Manage Palette and find that it doesn't work. Aside from the exception cases that require Dockerfile or where you need to force a specific version, it is quicker to install nodes via Manage Palette and applying updates is a bit easier too. But it's really up to you. If you're wondering about \"backup\", nodes installed via: Dockerfile \u2013 implicitly backed up when the Dockerfile is backed-up. Manage Palette or npm install \u2013 explicitly backed up when the ~/IOTstack/volumes directory is backed-up. Basically, if you're running IOTstack backups then your add-on nodes will be backed-up.","title":"Adding extra nodes"},{"location":"Containers/Node-RED/#node-precedence","text":"Add-on nodes that are installed via Dockerfile wind up at the internal path: /usr/src/node-red/node_modules Add-on nodes installed via Manage Palette wind up at the external path: ~/IOTstack/volumes/nodered/data/node_modules The Compose file volumes mapping: ./volumes/nodered/data:/data implies that add-on nodes installed via Manage Palette are made available to Node-RED at the internal path: /data/node_modules Because there are two places, this invites the question of what happens if a given node is installed in both? The answer is that add-ons installed at: /data/node_modules take precedence over those installed at: /usr/src/node-red/node_modules Or, to put it more simply: in any contest, Manage Palette prevails over Dockerfile .","title":"Node precedence"},{"location":"Containers/Node-RED/#resolving-node-duplication","text":"Sometimes, even when you are 100% certain that you didn't do it, an add-on node will turn up in both places. There is probably some logical reason for this but I don't know what it is. The problem this creates is that a later version of an add-on node installed via Dockerfile will be blocked by the presence of an older version of that node in: ~/IOTstack/volumes/nodered/data/node_modules The nodered_list_installed_nodes.sh script helps discover when this situation exists. For example: $ ~/IOTstack/scripts/nodered_list_installed_nodes.sh Nodes installed by Dockerfile INSIDE the container at /usr/src/node-red/node_modules ACTIVE: node-red-admin ACTIVE: node-red-configurable-ping ACTIVE: node-red-contrib-boolean-logic ACTIVE: node-red-contrib-generic-ble ACTIVE: node-red-contrib-influxdb ACTIVE: node-red-dashboard BLOCKED: node-red-node-email ACTIVE: node-red-node-pi-gpiod ACTIVE: node-red-node-rbe ACTIVE: node-red-node-sqlite ACTIVE: node-red-node-tail Nodes installed by \u00abManage Palette\u00bb OUTSIDE the container at /home/pi/IOTstack/volumes/nodered/data/node_modules node-red-contrib-boolean-logic-ultimate node-red-contrib-chartjs node-red-node-email node-red-contrib-md5 node-red-contrib-moment node-red-contrib-pushsafer Notice how node-red-node-email appears in both lists. To fix this problem: Move into the correct external directory: $ cd ~/IOTstack/volumes/nodered/data/node_modules Create a sub-directory to be the equivalent of a local trash can: $ sudo mkdir duplicates Move each duplicate node into the duplicates directory. For example, to move node-red-node-email you would: $ sudo mv node-red-node-email duplicates Tell Node-RED to restart. This causes it to forget about the nodes which have just been moved out of the way: $ docker-compose -f ~/IOTstack/docker-compose.yml restart nodered Finish off by erasing the duplicates folder: $ sudo rm -rf duplicates Always be extremely careful with any rm -rf , particularly when it is coupled with a sudo . Double-check your work before you press return.","title":"Resolving node duplication"},{"location":"Containers/Octoprint/","text":"OctoPrint \u2013 the snappy web interface for your 3D printer \u00b6 References \u00b6 OctoPrint home page OctoPrint Community Forum DockerHub octoprint/octoprint GitHub OctoPrint/octoprint-docker Device mappings \u00b6 When you select \"OctoPrint\" in the IOTstack menu, the service definition in your docker-compose.yml , contains the following under the devices: heading: devices: - /dev/ttyAMA0:/dev/ttyACM0 # - /dev/video0:/dev/video0 the /dev/ttyAMA0:/dev/ttyACM0 mapping \u00b6 The /dev/ttyAMA0:/dev/ttyACM0 mapping should be read as saying \"the physical Raspberry Pi device /dev/ttyAMA0 is mapped to the logical OctoPrint container device /dev/ttyACM0 \". The /dev/ttyAMA0 device is used as a default because it is always present on Raspbian. If you bring up your container like that, the mapping will succeed and the container is unlikely to go into a restart loop. However, the OctoPrint container is unlikely to be able to connect to your 3D printer via /dev/ttyAMA0 for the very simple reason that that is not how 3D printers usually appear on the Raspberry Pi. You need to work out how your printer presents itself and change the device mapping accordingly. option 1 - /dev/ttyUSBn \u00b6 Using \"ttyUSBn\" will \"work\" but, because of the inherent variability in the name, this approach is not recommended. The \"n\" in the \"ttyUSBn\" can vary depending on which USB devices are attached to your Raspberry Pi and the order in which they are attached. The \"n\" may also change as you add and remove devices. If the OctoPrint container is up when the device number changes, the container will crash, and it will either go into a restart loop if you try to bring it up when the expected device is not \"there\", or will try to communicate with a device that isn't your 3D printer. option 2 - /dev/serial/by-id/xxxxxxxx \u00b6 The \"xxxxxxxx\" is (usually) unique to your 3D printer. To find it, connect your printer to your Raspberry Pi, then run the command: $ ls -1 /dev/serial/by-id You will get an answer like this: usb-Silicon_Labs_CP2102N_USB_to_UART_Bridge_Controller_3b14eaa48a154d5e87032d59459d5206-if00-port0 Note: If you have multiple serial devices attached, you will get multiple lines in the output. It is up to you to sort out which one belongs to your 3D printer, possibly by disconnecting and re-attaching the printer and observing how the list changes. The uniqueness of device IDs is under the control of the device manufacturer. Each manufacturer should ensure their devices are unique but some manufacturers are more diligent than others. Assuming the above example output was the answer, edit docker-compose.yml to look like this: devices: - /dev/serial/by-id/usb-Silicon_Labs_CP2102N_USB_to_UART_Bridge_Controller_3b14eaa48a154d5e87032d59459d5206-if00-port0:/dev/ttyACM0 Notes: device by-id names follow the device. In other words, if you have two or more Raspberry Pis and a collection of serial devices (3D printers, Zigbee adapters, UARTs, and so on), a 3D printer will always get the same by-id name, irrespective of which Raspberry Pi it is attached to. device by-id names do not persist if the physical device is disconnected. If you switch off your 3D printer or disconnect the USB cable while the OctoPrint container is running, the container will crash. option 3 - /dev/humanReadableName \u00b6 Suppose your 3D printer is a MasterDisaster5000Pro, and that you would like to be able to set up the device to use a human-readable name like: /dev/MasterDisaster5000Pro Start by disconnecting your 3D printer from your Raspberry Pi. Next, run this command: $ tail -f /var/log/messages Connect your 3D printer and observe the log output. You are interested in messages that look like this: mmm dd hh:mm:ss mypi kernel: [423839.626522] cp210x 1-1.1.3:1.0: device disconnected mmm dd hh:mm:ss mypi kernel: [431265.973308] usb 1-1.1.3: new full-speed USB device number 10 using dwc_otg mmm dd hh:mm:ss mypi kernel: [431266.109418] usb 1-1.1.3: New USB device found, idVendor=dead, idProduct=beef, bcdDevice= 1.00 mmm dd hh:mm:ss mypi kernel: [431266.109439] usb 1-1.1.3: New USB device strings: Mfr=1, Product=2, SerialNumber=3 mmm dd hh:mm:ss mypi kernel: [431266.109456] usb 1-1.1.3: Product: CP2102N USB to UART Bridge Controller mmm dd hh:mm:ss mypi kernel: [431266.109471] usb 1-1.1.3: Manufacturer: Silicon Labs mmm dd hh:mm:ss mypi kernel: [431266.109486] usb 1-1.1.3: SerialNumber: cafe80facefeed mmm dd hh:mm:ss mypi kernel: [431266.110657] cp210x 1-1.1.3:1.0: cp210x converter detected mmm dd hh:mm:ss mypi kernel: [431266.119225] usb 1-1.1.3: cp210x converter now attached to ttyUSB0 and, in particular, these two lines: \u2026 New USB device found, idVendor=dead, idProduct=beef, bcdDevice= 1.00 \u2026 SerialNumber: cafe80facefeed Terminate the tail command by pressing Control+C. Use this line as a template: SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"\u00abidVendor\u00bb\", ATTRS{idProduct}==\"\u00abidProduct\u00bb\", ATTRS{serial}==\"\u00abSerialNumber\u00bb\", SYMLINK+=\"\u00absensibleName\u00bb\" Replace the \u00abdelimited\u00bb values with those you see in the log output. For example, given the above log output, and the desire to associate your 3D printer with the human-readable name of \"MasterDisaster5000Pro\", the result would be: SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"dead\", ATTRS{idProduct}==\"beef\", ATTRS{serial}==\"cafe80facefeed\", SYMLINK+=\"MasterDisaster5000Pro\" Next, ensure the required file exists by executing the following command: $ sudo touch /etc/udev/rules.d/99-usb-serial.rules If the file does not exist already, the touch command creates an empty file, owned by root, with mode 644 (rw-r--r--) permissions (all of which are correct). Use sudo and your favourite text editor to edit /etc/udev/rules.d/99-usb-serial.rules and insert the \"SUBSYSTEM==\" line you prepared earlier into that file, then save the file. Rules files are read on demand so there is no start or reload command to execute. Check your work by disconnecting, then re-connecting your 3D printer, and then run: $ ls /dev You should expect to see the human-readable name you chose in the list of devices. You can then edit docker-compose.yml to use the name in the device mapping. devices: - /dev/MasterDisaster5000Pro:/dev/ttyACM0 Notes: device names follow the device. In other words, if you have two or more Raspberry Pis and a collection of serial devices (3D printers, Zigbee adapters, UARTs, and so on), you can build a single 99-usb-serial.rules file that you install on all of your Raspberry Pis. Then, you can attach a named device to any of your Raspberry Pis and it will always get the same name. device names do not persist if the physical device is disconnected. If you switch off your 3D printer or disconnect the USB cable while the OctoPrint container is running, the container will crash. the /dev/video0:/dev/video0 mapping \u00b6 The /dev/video0 device is assumed to be an official Raspberry Pi camera attached via ribbon cable. See the Webcams topic of the Octoprint Community Forum for help configuring other kinds of cameras. The OctoPrint docker image includes an MJPG streamer. You do not need to run another container with a streamer unless you want to. To activate a Raspberry Pi camera attached via ribbon cable: Follow the instructions at raspberrypi.org to connect and test the camera. There are guides on YouTube ( example ) if you need help working out how to insert the ribbon cable. Confirm the presence of /dev/video0 . Edit docker-compose.yml and uncomment all of the commented-out lines in the following: devices: # - /dev/video0:/dev/video0 environment: # - ENABLE_MJPG_STREAMER=true # - MJPG_STREAMER_INPUT=-r 640x480 -f 10 -y # - CAMERA_DEV=/dev/video0 Note: The device path on the right hand side of the CAMERA_DEV environment variable corresponds with the right hand side (ie after the colon) of the device mapping. There should be no reason to change either. The three environment variables are required: environment: - ENABLE_MJPG_STREAMER=true - MJPG_STREAMER_INPUT=-r 640x480 -f 10 -y - CAMERA_DEV=/dev/video0 The \"640x480\" MJPG_STREAMER_INPUT settings will probably result in your camera feed being \"letterboxed\" but they will get you started. A full list of options is at mjpg-streamer-configuration-options . The typical specs for a baseline Raspberry Pi camera are: 1080p 720p 5Mp Webcam Max resolution: 2592x1944 Max frame rate: VGA 90fps, 1080p 30fps CODEC: MJPG H.264 AVC For that type of camera, the following is probably more appropriate: - MJPG_STREAMER_INPUT=-r 1152x648 -f 10 The resolution of 1152x648 is 60% of 1080p 1920x1080 and does not cause letterboxing. The resolution and rate of 10 frames per second won't over-tax your communications links, and the camera is MJPEG-capable so it does not need the -y option. Practical usage \u00b6 starting the OctoPrint container \u00b6 To start a print session: Turn the 3D printer on. Bring up the container: $ cd ~/IOTstack $ docker-compose up -d octoprint If you try to start the OctoPrint container before your 3D printer has been switched on and the USB interface has registered with the Raspberry Pi, the container will go into a restart loop. first run \u2013 the Setup Wizard \u00b6 Use a browser to point to port 9980 on your Raspberry Pi. For example: http://raspberrypi.local:9980 This will launch the \"Setup Wizard\". Click the \"Next\" button until you reach the \"Access Control\" screen: Define a Username and Password, and keep a record of your decisions. Click \"Create Account\". Ignore the alarming popup alert by clicking \"Ignore\". This alert is a result of OctoPrint running in a Docker container. Click \"Next\". At the \"Online Connectivity Check\" screen: Click \"Disable Connectivity Check\". Click \"Next\". At the \"Configure Anonymous Usage Tracking\" and \"Configure plugin blacklist processing\" screens: Make a decision about whether you want the feature enabled or disabled and click the appropriate button. Click \"Next\". At the \"Set up your printer profile\" screen: It is probably a good idea to visit the tabs and set values appropriate to your printer (build volume, at least). Click \"Next\". At the \"Server Commands\" screen: Enter the following in the \"Restart OctoPrint\" field: s6-svc -r /var/run/s6/services/octoprint Click \"Next\". At the \"Webcam & Timelapse Recordings\" screen, and assuming you are configuring a PiCamera: Enter the following in the \"Stream URL\" field: /webcam/?action=stream Click the \"Test\" button to confirm that the camera is working, then click \"Close\". Enter the following in the \"Snapshot URL\" field: http://localhost:8080/?action=snapshot Click the \"Test\" button to confirm that the camera is working, then click \"Close\". Enter the following in the \"Path to FFMPEG\" field: /usr/bin/ffmpeg The expected result is the message \"The path is valid\". Click \"Next\". Click \"Finish\" then click the button to reload the user interface. after the first run \u00b6 Use a browser to point to port 9980 on your Raspberry Pi. For example: http://raspberrypi.local:9980 Supply your user credentials and login. popup messages \u00b6 OctoPrint will display numerous messages in popup windows. These generally fall into two categories: Messages that refer to updates; and Messages that refer to other events. In general, you can ignore messages about updates. You will get all updates automatically the next time the octoprint-docker container is rebuilt and pushed to DockerHub. You can, if you wish, allow an update to proceed. It might be appropriate to do that if you want to test an update. Just be aware that: Updates are ephemeral and will disappear the next time the Octoprint container is created. Updates can change the structure of the persistent storage area in a way which can't be undone, and which may prevent the Octoprint container from starting the next time it is created. In other words, if you want to trial an update, take a backup of OctoPrint's persistent storage area first . restarting the OctoPrint container \u00b6 You can restart the OctoPrint service in two ways: via the Raspberry Pi command line; or via the OctoPrint user interface. Whichever method you choose will result in a refresh of the OctoPrint user interface and you will need to follow the prompts to reload your browser page. restarting via the command line \u00b6 Run the following commands: $ cd ~/IOTstack $ docker-compose restart octoprint restarting via OctoPrint user interface \u00b6 From the \"System\" icon in the OctoPrint toolbar (looks like a power button symbol): Choose \"Restart OctoPrint\". Note: If you do not see the \"System\" icon in the toolbar, fix it line this: Click the \"Settings\" icon (looks like a wrench) in the OctoPrint toolbar. Choose \"Server\". Enter the following into the \"Restart OctoPrint\" field: s6-svc -r /var/run/s6/services/octoprint Click \"Save\". stopping the OctoPrint container \u00b6 Unless you intend to leave your printer switched on 24 hours a day, you will also need to be careful when you switch off the printer: Terminate the container: $ cd ~/IOTstack $ docker-compose stop octoprint $ docker-compose rm -f octoprint Turn the 3D printer off. If you turn the printer off without terminating the container, you will crash the container. Video feed (built-in camera interface) \u00b6 You can view the video feed independently of the OctoPrint web interface like this: http://raspberrypi.local:9980/webcam/?action=stream Silencing the security warning \u00b6 OctoPrint assumes it is running \"natively\" rather than in a container. From a data-communications perspective, OctoPrint (the process running inside the OctoPrint container) sees itself as running on a computer attached to the internal Docker network. When you connect to OctoPrint's web interface from a client device attached to an external network, OctoPrint sees that your source IP address is not on the internal Docker network and it issues a security warning. To silence the warning: Terminate the container if it is running: $ cd ~/IOTstack $ docker-compose stop octoprint $ docker-compose rm -f octoprint use sudo and your favourite text editor to open the following file: ~/IOTstack/volumes/octoprint/octoprint/config.yaml Implement the following pattern: server: \u2026 ipCheck: enabled: true trustedSubnets: - 203.0.132.0/24 Notes: The server: , ipCheck: and enabled: directives may already be in place but the trustedSubnets: directive may not be. Add it, and then add your local subnet(s) where you see the \"192.168.1.0/24\" example. Remember to use spaces in YAML files. Do not use tabs. Save the file. Bring up the container: $ cd ~/IOTstack $ docker-compose up -d octoprint Routine container maintenance \u00b6 You can check for updates like this: $ cd ~/IOTstack $ docker-compose pull octoprint $ docker-compose up -d octoprint $ docker system prune If you forget your username and password \u00b6 You can view a list of usernames like this: $ docker exec octoprint octoprint --basedir /octoprint/octoprint user list To reset a user's password: Use the following line as a template and replace \u00abusername\u00bb and \u00abpassword\u00bb with appropriate values: docker exec octoprint octoprint --basedir /octoprint/octoprint user password --password \u00abpassword\u00bb \u00abusername\u00bb Execute the edited command. For example, to set the password for user \"me\" to \"verySecure\": $ docker exec octoprint octoprint --basedir /octoprint/octoprint user password --password verySecure me Restart OctoPrint: $ cd ~/IOTstack $ docker-compose restart octoprint Note: OctoPrint supports more than one username. To explore the further: $ docker exec octoprint octoprint --basedir /octoprint/octoprint user --help If all else fails\u2026 \u00b6 If the OctoPrint container seems to be misbehaving, you can get a \"clean slate\" by: $ cd ~/IOTstack $ docker-compose stop octoprint $ docker-compose rm -f octoprint $ sudo rm -rf ./volumes/octoprint $ docker-compose up -d octoprint The OctoPrint container is well-behaved and will re-initialise its persistent storage area correctly. OctoPrint will adopt \"first run\" behaviour and display the Setup Wizard.","title":"OctoPrint \u2013 the snappy web interface for your 3D printer"},{"location":"Containers/Octoprint/#octoprint-the-snappy-web-interface-for-your-3d-printer","text":"","title":"OctoPrint \u2013 the snappy web interface for your 3D printer"},{"location":"Containers/Octoprint/#references","text":"OctoPrint home page OctoPrint Community Forum DockerHub octoprint/octoprint GitHub OctoPrint/octoprint-docker","title":"References"},{"location":"Containers/Octoprint/#device-mappings","text":"When you select \"OctoPrint\" in the IOTstack menu, the service definition in your docker-compose.yml , contains the following under the devices: heading: devices: - /dev/ttyAMA0:/dev/ttyACM0 # - /dev/video0:/dev/video0","title":"Device mappings"},{"location":"Containers/Octoprint/#the-devttyama0devttyacm0-mapping","text":"The /dev/ttyAMA0:/dev/ttyACM0 mapping should be read as saying \"the physical Raspberry Pi device /dev/ttyAMA0 is mapped to the logical OctoPrint container device /dev/ttyACM0 \". The /dev/ttyAMA0 device is used as a default because it is always present on Raspbian. If you bring up your container like that, the mapping will succeed and the container is unlikely to go into a restart loop. However, the OctoPrint container is unlikely to be able to connect to your 3D printer via /dev/ttyAMA0 for the very simple reason that that is not how 3D printers usually appear on the Raspberry Pi. You need to work out how your printer presents itself and change the device mapping accordingly.","title":"the /dev/ttyAMA0:/dev/ttyACM0 mapping"},{"location":"Containers/Octoprint/#option-1-devttyusbn","text":"Using \"ttyUSBn\" will \"work\" but, because of the inherent variability in the name, this approach is not recommended. The \"n\" in the \"ttyUSBn\" can vary depending on which USB devices are attached to your Raspberry Pi and the order in which they are attached. The \"n\" may also change as you add and remove devices. If the OctoPrint container is up when the device number changes, the container will crash, and it will either go into a restart loop if you try to bring it up when the expected device is not \"there\", or will try to communicate with a device that isn't your 3D printer.","title":"option 1 - /dev/ttyUSBn"},{"location":"Containers/Octoprint/#option-2-devserialby-idxxxxxxxx","text":"The \"xxxxxxxx\" is (usually) unique to your 3D printer. To find it, connect your printer to your Raspberry Pi, then run the command: $ ls -1 /dev/serial/by-id You will get an answer like this: usb-Silicon_Labs_CP2102N_USB_to_UART_Bridge_Controller_3b14eaa48a154d5e87032d59459d5206-if00-port0 Note: If you have multiple serial devices attached, you will get multiple lines in the output. It is up to you to sort out which one belongs to your 3D printer, possibly by disconnecting and re-attaching the printer and observing how the list changes. The uniqueness of device IDs is under the control of the device manufacturer. Each manufacturer should ensure their devices are unique but some manufacturers are more diligent than others. Assuming the above example output was the answer, edit docker-compose.yml to look like this: devices: - /dev/serial/by-id/usb-Silicon_Labs_CP2102N_USB_to_UART_Bridge_Controller_3b14eaa48a154d5e87032d59459d5206-if00-port0:/dev/ttyACM0 Notes: device by-id names follow the device. In other words, if you have two or more Raspberry Pis and a collection of serial devices (3D printers, Zigbee adapters, UARTs, and so on), a 3D printer will always get the same by-id name, irrespective of which Raspberry Pi it is attached to. device by-id names do not persist if the physical device is disconnected. If you switch off your 3D printer or disconnect the USB cable while the OctoPrint container is running, the container will crash.","title":"option 2 - /dev/serial/by-id/xxxxxxxx"},{"location":"Containers/Octoprint/#option-3-devhumanreadablename","text":"Suppose your 3D printer is a MasterDisaster5000Pro, and that you would like to be able to set up the device to use a human-readable name like: /dev/MasterDisaster5000Pro Start by disconnecting your 3D printer from your Raspberry Pi. Next, run this command: $ tail -f /var/log/messages Connect your 3D printer and observe the log output. You are interested in messages that look like this: mmm dd hh:mm:ss mypi kernel: [423839.626522] cp210x 1-1.1.3:1.0: device disconnected mmm dd hh:mm:ss mypi kernel: [431265.973308] usb 1-1.1.3: new full-speed USB device number 10 using dwc_otg mmm dd hh:mm:ss mypi kernel: [431266.109418] usb 1-1.1.3: New USB device found, idVendor=dead, idProduct=beef, bcdDevice= 1.00 mmm dd hh:mm:ss mypi kernel: [431266.109439] usb 1-1.1.3: New USB device strings: Mfr=1, Product=2, SerialNumber=3 mmm dd hh:mm:ss mypi kernel: [431266.109456] usb 1-1.1.3: Product: CP2102N USB to UART Bridge Controller mmm dd hh:mm:ss mypi kernel: [431266.109471] usb 1-1.1.3: Manufacturer: Silicon Labs mmm dd hh:mm:ss mypi kernel: [431266.109486] usb 1-1.1.3: SerialNumber: cafe80facefeed mmm dd hh:mm:ss mypi kernel: [431266.110657] cp210x 1-1.1.3:1.0: cp210x converter detected mmm dd hh:mm:ss mypi kernel: [431266.119225] usb 1-1.1.3: cp210x converter now attached to ttyUSB0 and, in particular, these two lines: \u2026 New USB device found, idVendor=dead, idProduct=beef, bcdDevice= 1.00 \u2026 SerialNumber: cafe80facefeed Terminate the tail command by pressing Control+C. Use this line as a template: SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"\u00abidVendor\u00bb\", ATTRS{idProduct}==\"\u00abidProduct\u00bb\", ATTRS{serial}==\"\u00abSerialNumber\u00bb\", SYMLINK+=\"\u00absensibleName\u00bb\" Replace the \u00abdelimited\u00bb values with those you see in the log output. For example, given the above log output, and the desire to associate your 3D printer with the human-readable name of \"MasterDisaster5000Pro\", the result would be: SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"dead\", ATTRS{idProduct}==\"beef\", ATTRS{serial}==\"cafe80facefeed\", SYMLINK+=\"MasterDisaster5000Pro\" Next, ensure the required file exists by executing the following command: $ sudo touch /etc/udev/rules.d/99-usb-serial.rules If the file does not exist already, the touch command creates an empty file, owned by root, with mode 644 (rw-r--r--) permissions (all of which are correct). Use sudo and your favourite text editor to edit /etc/udev/rules.d/99-usb-serial.rules and insert the \"SUBSYSTEM==\" line you prepared earlier into that file, then save the file. Rules files are read on demand so there is no start or reload command to execute. Check your work by disconnecting, then re-connecting your 3D printer, and then run: $ ls /dev You should expect to see the human-readable name you chose in the list of devices. You can then edit docker-compose.yml to use the name in the device mapping. devices: - /dev/MasterDisaster5000Pro:/dev/ttyACM0 Notes: device names follow the device. In other words, if you have two or more Raspberry Pis and a collection of serial devices (3D printers, Zigbee adapters, UARTs, and so on), you can build a single 99-usb-serial.rules file that you install on all of your Raspberry Pis. Then, you can attach a named device to any of your Raspberry Pis and it will always get the same name. device names do not persist if the physical device is disconnected. If you switch off your 3D printer or disconnect the USB cable while the OctoPrint container is running, the container will crash.","title":"option 3 - /dev/humanReadableName"},{"location":"Containers/Octoprint/#the-devvideo0devvideo0-mapping","text":"The /dev/video0 device is assumed to be an official Raspberry Pi camera attached via ribbon cable. See the Webcams topic of the Octoprint Community Forum for help configuring other kinds of cameras. The OctoPrint docker image includes an MJPG streamer. You do not need to run another container with a streamer unless you want to. To activate a Raspberry Pi camera attached via ribbon cable: Follow the instructions at raspberrypi.org to connect and test the camera. There are guides on YouTube ( example ) if you need help working out how to insert the ribbon cable. Confirm the presence of /dev/video0 . Edit docker-compose.yml and uncomment all of the commented-out lines in the following: devices: # - /dev/video0:/dev/video0 environment: # - ENABLE_MJPG_STREAMER=true # - MJPG_STREAMER_INPUT=-r 640x480 -f 10 -y # - CAMERA_DEV=/dev/video0 Note: The device path on the right hand side of the CAMERA_DEV environment variable corresponds with the right hand side (ie after the colon) of the device mapping. There should be no reason to change either. The three environment variables are required: environment: - ENABLE_MJPG_STREAMER=true - MJPG_STREAMER_INPUT=-r 640x480 -f 10 -y - CAMERA_DEV=/dev/video0 The \"640x480\" MJPG_STREAMER_INPUT settings will probably result in your camera feed being \"letterboxed\" but they will get you started. A full list of options is at mjpg-streamer-configuration-options . The typical specs for a baseline Raspberry Pi camera are: 1080p 720p 5Mp Webcam Max resolution: 2592x1944 Max frame rate: VGA 90fps, 1080p 30fps CODEC: MJPG H.264 AVC For that type of camera, the following is probably more appropriate: - MJPG_STREAMER_INPUT=-r 1152x648 -f 10 The resolution of 1152x648 is 60% of 1080p 1920x1080 and does not cause letterboxing. The resolution and rate of 10 frames per second won't over-tax your communications links, and the camera is MJPEG-capable so it does not need the -y option.","title":"the /dev/video0:/dev/video0 mapping"},{"location":"Containers/Octoprint/#practical-usage","text":"","title":"Practical usage"},{"location":"Containers/Octoprint/#starting-the-octoprint-container","text":"To start a print session: Turn the 3D printer on. Bring up the container: $ cd ~/IOTstack $ docker-compose up -d octoprint If you try to start the OctoPrint container before your 3D printer has been switched on and the USB interface has registered with the Raspberry Pi, the container will go into a restart loop.","title":"starting the OctoPrint container"},{"location":"Containers/Octoprint/#first-run-the-setup-wizard","text":"Use a browser to point to port 9980 on your Raspberry Pi. For example: http://raspberrypi.local:9980 This will launch the \"Setup Wizard\". Click the \"Next\" button until you reach the \"Access Control\" screen: Define a Username and Password, and keep a record of your decisions. Click \"Create Account\". Ignore the alarming popup alert by clicking \"Ignore\". This alert is a result of OctoPrint running in a Docker container. Click \"Next\". At the \"Online Connectivity Check\" screen: Click \"Disable Connectivity Check\". Click \"Next\". At the \"Configure Anonymous Usage Tracking\" and \"Configure plugin blacklist processing\" screens: Make a decision about whether you want the feature enabled or disabled and click the appropriate button. Click \"Next\". At the \"Set up your printer profile\" screen: It is probably a good idea to visit the tabs and set values appropriate to your printer (build volume, at least). Click \"Next\". At the \"Server Commands\" screen: Enter the following in the \"Restart OctoPrint\" field: s6-svc -r /var/run/s6/services/octoprint Click \"Next\". At the \"Webcam & Timelapse Recordings\" screen, and assuming you are configuring a PiCamera: Enter the following in the \"Stream URL\" field: /webcam/?action=stream Click the \"Test\" button to confirm that the camera is working, then click \"Close\". Enter the following in the \"Snapshot URL\" field: http://localhost:8080/?action=snapshot Click the \"Test\" button to confirm that the camera is working, then click \"Close\". Enter the following in the \"Path to FFMPEG\" field: /usr/bin/ffmpeg The expected result is the message \"The path is valid\". Click \"Next\". Click \"Finish\" then click the button to reload the user interface.","title":"first run \u2013 the Setup Wizard"},{"location":"Containers/Octoprint/#after-the-first-run","text":"Use a browser to point to port 9980 on your Raspberry Pi. For example: http://raspberrypi.local:9980 Supply your user credentials and login.","title":"after the first run"},{"location":"Containers/Octoprint/#popup-messages","text":"OctoPrint will display numerous messages in popup windows. These generally fall into two categories: Messages that refer to updates; and Messages that refer to other events. In general, you can ignore messages about updates. You will get all updates automatically the next time the octoprint-docker container is rebuilt and pushed to DockerHub. You can, if you wish, allow an update to proceed. It might be appropriate to do that if you want to test an update. Just be aware that: Updates are ephemeral and will disappear the next time the Octoprint container is created. Updates can change the structure of the persistent storage area in a way which can't be undone, and which may prevent the Octoprint container from starting the next time it is created. In other words, if you want to trial an update, take a backup of OctoPrint's persistent storage area first .","title":"popup messages"},{"location":"Containers/Octoprint/#restarting-the-octoprint-container","text":"You can restart the OctoPrint service in two ways: via the Raspberry Pi command line; or via the OctoPrint user interface. Whichever method you choose will result in a refresh of the OctoPrint user interface and you will need to follow the prompts to reload your browser page.","title":"restarting the OctoPrint container"},{"location":"Containers/Octoprint/#restarting-via-the-command-line","text":"Run the following commands: $ cd ~/IOTstack $ docker-compose restart octoprint","title":"restarting via the command line"},{"location":"Containers/Octoprint/#restarting-via-octoprint-user-interface","text":"From the \"System\" icon in the OctoPrint toolbar (looks like a power button symbol): Choose \"Restart OctoPrint\". Note: If you do not see the \"System\" icon in the toolbar, fix it line this: Click the \"Settings\" icon (looks like a wrench) in the OctoPrint toolbar. Choose \"Server\". Enter the following into the \"Restart OctoPrint\" field: s6-svc -r /var/run/s6/services/octoprint Click \"Save\".","title":"restarting via OctoPrint user interface"},{"location":"Containers/Octoprint/#stopping-the-octoprint-container","text":"Unless you intend to leave your printer switched on 24 hours a day, you will also need to be careful when you switch off the printer: Terminate the container: $ cd ~/IOTstack $ docker-compose stop octoprint $ docker-compose rm -f octoprint Turn the 3D printer off. If you turn the printer off without terminating the container, you will crash the container.","title":"stopping the OctoPrint container"},{"location":"Containers/Octoprint/#video-feed-built-in-camera-interface","text":"You can view the video feed independently of the OctoPrint web interface like this: http://raspberrypi.local:9980/webcam/?action=stream","title":"Video feed (built-in camera interface)"},{"location":"Containers/Octoprint/#silencing-the-security-warning","text":"OctoPrint assumes it is running \"natively\" rather than in a container. From a data-communications perspective, OctoPrint (the process running inside the OctoPrint container) sees itself as running on a computer attached to the internal Docker network. When you connect to OctoPrint's web interface from a client device attached to an external network, OctoPrint sees that your source IP address is not on the internal Docker network and it issues a security warning. To silence the warning: Terminate the container if it is running: $ cd ~/IOTstack $ docker-compose stop octoprint $ docker-compose rm -f octoprint use sudo and your favourite text editor to open the following file: ~/IOTstack/volumes/octoprint/octoprint/config.yaml Implement the following pattern: server: \u2026 ipCheck: enabled: true trustedSubnets: - 203.0.132.0/24 Notes: The server: , ipCheck: and enabled: directives may already be in place but the trustedSubnets: directive may not be. Add it, and then add your local subnet(s) where you see the \"192.168.1.0/24\" example. Remember to use spaces in YAML files. Do not use tabs. Save the file. Bring up the container: $ cd ~/IOTstack $ docker-compose up -d octoprint","title":"Silencing the security warning"},{"location":"Containers/Octoprint/#routine-container-maintenance","text":"You can check for updates like this: $ cd ~/IOTstack $ docker-compose pull octoprint $ docker-compose up -d octoprint $ docker system prune","title":"Routine container maintenance"},{"location":"Containers/Octoprint/#if-you-forget-your-username-and-password","text":"You can view a list of usernames like this: $ docker exec octoprint octoprint --basedir /octoprint/octoprint user list To reset a user's password: Use the following line as a template and replace \u00abusername\u00bb and \u00abpassword\u00bb with appropriate values: docker exec octoprint octoprint --basedir /octoprint/octoprint user password --password \u00abpassword\u00bb \u00abusername\u00bb Execute the edited command. For example, to set the password for user \"me\" to \"verySecure\": $ docker exec octoprint octoprint --basedir /octoprint/octoprint user password --password verySecure me Restart OctoPrint: $ cd ~/IOTstack $ docker-compose restart octoprint Note: OctoPrint supports more than one username. To explore the further: $ docker exec octoprint octoprint --basedir /octoprint/octoprint user --help","title":"If you forget your username and password"},{"location":"Containers/Octoprint/#if-all-else-fails","text":"If the OctoPrint container seems to be misbehaving, you can get a \"clean slate\" by: $ cd ~/IOTstack $ docker-compose stop octoprint $ docker-compose rm -f octoprint $ sudo rm -rf ./volumes/octoprint $ docker-compose up -d octoprint The OctoPrint container is well-behaved and will re-initialise its persistent storage area correctly. OctoPrint will adopt \"first run\" behaviour and display the Setup Wizard.","title":"If all else fails\u2026"},{"location":"Containers/OpenHab/","text":"openHAB \u00b6 References \u00b6 DockerHub GitHub openHAB website openHAB runs in \"host mode\" so there are no port mappings. The default port bindings on IOTstack are: 4050 - the HTTP port of the web interface (instead of 8080) 4051 - the HTTPS port of the web interface (instead of 8443) 8101 - the SSH port of the Console (since openHAB 2.0.0) 5007 - the LSP port for validating rules (since openHAB 2.2.0) If you want to change either of the first two: Edit the openhab fragment in docker-compose.yml : - OPENHAB_HTTP_PORT=4050 - OPENHAB_HTTPS_PORT=4051 Recreate the openHAB container: $ cd ~/IOTstack $ docker-compose up -d openhab There do not appear to be any environment variables to control ports 8101 or 5007 so, if other containers you need to run also depend on those ports, you will have to figure out some way of resolving the conflict. Note: The original IOTstack documentation included: openHAB has been added without Amazon Dashbutton binding. but it is not clear if this is still the case. Amazon Dashbuttons have been discontinued so this may no longer be relevant.","title":"openHAB"},{"location":"Containers/OpenHab/#openhab","text":"","title":"openHAB"},{"location":"Containers/OpenHab/#references","text":"DockerHub GitHub openHAB website openHAB runs in \"host mode\" so there are no port mappings. The default port bindings on IOTstack are: 4050 - the HTTP port of the web interface (instead of 8080) 4051 - the HTTPS port of the web interface (instead of 8443) 8101 - the SSH port of the Console (since openHAB 2.0.0) 5007 - the LSP port for validating rules (since openHAB 2.2.0) If you want to change either of the first two: Edit the openhab fragment in docker-compose.yml : - OPENHAB_HTTP_PORT=4050 - OPENHAB_HTTPS_PORT=4051 Recreate the openHAB container: $ cd ~/IOTstack $ docker-compose up -d openhab There do not appear to be any environment variables to control ports 8101 or 5007 so, if other containers you need to run also depend on those ports, you will have to figure out some way of resolving the conflict. Note: The original IOTstack documentation included: openHAB has been added without Amazon Dashbutton binding. but it is not clear if this is still the case. Amazon Dashbuttons have been discontinued so this may no longer be relevant.","title":"References"},{"location":"Containers/Pi-hole/","text":"Pi-hole \u00b6 Pi-hole is a fantastic utility to reduce ads. The interface can be found on http://\"your_ip\":8089/admin Default password is IOtSt4ckP1Hol3 . This can be changed with: docker exec pihole pihole -a -p myNewPassword Pi-hole as DNS server \u00b6 In order for the Pi-hole to work, it needs to be defined as the DNS server. This can either be done manually to each device or you can define it as a DNS-nameserver for the whole LAN. Assuming your pihole hostname is raspberrypi and has the IP 192.168.1.10 : Go to the Pi-hole web interface: http://raspberrypi.local:8089/admin and Login From Left menu: Select Local DNS -> DNS Records Enter Domain: raspberrypi.home.arpa and IP Address: 192.168.1.10 . Press Add. Go to your DHCP server, usually this is your Wireless Access Point / WLAN Router web interface. Find where DNS servers are defined Change all DNS fields to 192.168.1.10 . All local machines have to be rebooted or have their DHCP leases released. Without this they will continue to use the old DNS setting from an old DHCP lease for quite some time. Now you can use raspberrypi.home.arpa as the domain name for the Raspberry Pi in your whole local network. For the Raspberry Pi itself to also use the Pi-hole DNS server, run: echo \"name_servers=127.0.0.1\" | sudo tee -a /etc/resolvconf.conf echo \"name_servers_append=8.8.8.8\" | sudo tee -a /etc/resolvconf.conf echo \"resolv_conf_local_only=NO\" | sudo tee -a /etc/resolvconf.conf sudo resolvconf -u # Ignore \"Too few arguments.\"-complaint Quick explanation: resolv_conf_local_only is disabled and a public nameserver is added, so that in case the Pi-hole container is stopped, the Raspberry won't lose DNS functionality. It will just fallback to 8.8.8.8. Testing & Troubleshooting \u00b6 Install dig: apt install dnsutils Test that pi-hole is correctly configured (should respond 192.168.1.10): dig raspberrypi.home.arpa @192.168.1.10 To test on your desktop if your network configuration is correct, and an ESP will resolve its DNS queries correctly, restart your desktop machine to ensure DNS changes are updated and then use: dig raspberrypi.home.arpa This should produce the same result as the previous command. If this fails to resolve the IP, check that the server in the response is 192.168.1.10 . If it's 127.0.0.xx check /etc/resolv.conf begins with nameserver 192.168.1.10 . Microcontrollers \u00b6 If you want to avoid hardcoding your Raspberry Pi IP to your ESPhome devices, you need a DNS server that will do the resolving. This can be done using the Pi-hole container as described above. Why .home.arpa? \u00b6 Instead of .home.arpa - which is the real standard, but a mouthful - you may use .internal . Using .local would technically also work, but it should be reserved only for mDNS use. Note: There is a special case for resolving *.local addresses. If you do a ping raspberrypi.local on your desktop linux or the RPI, it will first try using mDNS/bonjour to resolve the IP address raspberrypi.local. If this fails it will then ask the DNS server. Esphome devices can't use mDNS to resolve an IP address. You need a proper DNS server to respond to queries made by an ESP. As such, dig raspberrypi.local will fail, simulating ESPhome device behavior. This is as intended, and you should use raspberrypi.home.arpa as the address on your ESP-device.","title":"Pi-hole"},{"location":"Containers/Pi-hole/#pi-hole","text":"Pi-hole is a fantastic utility to reduce ads. The interface can be found on http://\"your_ip\":8089/admin Default password is IOtSt4ckP1Hol3 . This can be changed with: docker exec pihole pihole -a -p myNewPassword","title":"Pi-hole"},{"location":"Containers/Pi-hole/#pi-hole-as-dns-server","text":"In order for the Pi-hole to work, it needs to be defined as the DNS server. This can either be done manually to each device or you can define it as a DNS-nameserver for the whole LAN. Assuming your pihole hostname is raspberrypi and has the IP 192.168.1.10 : Go to the Pi-hole web interface: http://raspberrypi.local:8089/admin and Login From Left menu: Select Local DNS -> DNS Records Enter Domain: raspberrypi.home.arpa and IP Address: 192.168.1.10 . Press Add. Go to your DHCP server, usually this is your Wireless Access Point / WLAN Router web interface. Find where DNS servers are defined Change all DNS fields to 192.168.1.10 . All local machines have to be rebooted or have their DHCP leases released. Without this they will continue to use the old DNS setting from an old DHCP lease for quite some time. Now you can use raspberrypi.home.arpa as the domain name for the Raspberry Pi in your whole local network. For the Raspberry Pi itself to also use the Pi-hole DNS server, run: echo \"name_servers=127.0.0.1\" | sudo tee -a /etc/resolvconf.conf echo \"name_servers_append=8.8.8.8\" | sudo tee -a /etc/resolvconf.conf echo \"resolv_conf_local_only=NO\" | sudo tee -a /etc/resolvconf.conf sudo resolvconf -u # Ignore \"Too few arguments.\"-complaint Quick explanation: resolv_conf_local_only is disabled and a public nameserver is added, so that in case the Pi-hole container is stopped, the Raspberry won't lose DNS functionality. It will just fallback to 8.8.8.8.","title":"Pi-hole as DNS server"},{"location":"Containers/Pi-hole/#testing-troubleshooting","text":"Install dig: apt install dnsutils Test that pi-hole is correctly configured (should respond 192.168.1.10): dig raspberrypi.home.arpa @192.168.1.10 To test on your desktop if your network configuration is correct, and an ESP will resolve its DNS queries correctly, restart your desktop machine to ensure DNS changes are updated and then use: dig raspberrypi.home.arpa This should produce the same result as the previous command. If this fails to resolve the IP, check that the server in the response is 192.168.1.10 . If it's 127.0.0.xx check /etc/resolv.conf begins with nameserver 192.168.1.10 .","title":"Testing &amp; Troubleshooting"},{"location":"Containers/Pi-hole/#microcontrollers","text":"If you want to avoid hardcoding your Raspberry Pi IP to your ESPhome devices, you need a DNS server that will do the resolving. This can be done using the Pi-hole container as described above.","title":"Microcontrollers"},{"location":"Containers/Pi-hole/#why-homearpa","text":"Instead of .home.arpa - which is the real standard, but a mouthful - you may use .internal . Using .local would technically also work, but it should be reserved only for mDNS use. Note: There is a special case for resolving *.local addresses. If you do a ping raspberrypi.local on your desktop linux or the RPI, it will first try using mDNS/bonjour to resolve the IP address raspberrypi.local. If this fails it will then ask the DNS server. Esphome devices can't use mDNS to resolve an IP address. You need a proper DNS server to respond to queries made by an ESP. As such, dig raspberrypi.local will fail, simulating ESPhome device behavior. This is as intended, and you should use raspberrypi.home.arpa as the address on your ESP-device.","title":"Why .home.arpa?"},{"location":"Containers/Plex/","text":"Plex \u00b6 References \u00b6 Homepage Docker Web interface \u00b6 The web UI can be found on \"your_ip\":32400/web Mounting an external drive by UUID to the home directory \u00b6 official mounting guide Create a directory in you home directory called mnt with a subdirectory HDD . Follow the instruction above to mount your external drive to /home/pi/mnt/HDD in you fstab edit your docker-compose.yml file under plex and uncomment the volumes for tv series and movies (modify the path to point to your media locations). Run docker-compose up -d to rebuild plex with the new volumes","title":"Plex"},{"location":"Containers/Plex/#plex","text":"","title":"Plex"},{"location":"Containers/Plex/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/Plex/#web-interface","text":"The web UI can be found on \"your_ip\":32400/web","title":"Web interface"},{"location":"Containers/Plex/#mounting-an-external-drive-by-uuid-to-the-home-directory","text":"official mounting guide Create a directory in you home directory called mnt with a subdirectory HDD . Follow the instruction above to mount your external drive to /home/pi/mnt/HDD in you fstab edit your docker-compose.yml file under plex and uncomment the volumes for tv series and movies (modify the path to point to your media locations). Run docker-compose up -d to rebuild plex with the new volumes","title":"Mounting an external drive by UUID to the home directory"},{"location":"Containers/Portainer-agent/","text":"Portainer agent \u00b6 References \u00b6 Docker Docs About \u00b6 The protainer agent is a great way to add a second docker instance to a existing portainer instance. this allows you to mananage multiple docker enviroments form one prortainer instance Adding to an existing instance \u00b6 When you want to add the the agent to an existing portianer instance. You go to the endpoints tab. Click on Add endpoint Select Agent Enter the name of the agent Enter the url of the endpoint ip-of-agent-instance:9001 Click on add endpoint","title":"Portainer agent"},{"location":"Containers/Portainer-agent/#portainer-agent","text":"","title":"Portainer agent"},{"location":"Containers/Portainer-agent/#references","text":"Docker Docs","title":"References"},{"location":"Containers/Portainer-agent/#about","text":"The protainer agent is a great way to add a second docker instance to a existing portainer instance. this allows you to mananage multiple docker enviroments form one prortainer instance","title":"About"},{"location":"Containers/Portainer-agent/#adding-to-an-existing-instance","text":"When you want to add the the agent to an existing portianer instance. You go to the endpoints tab. Click on Add endpoint Select Agent Enter the name of the agent Enter the url of the endpoint ip-of-agent-instance:9001 Click on add endpoint","title":"Adding to an existing instance"},{"location":"Containers/Portainer-ce/","text":"Portainer CE \u00b6 References \u00b6 Docker Website Definition \u00b6 \"#yourip\" means any of the following: the IP address of your Raspberry Pi (eg 192.168.1.10 ) the multicast domain name of your Raspberry Pi (eg iot-hub.local ) the domain name of your Raspberry Pi (eg iot-hub.mydomain.com ) About Portainer CE \u00b6 Portainer CE (Community Edition) is an application for managing Docker. It is a successor to Portainer . According to the Portainer CE documentation Portainer 1.24.x will continue as a separate code branch, released as portainer/portainer:latest, and will receive ongoing security updates until at least 1st Sept 2021. No new features will be added beyond what was available in 1.24.1. From that it should be clear that Portainer is deprecated and that Portainer CE is the way forward. Installing Portainer CE \u00b6 Run the menu: $ cd ~/IOTstack $ ./menu.sh Choose \"Build Stack\", select \"Portainer-ce\", press [TAB] then \"\\<Ok>\" and follow through to the end of the menu process, typically choosing \"Do not overwrite\" for any existing services. When the menu finishes: $ docker-compose up -d Ignore any message like this: WARNING: Found orphan containers (portainer) for this project \u2026 First run of Portainer CE \u00b6 In your web browser navigate to #yourip:9000/ : the first screen will suggest a username of \"admin\" and ask for a password. Supply those credentials and click \"Create User\". the second screen will ask you to select a connection method. For IOTstack, \"Docker (Manage the local Docker environment)\" is usually appropriate so click that and then click \"Connect\". From there, you can click on the \"Local\" group and take a look around. One of the things Portainer CE can help you do is find unused containers but beware of reading too much into this because, sometimes, an \"unused\" container is actually the base for another container (eg Node-RED). There are 'Quick actions' to view logs and other stats. This can all be done from terminal commands but Portainer CE makes it easier. Setting the Public IP address for your end-point \u00b6 If you click on a \"Published Port\" in the \"Containers\" list, your browser may return an error saying something like \"can't connect to server\" associated with an IP address of \"0.0.0.0\". To fix that problem, proceed as shown below: Click \"Endpoints\" in the left hand panel. Click the name \"local\" in the list of Endpoints. Click in the \"Public IP\" field. Enter one of the following: The multicast DNS (MDNS) name of your Raspberry Pi (eg iot-hub.local ) The fully-qualified domain name (FQDN) of your Raspberry Pi (eg iot-hub.mydomain.com ) The IP address of your Raspberry Pi (eg 192.168.1.10 ) Click \"Update endpoint\". To remove the Public IP address, repeat the above steps but clear the \"Public IP\" field in step 3. The reason why you have to tell Portainer CE which Public IP address to use is because an instance of Portainer CE does not necessarily have to be running on the same Raspberry Pi as the Docker containers it is managing. Keep in mind that clicking on a \"Published Port\" does not guarantee that your browser can open a connection. For example: Port 1883 for Mosquitto expects MQTT packets. It will not respond to HTTP, so any attempt will fail. Port 8089 for PiHole will respond to HTTP but PiHole may reject or mis-handle your attempt. Port 1880 for NodeRed will respond normally. All things considered, you will get more consistent behaviour if you simply bookmark the URLs you want to use for your IOTstack services. If you forget your password \u00b6 If you forget the password you created for Portainer CE , you can recover by doing the following: $ cd ~/IOTstack $ docker-compose stop portainer-ce $ sudo rm -r ./volumes/portainer-ce $ docker-compose start portainer-ce Then, follow the steps in: First run of Portainer CE ; and Setting the Public IP address for your end-point .","title":"Portainer CE"},{"location":"Containers/Portainer-ce/#portainer-ce","text":"","title":"Portainer CE"},{"location":"Containers/Portainer-ce/#references","text":"Docker Website","title":"References"},{"location":"Containers/Portainer-ce/#definition","text":"\"#yourip\" means any of the following: the IP address of your Raspberry Pi (eg 192.168.1.10 ) the multicast domain name of your Raspberry Pi (eg iot-hub.local ) the domain name of your Raspberry Pi (eg iot-hub.mydomain.com )","title":"Definition"},{"location":"Containers/Portainer-ce/#about-portainer-ce","text":"Portainer CE (Community Edition) is an application for managing Docker. It is a successor to Portainer . According to the Portainer CE documentation Portainer 1.24.x will continue as a separate code branch, released as portainer/portainer:latest, and will receive ongoing security updates until at least 1st Sept 2021. No new features will be added beyond what was available in 1.24.1. From that it should be clear that Portainer is deprecated and that Portainer CE is the way forward.","title":"About Portainer CE"},{"location":"Containers/Portainer-ce/#installing-portainer-ce","text":"Run the menu: $ cd ~/IOTstack $ ./menu.sh Choose \"Build Stack\", select \"Portainer-ce\", press [TAB] then \"\\<Ok>\" and follow through to the end of the menu process, typically choosing \"Do not overwrite\" for any existing services. When the menu finishes: $ docker-compose up -d Ignore any message like this: WARNING: Found orphan containers (portainer) for this project \u2026","title":"Installing Portainer CE"},{"location":"Containers/Portainer-ce/#first-run-of-portainer-ce","text":"In your web browser navigate to #yourip:9000/ : the first screen will suggest a username of \"admin\" and ask for a password. Supply those credentials and click \"Create User\". the second screen will ask you to select a connection method. For IOTstack, \"Docker (Manage the local Docker environment)\" is usually appropriate so click that and then click \"Connect\". From there, you can click on the \"Local\" group and take a look around. One of the things Portainer CE can help you do is find unused containers but beware of reading too much into this because, sometimes, an \"unused\" container is actually the base for another container (eg Node-RED). There are 'Quick actions' to view logs and other stats. This can all be done from terminal commands but Portainer CE makes it easier.","title":"First run of Portainer CE"},{"location":"Containers/Portainer-ce/#setting-the-public-ip-address-for-your-end-point","text":"If you click on a \"Published Port\" in the \"Containers\" list, your browser may return an error saying something like \"can't connect to server\" associated with an IP address of \"0.0.0.0\". To fix that problem, proceed as shown below: Click \"Endpoints\" in the left hand panel. Click the name \"local\" in the list of Endpoints. Click in the \"Public IP\" field. Enter one of the following: The multicast DNS (MDNS) name of your Raspberry Pi (eg iot-hub.local ) The fully-qualified domain name (FQDN) of your Raspberry Pi (eg iot-hub.mydomain.com ) The IP address of your Raspberry Pi (eg 192.168.1.10 ) Click \"Update endpoint\". To remove the Public IP address, repeat the above steps but clear the \"Public IP\" field in step 3. The reason why you have to tell Portainer CE which Public IP address to use is because an instance of Portainer CE does not necessarily have to be running on the same Raspberry Pi as the Docker containers it is managing. Keep in mind that clicking on a \"Published Port\" does not guarantee that your browser can open a connection. For example: Port 1883 for Mosquitto expects MQTT packets. It will not respond to HTTP, so any attempt will fail. Port 8089 for PiHole will respond to HTTP but PiHole may reject or mis-handle your attempt. Port 1880 for NodeRed will respond normally. All things considered, you will get more consistent behaviour if you simply bookmark the URLs you want to use for your IOTstack services.","title":"Setting the Public IP address for your end-point"},{"location":"Containers/Portainer-ce/#if-you-forget-your-password","text":"If you forget the password you created for Portainer CE , you can recover by doing the following: $ cd ~/IOTstack $ docker-compose stop portainer-ce $ sudo rm -r ./volumes/portainer-ce $ docker-compose start portainer-ce Then, follow the steps in: First run of Portainer CE ; and Setting the Public IP address for your end-point .","title":"If you forget your password"},{"location":"Containers/PostgreSQL/","text":"PostgreSQL \u00b6 References \u00b6 Docker Website About \u00b6 PostgreSQL is an SQL server, for those that need an SQL database. The database credentials can be found in the file ./volumes/postgres/postgres.env . It is highly recommended to change the user, password and default database","title":"PostgreSQL"},{"location":"Containers/PostgreSQL/#postgresql","text":"","title":"PostgreSQL"},{"location":"Containers/PostgreSQL/#references","text":"Docker Website","title":"References"},{"location":"Containers/PostgreSQL/#about","text":"PostgreSQL is an SQL server, for those that need an SQL database. The database credentials can be found in the file ./volumes/postgres/postgres.env . It is highly recommended to change the user, password and default database","title":"About"},{"location":"Containers/Prometheus/","text":"Prometheus \u00b6 References \u00b6 Prometheus home GitHub : Prometheus CAdvisor Node Exporter DockerHub : Prometheus CAdvisor Node Exporter Overview \u00b6 Three containers are installed when you select Prometheus in the IOTstack menu: Prometheus CAdvisor Node Exporter The default configuration for Prometheus supplied with IOTstack scrapes information from all three containers. Significant directories and files \u00b6 ~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 prometheus \u2502 \u251c\u2500\u2500 service.yml \u2776 \u2502 \u251c\u2500\u2500 Dockerfile \u2777 \u2502 \u251c\u2500\u2500 docker-entrypoint.sh \u2778 \u2502 \u2514\u2500\u2500 iotstack_defaults \u2779 \u2502 \u2514\u2500\u2500 config.yml \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 prometheus \u2502 \u2514\u2500\u2500 service.yml \u277a \u251c\u2500\u2500 docker-compose.yml \u277b \u2514\u2500\u2500 volumes \u2514\u2500\u2500 prometheus \u277c \u2514\u2500\u2500 data \u251c\u2500\u2500 config \u277d \u2502 \u251c\u2500\u2500 config.yml \u2502 \u2514\u2500\u2500 prometheus.yml \u2514\u2500\u2500 data The template service definition . The Dockerfile used to customise Prometheus for IOTstack. A pre-launch script to handle container self-repair before launching the Prometheus service. Defaults for IOTstack, used to initialise on first run, and for container self-repair. The working service definition (only relevant to old-menu, copied from \u2776). The Compose file (includes \u2776). The persistent storage area . The configuration directory . How Prometheus gets built for IOTstack \u00b6 Prometheus source code ( GitHub ) \u00b6 The source code for Prometheus lives at GitHub prometheus/prometheus . Prometheus images ( DockerHub ) \u00b6 Periodically, the source code is recompiled and the resulting image is pushed to prom/prometheus on DockerHub . IOTstack menu \u00b6 When you select Prometheus in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used. IOTstack first run \u00b6 On a first install of IOTstack, you run the menu, choose Prometheus as one of your containers, and are told to do this: $ cd ~/IOTstack $ docker-compose up -d docker-compose reads the Compose file. When it arrives at the prometheus fragment, it finds: prometheus : container_name : prometheus build : ./.templates/prometheus/. The build statement tells docker-compose to look for: ~/IOTstack/.templates/prometheus/Dockerfile The Dockerfile is in the .templates directory because it is intended to be a common build for all IOTstack users. This is different to the arrangement for Node-RED where the Dockerfile is in the services directory because it is how each individual IOTstack user's version of Node-RED is customised. The Dockerfile begins with: FROM prom/prometheus:latest If you need to pin to a particular version of Prometheus , the Dockerfile is the place to do it. See Prometheus version pinning . The FROM statement tells the build process to pull down the base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The customisations are: Add configuration defaults appropriate for IOTstack. Add docker-entrypoint.sh which: Ensures the internal directory /prometheus/config/ exists; Copies any configuration files that have gone missing into that directory. Enforces \"pi:pi\" ownership in ~/IOTstack/volumes/prometheus/data/config . Launches the Prometheus service. The local image is instantiated to become your running container. When you run the docker images command after Prometheus has been built, you will see two rows for Prometheus : $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_prometheus latest 1815f63da5f0 23 minutes ago 169MB prom/prometheus latest 3f9575991a6c 3 days ago 169MB prom/prometheus is the base image ; and iotstack_prometheus is the local image . You will see the same pattern in Portainer, which reports the base image as \"unused\". You should not remove the base image, even though it appears to be unused. Dependencies: CAdvisor and Node Exporter \u00b6 The CAdvisor and Node Exporter are included in the Prometheus service definition as dependent containers. What that means is that each time you start Prometheus , docker-compose ensures that CAdvisor and Node Exporter are already running, and keeps them running. The default configuration for Prometheus assumes CAdvisor and Node Exporter are running and starts scraping information from those targets as soon as it launches. Configuring Prometheus \u00b6 Configuration directory \u00b6 The configuration directory for the IOTstack implementation of Prometheus is at the path: ~/IOTstack/volumes/prometheus/data/config That directory contains two files: config.yml ; and prometheus.yml . If you delete either file, Prometheus will replace it with a default the next time the container starts. This \"self-repair\" function is intended to provide reasonable assurance that Prometheus will at least start instead of going into a restart loop. Unless you decide to change it , the config folder and its contents are owned by \"pi:pi\". This means you can edit the files in the configuration directory without needing the sudo command. Ownership is enforced each time the container restarts. Active configuration file \u00b6 The file named config.yml is the active configuration. This is the file you should edit if you want to make changes. The default structure of the file is: global : scrape_interval : 10s evaluation_interval : 10s scrape_configs : - job_name : \"iotstack\" static_configs : - targets : - localhost:9090 - cadvisor:8080 - nodeexporter:9100 To cause a running instance of Prometheus to notice a change to this file: $ cd ~/IOTstack $ docker-compose restart prometheus $ docker logs prometheus Note: The YAML parser used by Prometheus seems to be exceptionally sensitive to syntax errors (far less tolerant than docker-compose ). For this reason, you should always check the Prometheus log after any configuration change. Reference configuration file \u00b6 The file named prometheus.yml is a reference configuration. It is a copy of the original configuration file that ships inside the Prometheus container at the path: /etc/prometheus/prometheus.yml Editing prometheus.yml has no effect. It is provided as a convenience to help you follow examples on the web. If you want to make the contents of prometheus.yml the active configuration, you need to do this: $ cd ~/IOTstack/volumes/prometheus/data/config $ cp prometheus.yml config.yml $ cd ~/IOTstack $ docker-compose restart prometheus $ docker logs prometheus Environment variables \u00b6 The IOTstack implementation of Prometheus supports two environment variables: environment : - IOTSTACK_UID=1000 - IOTSTACK_GID=1000 Those variables control ownership of the Configuration directory and its contents. Those environment variables are present in the standard IOTstack service definition for Prometheus and have the effect of assigning ownership to \"pi:pi\". If you delete those environment variables from your Compose file, the Configuration directory will be owned by \"nobody:nobody\"; otherwise the directory and its contents will be owned by whatever values you pass for those variables. Migration considerations \u00b6 Under the original IOTstack implementation of Prometheus (just \"as it comes\" from DockerHub ), the service definition expected the configuration file to be at: ~/IOTstack/services/prometheus/config.yml Under this implementation of Prometheus , the configuration file has moved to: ~/IOTstack/volumes/prometheus/data/config/config.yml The change of location is one of the things that allows self-repair to work properly. Some of the assumptions behind the default configuration file have changed. In particular, instead of the entire scrape_configs block being commented-out, it is active and defines localhost , cadvisor and nodeexporter as targets. You should compare the old and new versions and decide which settings need to be migrated into the new configuration file. If you change the configuration file, restart Prometheus and then check the log for errors: $ docker-compose restart prometheus $ docker logs prometheus Note: The YAML parser used by Prometheus is very sensitive to syntax errors. Always check the Prometheus log after any configuration change. Upgrading Prometheus \u00b6 You can update cadvisor and nodeexporter like this: $ cd ~/IOTstack $ docker-compose pull cadvisor nodeexporter $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. This \"simple pull\" strategy doesn't work when a Dockerfile is used to build a local image on top of a base image downloaded from DockerHub . The local image is what is running so there is no way for the pull to sense when a newer version becomes available. The only way to know when an update to Prometheus is available is to check the prom/prometheus tags page on DockerHub . Once a new version appears on DockerHub , you can upgrade Prometheus like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull prometheus $ docker-compose up -d prometheus $ docker system prune $ docker system prune Breaking it down into parts: build causes the named container to be rebuilt; --no-cache tells the Dockerfile process that it must not take any shortcuts. It really must rebuild the local image ; --pull tells the Dockerfile process to actually check with DockerHub to see if there is a later version of the base image and, if so, to download it before starting the build; prometheus is the named container argument required by the build command. Your existing Prometheus container continues to run while the rebuild proceeds. Once the freshly-built local image is ready, the up tells docker-compose to do a new-for-old swap. There is barely any downtime for your service. The prune is the simplest way of cleaning up. The first call removes the old local image . The second call cleans up the old base image . Prometheus version pinning \u00b6 If you need to pin Prometheus to a particular version: Use your favourite text editor to open the following file: ~/IOTstack/.templates/prometheus/Dockerfile Find the line: FROM prom/prometheus:latest Replace latest with the version you wish to pin to. For example, to pin to version 2.30.2: FROM prom/prometheus:2.30.2 Save the file and tell docker-compose to rebuild the local image: $ cd ~/IOTstack $ docker-compose up -d --build prometheus $ docker system prune The new local image is built, then the new container is instantiated based on that image. The prune deletes the old local image . Note: As well as preventing Docker from updating the base image , pinning will also block incoming updates to the Dockerfile from a git pull . Nothing will change until you decide to remove the pin.","title":"Prometheus"},{"location":"Containers/Prometheus/#prometheus","text":"","title":"Prometheus"},{"location":"Containers/Prometheus/#references","text":"Prometheus home GitHub : Prometheus CAdvisor Node Exporter DockerHub : Prometheus CAdvisor Node Exporter","title":"References"},{"location":"Containers/Prometheus/#overview","text":"Three containers are installed when you select Prometheus in the IOTstack menu: Prometheus CAdvisor Node Exporter The default configuration for Prometheus supplied with IOTstack scrapes information from all three containers.","title":"Overview"},{"location":"Containers/Prometheus/#significant-directories-and-files","text":"~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 prometheus \u2502 \u251c\u2500\u2500 service.yml \u2776 \u2502 \u251c\u2500\u2500 Dockerfile \u2777 \u2502 \u251c\u2500\u2500 docker-entrypoint.sh \u2778 \u2502 \u2514\u2500\u2500 iotstack_defaults \u2779 \u2502 \u2514\u2500\u2500 config.yml \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 prometheus \u2502 \u2514\u2500\u2500 service.yml \u277a \u251c\u2500\u2500 docker-compose.yml \u277b \u2514\u2500\u2500 volumes \u2514\u2500\u2500 prometheus \u277c \u2514\u2500\u2500 data \u251c\u2500\u2500 config \u277d \u2502 \u251c\u2500\u2500 config.yml \u2502 \u2514\u2500\u2500 prometheus.yml \u2514\u2500\u2500 data The template service definition . The Dockerfile used to customise Prometheus for IOTstack. A pre-launch script to handle container self-repair before launching the Prometheus service. Defaults for IOTstack, used to initialise on first run, and for container self-repair. The working service definition (only relevant to old-menu, copied from \u2776). The Compose file (includes \u2776). The persistent storage area . The configuration directory .","title":"Significant directories and files"},{"location":"Containers/Prometheus/#how-prometheus-gets-built-for-iotstack","text":"","title":"How Prometheus gets built for IOTstack"},{"location":"Containers/Prometheus/#prometheus-source-code-github","text":"The source code for Prometheus lives at GitHub prometheus/prometheus .","title":"Prometheus source code (GitHub)"},{"location":"Containers/Prometheus/#prometheus-images-dockerhub","text":"Periodically, the source code is recompiled and the resulting image is pushed to prom/prometheus on DockerHub .","title":"Prometheus images (DockerHub)"},{"location":"Containers/Prometheus/#iotstack-menu","text":"When you select Prometheus in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used.","title":"IOTstack menu"},{"location":"Containers/Prometheus/#iotstack-first-run","text":"On a first install of IOTstack, you run the menu, choose Prometheus as one of your containers, and are told to do this: $ cd ~/IOTstack $ docker-compose up -d docker-compose reads the Compose file. When it arrives at the prometheus fragment, it finds: prometheus : container_name : prometheus build : ./.templates/prometheus/. The build statement tells docker-compose to look for: ~/IOTstack/.templates/prometheus/Dockerfile The Dockerfile is in the .templates directory because it is intended to be a common build for all IOTstack users. This is different to the arrangement for Node-RED where the Dockerfile is in the services directory because it is how each individual IOTstack user's version of Node-RED is customised. The Dockerfile begins with: FROM prom/prometheus:latest If you need to pin to a particular version of Prometheus , the Dockerfile is the place to do it. See Prometheus version pinning . The FROM statement tells the build process to pull down the base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The customisations are: Add configuration defaults appropriate for IOTstack. Add docker-entrypoint.sh which: Ensures the internal directory /prometheus/config/ exists; Copies any configuration files that have gone missing into that directory. Enforces \"pi:pi\" ownership in ~/IOTstack/volumes/prometheus/data/config . Launches the Prometheus service. The local image is instantiated to become your running container. When you run the docker images command after Prometheus has been built, you will see two rows for Prometheus : $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_prometheus latest 1815f63da5f0 23 minutes ago 169MB prom/prometheus latest 3f9575991a6c 3 days ago 169MB prom/prometheus is the base image ; and iotstack_prometheus is the local image . You will see the same pattern in Portainer, which reports the base image as \"unused\". You should not remove the base image, even though it appears to be unused.","title":"IOTstack first run"},{"location":"Containers/Prometheus/#dependencies-cadvisor-and-node-exporter","text":"The CAdvisor and Node Exporter are included in the Prometheus service definition as dependent containers. What that means is that each time you start Prometheus , docker-compose ensures that CAdvisor and Node Exporter are already running, and keeps them running. The default configuration for Prometheus assumes CAdvisor and Node Exporter are running and starts scraping information from those targets as soon as it launches.","title":"Dependencies: CAdvisor and Node Exporter"},{"location":"Containers/Prometheus/#configuring-prometheus","text":"","title":"Configuring Prometheus"},{"location":"Containers/Prometheus/#configuration-directory","text":"The configuration directory for the IOTstack implementation of Prometheus is at the path: ~/IOTstack/volumes/prometheus/data/config That directory contains two files: config.yml ; and prometheus.yml . If you delete either file, Prometheus will replace it with a default the next time the container starts. This \"self-repair\" function is intended to provide reasonable assurance that Prometheus will at least start instead of going into a restart loop. Unless you decide to change it , the config folder and its contents are owned by \"pi:pi\". This means you can edit the files in the configuration directory without needing the sudo command. Ownership is enforced each time the container restarts.","title":"Configuration directory"},{"location":"Containers/Prometheus/#active-configuration-file","text":"The file named config.yml is the active configuration. This is the file you should edit if you want to make changes. The default structure of the file is: global : scrape_interval : 10s evaluation_interval : 10s scrape_configs : - job_name : \"iotstack\" static_configs : - targets : - localhost:9090 - cadvisor:8080 - nodeexporter:9100 To cause a running instance of Prometheus to notice a change to this file: $ cd ~/IOTstack $ docker-compose restart prometheus $ docker logs prometheus Note: The YAML parser used by Prometheus seems to be exceptionally sensitive to syntax errors (far less tolerant than docker-compose ). For this reason, you should always check the Prometheus log after any configuration change.","title":"Active configuration file"},{"location":"Containers/Prometheus/#reference-configuration-file","text":"The file named prometheus.yml is a reference configuration. It is a copy of the original configuration file that ships inside the Prometheus container at the path: /etc/prometheus/prometheus.yml Editing prometheus.yml has no effect. It is provided as a convenience to help you follow examples on the web. If you want to make the contents of prometheus.yml the active configuration, you need to do this: $ cd ~/IOTstack/volumes/prometheus/data/config $ cp prometheus.yml config.yml $ cd ~/IOTstack $ docker-compose restart prometheus $ docker logs prometheus","title":"Reference configuration file"},{"location":"Containers/Prometheus/#environment-variables","text":"The IOTstack implementation of Prometheus supports two environment variables: environment : - IOTSTACK_UID=1000 - IOTSTACK_GID=1000 Those variables control ownership of the Configuration directory and its contents. Those environment variables are present in the standard IOTstack service definition for Prometheus and have the effect of assigning ownership to \"pi:pi\". If you delete those environment variables from your Compose file, the Configuration directory will be owned by \"nobody:nobody\"; otherwise the directory and its contents will be owned by whatever values you pass for those variables.","title":"Environment variables"},{"location":"Containers/Prometheus/#migration-considerations","text":"Under the original IOTstack implementation of Prometheus (just \"as it comes\" from DockerHub ), the service definition expected the configuration file to be at: ~/IOTstack/services/prometheus/config.yml Under this implementation of Prometheus , the configuration file has moved to: ~/IOTstack/volumes/prometheus/data/config/config.yml The change of location is one of the things that allows self-repair to work properly. Some of the assumptions behind the default configuration file have changed. In particular, instead of the entire scrape_configs block being commented-out, it is active and defines localhost , cadvisor and nodeexporter as targets. You should compare the old and new versions and decide which settings need to be migrated into the new configuration file. If you change the configuration file, restart Prometheus and then check the log for errors: $ docker-compose restart prometheus $ docker logs prometheus Note: The YAML parser used by Prometheus is very sensitive to syntax errors. Always check the Prometheus log after any configuration change.","title":"Migration considerations"},{"location":"Containers/Prometheus/#upgrading-prometheus","text":"You can update cadvisor and nodeexporter like this: $ cd ~/IOTstack $ docker-compose pull cadvisor nodeexporter $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. This \"simple pull\" strategy doesn't work when a Dockerfile is used to build a local image on top of a base image downloaded from DockerHub . The local image is what is running so there is no way for the pull to sense when a newer version becomes available. The only way to know when an update to Prometheus is available is to check the prom/prometheus tags page on DockerHub . Once a new version appears on DockerHub , you can upgrade Prometheus like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull prometheus $ docker-compose up -d prometheus $ docker system prune $ docker system prune Breaking it down into parts: build causes the named container to be rebuilt; --no-cache tells the Dockerfile process that it must not take any shortcuts. It really must rebuild the local image ; --pull tells the Dockerfile process to actually check with DockerHub to see if there is a later version of the base image and, if so, to download it before starting the build; prometheus is the named container argument required by the build command. Your existing Prometheus container continues to run while the rebuild proceeds. Once the freshly-built local image is ready, the up tells docker-compose to do a new-for-old swap. There is barely any downtime for your service. The prune is the simplest way of cleaning up. The first call removes the old local image . The second call cleans up the old base image .","title":"Upgrading Prometheus"},{"location":"Containers/Prometheus/#prometheus-version-pinning","text":"If you need to pin Prometheus to a particular version: Use your favourite text editor to open the following file: ~/IOTstack/.templates/prometheus/Dockerfile Find the line: FROM prom/prometheus:latest Replace latest with the version you wish to pin to. For example, to pin to version 2.30.2: FROM prom/prometheus:2.30.2 Save the file and tell docker-compose to rebuild the local image: $ cd ~/IOTstack $ docker-compose up -d --build prometheus $ docker system prune The new local image is built, then the new container is instantiated based on that image. The prune deletes the old local image . Note: As well as preventing Docker from updating the base image , pinning will also block incoming updates to the Dockerfile from a git pull . Nothing will change until you decide to remove the pin.","title":"Prometheus version pinning"},{"location":"Containers/Python/","text":"Python \u00b6 references \u00b6 Python.org Dockerhub image library GitHub docker-library/python selecting Python in the IOTstack menu \u00b6 When you select Python in the menu: The following folder and file structure is created: $ tree ~/IOTstack/services/python /home/pi/IOTstack/services/python \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 app.py \u251c\u2500\u2500 docker-entrypoint.sh \u2514\u2500\u2500 Dockerfile Note: Under \"old menu\" (old-menu branch), the service.yml is also copied into the python directory but is then not used. This service definition is added to your docker-compose.yml : python : container_name : python build : ./services/python/. restart : unless-stopped environment : - TZ=Etc/UTC - IOTSTACK_UID=1000 - IOTSTACK_GID=1000 # ports: # - \"external:internal\" volumes : - ./volumes/python/app:/usr/src/app customising your Python service definition \u00b6 The service definition contains a number of customisation points: restart: unless-stopped assumes your Python script will run in an infinite loop. If your script is intended to run once and terminate, you should remove this directive. TZ=Etc/UTC should be set to your local time-zone. Never use quote marks on the right hand side of a TZ= variable. If you are running as a different user ID, you may want to change both IOTSTACK_UID and IOTSTACK_GID to appropriate values. Notes: Don't use user and group names because these variables are applied inside the container where those names are (probably) undefined. The only thing these variables affect is the ownership of: ~/IOTstack/volumes/python/app and its contents. If you want everything to be owned by root, set both of these variables to zero (eg IOTSTACK_UID=0 ). If your Python script listens to data-communications traffic, you can set up the port mappings by uncommenting the ports: directive. If your Python container is already running when you make a change to its service definition, you can apply it via: $ cd ~/IOTstack $ docker-compose up -d python Python - first launch \u00b6 After running the menu, you are told to run the commands: $ cd ~/IOTstack $ docker-compose up -d This is what happens: docker-compose reads your docker-compose.yml . When it finds the service definition for Python, it encounters: build: ./services/python/. The leading period means \"the directory containing docker-compose.yml while the trailing period means \"Dockerfile\", so the path expands to: ~/IOTstack/services/python/Dockerfile The Dockerfile is processed. It downloads the base image for Python from Dockerhub and then makes changes including: copying the contents of the following directory into the image as a set of defaults: /home/pi/IOTstack/services/python/app copying the following file into the image: /home/pi/IOTstack/services/python/docker-entrypoint.sh The docker-entrypoint.sh script runs each time the container launches and performs initialisation and \"self repair\" functions. The output of the Dockerfile run is a new local image tagged with the name iotstack_python . The iotstack_python image is instantiated to become the running container. When the container starts, the docker-entrypoint.sh script runs and initialises the container's persistent storage area: $ tree -pu ~/IOTstack/volumes /home/pi/IOTstack/volumes \u2514\u2500\u2500 [ drwxr-xr-x root ] python \u2514\u2500\u2500 [ drwxr-xr-x pi ] app \u2514\u2500\u2500 [ -rwxr-xr-x pi ] app.py Note: the top-level python folder is owned by \"root\" but the app directory and its contents are owned by \"pi\". The initial app.py Python script is a \"hello world\" placeholder. It runs as an infinite loop emitting messages every 10 seconds until terminated. You can see what it is doing by running: $ docker logs -f python The world is born. Hello World. The world is re-born. Hello World. The world is re-born. Hello World. \u2026 Pressing control + c terminates the log display but does not terminate the running container. stopping the Python service \u00b6 To stop the container from running, either: take down your whole stack: $ cd ~/IOTstack $ docker-compose down terminate the python container $ cd ~/IOTstack $ docker-compose rm --force --stop -v python starting the Python service \u00b6 To bring up the container again after you have stopped it, either: bring up your whole stack: $ cd ~/IOTstack $ docker-compose up -d bring up the python container $ cd ~/IOTstack $ docker-compose up -d python Python - second-and-subsequent launch \u00b6 Each time you launch the Python container after the first launch: The existing local image ( iotstack_python ) is instantiated to become the running container. The docker-entrypoint.sh script runs and performs \"self-repair\" by replacing any files that have gone missing from the persistent storage area. Self-repair does not overwrite existing files! The app.py Python script is run. when things go wrong - check the log \u00b6 If the container misbehaves, the log is your friend: $ docker logs python project development life-cycle \u00b6 It is critical that you understand that all of your project development should occur within the folder: ~/IOTstack/volumes/python/app So long as you are performing some sort of routine backup (either with a supplied script or a third party solution like Paraphraser/IOTstackBackup ), your work will be protected. getting started \u00b6 Start by editing the file: ~/IOTstack/volumes/python/app/app.py If you need other supporting scripts or data files, also add those to the directory: ~/IOTstack/volumes/python/app Any time you change something in the app folder, tell the running python container to notice the change by: $ cd ~/IOTstack $ docker-compose restart python reading and writing to disk \u00b6 Consider this line in the service definition: - ./volumes/python/app:/usr/src/app The leading period means \"the directory containing docker-compose.yml \" so it the same as: - ~/IOTstack/volumes/python/app:/usr/src/app Then, you split the line at the \":\", resulting in: The external directory = ~/IOTstack/volumes/python/app The internal directory = /usr/src/app What it means is that: Any file you put into the external directory (or any sub-directories you create within the external directory) will be visible to your Python script running inside the container at the same relative position in the internal directory. Any file or sub-directory created in the internal directory by your Python script running inside the container will be visible outside the container at the same relative position in the external directory. The contents of external directory and, therefore, the internal directory will persist across container launches. If your script writes into any other directory inside the container, the data will be lost when the container re-launches. getting a clean slate \u00b6 If you make a mess of things and need to start from a clean slate, erase the persistent storage area: $ cd ~/IOTstack $ docker-compose rm --force --stop -v python $ sudo rm -rf ./volumes/python $ docker-compose up -d python The container will re-initialise the persistent storage area from its defaults. adding packages \u00b6 As you develop your project, you may find that you need to add supporting packages. For this example, we will assume you want to add \" Flask \" and \" beautifulsoup4 \". If you were developing a project outside of container-space, you would simply run: $ pip3 install -U Flask beautifulsoup4 You can do the same thing with the running container: $ docker exec python pip3 install -U Flask beautifulsoup4 and that will work \u2014 until the container is re-launched, at which point the added packages will disappear. To make Flask and beautifulsoup4 a permanent part of your container: Change your working directory: $ cd ~/IOTstack/services/python/app Use your favourite text editor to create the file requirements.txt in that directory. Each package you want to add should be on a line by itself: Flask beautifulsoup4 Tell Docker to rebuild the local Python image: $ cd ~/IOTstack $ docker-compose build --force-rm python $ docker-compose up -d --force-recreate python $ docker system prune -f Note: You will see a warning about running pip as root - ignore it. Confirm that the packages have been added: $ docker exec python pip3 freeze | grep -e \"Flask\" -e \"beautifulsoup4\" beautifulsoup4==4.10.0 Flask==2.0.1 Continue your development work by returning to getting started . Note: The first time you following the process described above to create requirements.txt , a copy will appear at: ~/IOTstack/volumes/python/app/requirements.txt This copy is the result of the \"self-repair\" code that runs each time the container starts noticing that requirements.txt is missing and making a copy from the defaults stored inside the image. If you make more changes to the master version of requirements.txt in the services directory and rebuild the local image, the copy in the volumes directory will not be kept in-sync. That's because the \"self-repair\" code never overwrites existing files. If you want to bring the copy of requirements.txt in the volumes directory up-to-date: $ cd ~/IOTstack $ rm ./volumes/python/app/requirements.txt $ docker-compose restart python The requirements.txt file will be recreated and it will be a copy of the version in the services directory as of the last image rebuild. making your own Python script the default \u00b6 Suppose the Python script you have been developing reaches a major milestone and you decide to \"freeze dry\" your work up to that point so that it becomes the default when you ask for a clean slate . Proceed like this: If you have added any packages by following the steps in adding packages , run the following command: $ docker exec python bash -c 'pip3 freeze >requirements.txt' That generates a requirements.txt representing the state of play inside the running container. Because it is running inside the container, the requirements.txt created by that command appears outside the container at: ~/IOTstack/volumes/python/app/requirements.txt Make your work the default: $ cd ~/IOTstack $ cp -r ./volumes/python/app/* ./services/python/app The cp command copies: your Python script; the optional requirements.txt (from step 1); and any other files you may have put into the Python working directory. Key point: everything copied into ./services/python/app will become part of the new local image. Terminate the Python container and erase its persistent storage area: $ cd ~/IOTstack $ docker-compose rm --force --stop -v python $ sudo rm -rf ./volumes/python Note: If erasing the persistent storage area feels too risky, just move it out of the way: $ cd ~/IOTstack/volumes $ sudo mv python python.off Rebuild the local image: $ cd ~/IOTstack $ docker-compose build --force-rm python $ docker-compose up -d --force-recreate python On its first launch, the new container will re-populate the persistent storage area but, this time, it will be your Python script and any other supporting files, rather than the original \"hello world\" script. Clean up by removing the old local image: $ docker system prune -f canning your project \u00b6 Suppose your project has reached the stage where you wish to put it into production as a service under its own name. Make two further assumptions: You have gone through the steps in making your own Python script the default and you are certain that the content of ./services/python/app correctly captures your project. You want to give your project the name \"wishbone\". Proceed like this: Stop the development project: $ cd ~/IOTstack $ docker-compose rm --force --stop -v python Remove the existing local image: $ docker rmi iotstack_python Rename the python services directory to the name of your project: $ cd ~/IOTstack/services $ mv python wishbone Edit the python service definition in docker-compose.yml and replace references to python with the name of your project. In the following, the original is on the left, the edited version on the right, and the lines that need to change are indicated with a \"|\": python: | wishbone: container_name: python | container_name: wishbone build: ./services/python/. | build: ./services/wishbone/. restart: unless-stopped restart: unless-stopped environment: environment: - TZ=Etc/UTC - TZ=Etc/UTC - IOTSTACK_UID=1000 - IOTSTACK_UID=1000 - IOTSTACK_GID=1000 - IOTSTACK_GID=1000 # ports: # ports: # - \"external:internal\" # - \"external:internal\" volumes: volumes: - ./volumes/python/app:/usr/src/app | - ./volumes/wishbone/app:/usr/src/app Note: if you make a copy of the python service definition and then perform the required \"wishbone\" edits on the copy, the python definition will still be active so docker-compose may try to bring up both services. You will eliminate the risk of confusing yourself if you follow these instructions \"as written\" by not leaving the python service definition in place. Start the renamed service: $ cd ~/IOTstack $ docker-compose up -d wishbone Remember: After you have done this, the persistent storage area will be at the path: ~/IOTstack/volumes/wishbone/app routine maintenance \u00b6 To make sure you are running from the most-recent base image of Python from Dockerhub: $ cd ~/IOTstack $ docker-compose build --no-cache --pull python $ docker-compose up -d python $ docker system prune -f $ docker system prune -f In words: Be in the right directory. Force docker-compose to download the most-recent version of the Python base image from Dockerhub, and then run the Dockerfile to build a new local image. Instantiate the newly-built local image. Remove the old local image. Remove the old base image The old base image can't be removed until the old local image has been removed, which is why the prune command needs to be run twice. Note: If you have followed the steps in canning your project and your service has a name other than python , just substitute the new name where you see python in the two dockerc-compose commands.","title":"Python"},{"location":"Containers/Python/#python","text":"","title":"Python"},{"location":"Containers/Python/#references","text":"Python.org Dockerhub image library GitHub docker-library/python","title":"references"},{"location":"Containers/Python/#selecting-python-in-the-iotstack-menu","text":"When you select Python in the menu: The following folder and file structure is created: $ tree ~/IOTstack/services/python /home/pi/IOTstack/services/python \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 app.py \u251c\u2500\u2500 docker-entrypoint.sh \u2514\u2500\u2500 Dockerfile Note: Under \"old menu\" (old-menu branch), the service.yml is also copied into the python directory but is then not used. This service definition is added to your docker-compose.yml : python : container_name : python build : ./services/python/. restart : unless-stopped environment : - TZ=Etc/UTC - IOTSTACK_UID=1000 - IOTSTACK_GID=1000 # ports: # - \"external:internal\" volumes : - ./volumes/python/app:/usr/src/app","title":"selecting Python in the IOTstack menu"},{"location":"Containers/Python/#customising-your-python-service-definition","text":"The service definition contains a number of customisation points: restart: unless-stopped assumes your Python script will run in an infinite loop. If your script is intended to run once and terminate, you should remove this directive. TZ=Etc/UTC should be set to your local time-zone. Never use quote marks on the right hand side of a TZ= variable. If you are running as a different user ID, you may want to change both IOTSTACK_UID and IOTSTACK_GID to appropriate values. Notes: Don't use user and group names because these variables are applied inside the container where those names are (probably) undefined. The only thing these variables affect is the ownership of: ~/IOTstack/volumes/python/app and its contents. If you want everything to be owned by root, set both of these variables to zero (eg IOTSTACK_UID=0 ). If your Python script listens to data-communications traffic, you can set up the port mappings by uncommenting the ports: directive. If your Python container is already running when you make a change to its service definition, you can apply it via: $ cd ~/IOTstack $ docker-compose up -d python","title":"customising your Python service definition"},{"location":"Containers/Python/#python-first-launch","text":"After running the menu, you are told to run the commands: $ cd ~/IOTstack $ docker-compose up -d This is what happens: docker-compose reads your docker-compose.yml . When it finds the service definition for Python, it encounters: build: ./services/python/. The leading period means \"the directory containing docker-compose.yml while the trailing period means \"Dockerfile\", so the path expands to: ~/IOTstack/services/python/Dockerfile The Dockerfile is processed. It downloads the base image for Python from Dockerhub and then makes changes including: copying the contents of the following directory into the image as a set of defaults: /home/pi/IOTstack/services/python/app copying the following file into the image: /home/pi/IOTstack/services/python/docker-entrypoint.sh The docker-entrypoint.sh script runs each time the container launches and performs initialisation and \"self repair\" functions. The output of the Dockerfile run is a new local image tagged with the name iotstack_python . The iotstack_python image is instantiated to become the running container. When the container starts, the docker-entrypoint.sh script runs and initialises the container's persistent storage area: $ tree -pu ~/IOTstack/volumes /home/pi/IOTstack/volumes \u2514\u2500\u2500 [ drwxr-xr-x root ] python \u2514\u2500\u2500 [ drwxr-xr-x pi ] app \u2514\u2500\u2500 [ -rwxr-xr-x pi ] app.py Note: the top-level python folder is owned by \"root\" but the app directory and its contents are owned by \"pi\". The initial app.py Python script is a \"hello world\" placeholder. It runs as an infinite loop emitting messages every 10 seconds until terminated. You can see what it is doing by running: $ docker logs -f python The world is born. Hello World. The world is re-born. Hello World. The world is re-born. Hello World. \u2026 Pressing control + c terminates the log display but does not terminate the running container.","title":"Python - first launch"},{"location":"Containers/Python/#stopping-the-python-service","text":"To stop the container from running, either: take down your whole stack: $ cd ~/IOTstack $ docker-compose down terminate the python container $ cd ~/IOTstack $ docker-compose rm --force --stop -v python","title":"stopping the Python service"},{"location":"Containers/Python/#starting-the-python-service","text":"To bring up the container again after you have stopped it, either: bring up your whole stack: $ cd ~/IOTstack $ docker-compose up -d bring up the python container $ cd ~/IOTstack $ docker-compose up -d python","title":"starting the Python service"},{"location":"Containers/Python/#python-second-and-subsequent-launch","text":"Each time you launch the Python container after the first launch: The existing local image ( iotstack_python ) is instantiated to become the running container. The docker-entrypoint.sh script runs and performs \"self-repair\" by replacing any files that have gone missing from the persistent storage area. Self-repair does not overwrite existing files! The app.py Python script is run.","title":"Python - second-and-subsequent launch"},{"location":"Containers/Python/#when-things-go-wrong-check-the-log","text":"If the container misbehaves, the log is your friend: $ docker logs python","title":"when things go wrong - check the log"},{"location":"Containers/Python/#project-development-life-cycle","text":"It is critical that you understand that all of your project development should occur within the folder: ~/IOTstack/volumes/python/app So long as you are performing some sort of routine backup (either with a supplied script or a third party solution like Paraphraser/IOTstackBackup ), your work will be protected.","title":"project development life-cycle"},{"location":"Containers/Python/#getting-started","text":"Start by editing the file: ~/IOTstack/volumes/python/app/app.py If you need other supporting scripts or data files, also add those to the directory: ~/IOTstack/volumes/python/app Any time you change something in the app folder, tell the running python container to notice the change by: $ cd ~/IOTstack $ docker-compose restart python","title":"getting started"},{"location":"Containers/Python/#reading-and-writing-to-disk","text":"Consider this line in the service definition: - ./volumes/python/app:/usr/src/app The leading period means \"the directory containing docker-compose.yml \" so it the same as: - ~/IOTstack/volumes/python/app:/usr/src/app Then, you split the line at the \":\", resulting in: The external directory = ~/IOTstack/volumes/python/app The internal directory = /usr/src/app What it means is that: Any file you put into the external directory (or any sub-directories you create within the external directory) will be visible to your Python script running inside the container at the same relative position in the internal directory. Any file or sub-directory created in the internal directory by your Python script running inside the container will be visible outside the container at the same relative position in the external directory. The contents of external directory and, therefore, the internal directory will persist across container launches. If your script writes into any other directory inside the container, the data will be lost when the container re-launches.","title":"reading and writing to disk"},{"location":"Containers/Python/#getting-a-clean-slate","text":"If you make a mess of things and need to start from a clean slate, erase the persistent storage area: $ cd ~/IOTstack $ docker-compose rm --force --stop -v python $ sudo rm -rf ./volumes/python $ docker-compose up -d python The container will re-initialise the persistent storage area from its defaults.","title":"getting a clean slate"},{"location":"Containers/Python/#adding-packages","text":"As you develop your project, you may find that you need to add supporting packages. For this example, we will assume you want to add \" Flask \" and \" beautifulsoup4 \". If you were developing a project outside of container-space, you would simply run: $ pip3 install -U Flask beautifulsoup4 You can do the same thing with the running container: $ docker exec python pip3 install -U Flask beautifulsoup4 and that will work \u2014 until the container is re-launched, at which point the added packages will disappear. To make Flask and beautifulsoup4 a permanent part of your container: Change your working directory: $ cd ~/IOTstack/services/python/app Use your favourite text editor to create the file requirements.txt in that directory. Each package you want to add should be on a line by itself: Flask beautifulsoup4 Tell Docker to rebuild the local Python image: $ cd ~/IOTstack $ docker-compose build --force-rm python $ docker-compose up -d --force-recreate python $ docker system prune -f Note: You will see a warning about running pip as root - ignore it. Confirm that the packages have been added: $ docker exec python pip3 freeze | grep -e \"Flask\" -e \"beautifulsoup4\" beautifulsoup4==4.10.0 Flask==2.0.1 Continue your development work by returning to getting started . Note: The first time you following the process described above to create requirements.txt , a copy will appear at: ~/IOTstack/volumes/python/app/requirements.txt This copy is the result of the \"self-repair\" code that runs each time the container starts noticing that requirements.txt is missing and making a copy from the defaults stored inside the image. If you make more changes to the master version of requirements.txt in the services directory and rebuild the local image, the copy in the volumes directory will not be kept in-sync. That's because the \"self-repair\" code never overwrites existing files. If you want to bring the copy of requirements.txt in the volumes directory up-to-date: $ cd ~/IOTstack $ rm ./volumes/python/app/requirements.txt $ docker-compose restart python The requirements.txt file will be recreated and it will be a copy of the version in the services directory as of the last image rebuild.","title":"adding packages"},{"location":"Containers/Python/#making-your-own-python-script-the-default","text":"Suppose the Python script you have been developing reaches a major milestone and you decide to \"freeze dry\" your work up to that point so that it becomes the default when you ask for a clean slate . Proceed like this: If you have added any packages by following the steps in adding packages , run the following command: $ docker exec python bash -c 'pip3 freeze >requirements.txt' That generates a requirements.txt representing the state of play inside the running container. Because it is running inside the container, the requirements.txt created by that command appears outside the container at: ~/IOTstack/volumes/python/app/requirements.txt Make your work the default: $ cd ~/IOTstack $ cp -r ./volumes/python/app/* ./services/python/app The cp command copies: your Python script; the optional requirements.txt (from step 1); and any other files you may have put into the Python working directory. Key point: everything copied into ./services/python/app will become part of the new local image. Terminate the Python container and erase its persistent storage area: $ cd ~/IOTstack $ docker-compose rm --force --stop -v python $ sudo rm -rf ./volumes/python Note: If erasing the persistent storage area feels too risky, just move it out of the way: $ cd ~/IOTstack/volumes $ sudo mv python python.off Rebuild the local image: $ cd ~/IOTstack $ docker-compose build --force-rm python $ docker-compose up -d --force-recreate python On its first launch, the new container will re-populate the persistent storage area but, this time, it will be your Python script and any other supporting files, rather than the original \"hello world\" script. Clean up by removing the old local image: $ docker system prune -f","title":"making your own Python script the default"},{"location":"Containers/Python/#canning-your-project","text":"Suppose your project has reached the stage where you wish to put it into production as a service under its own name. Make two further assumptions: You have gone through the steps in making your own Python script the default and you are certain that the content of ./services/python/app correctly captures your project. You want to give your project the name \"wishbone\". Proceed like this: Stop the development project: $ cd ~/IOTstack $ docker-compose rm --force --stop -v python Remove the existing local image: $ docker rmi iotstack_python Rename the python services directory to the name of your project: $ cd ~/IOTstack/services $ mv python wishbone Edit the python service definition in docker-compose.yml and replace references to python with the name of your project. In the following, the original is on the left, the edited version on the right, and the lines that need to change are indicated with a \"|\": python: | wishbone: container_name: python | container_name: wishbone build: ./services/python/. | build: ./services/wishbone/. restart: unless-stopped restart: unless-stopped environment: environment: - TZ=Etc/UTC - TZ=Etc/UTC - IOTSTACK_UID=1000 - IOTSTACK_UID=1000 - IOTSTACK_GID=1000 - IOTSTACK_GID=1000 # ports: # ports: # - \"external:internal\" # - \"external:internal\" volumes: volumes: - ./volumes/python/app:/usr/src/app | - ./volumes/wishbone/app:/usr/src/app Note: if you make a copy of the python service definition and then perform the required \"wishbone\" edits on the copy, the python definition will still be active so docker-compose may try to bring up both services. You will eliminate the risk of confusing yourself if you follow these instructions \"as written\" by not leaving the python service definition in place. Start the renamed service: $ cd ~/IOTstack $ docker-compose up -d wishbone Remember: After you have done this, the persistent storage area will be at the path: ~/IOTstack/volumes/wishbone/app","title":"canning your project"},{"location":"Containers/Python/#routine-maintenance","text":"To make sure you are running from the most-recent base image of Python from Dockerhub: $ cd ~/IOTstack $ docker-compose build --no-cache --pull python $ docker-compose up -d python $ docker system prune -f $ docker system prune -f In words: Be in the right directory. Force docker-compose to download the most-recent version of the Python base image from Dockerhub, and then run the Dockerfile to build a new local image. Instantiate the newly-built local image. Remove the old local image. Remove the old base image The old base image can't be removed until the old local image has been removed, which is why the prune command needs to be run twice. Note: If you have followed the steps in canning your project and your service has a name other than python , just substitute the new name where you see python in the two dockerc-compose commands.","title":"routine maintenance"},{"location":"Containers/RTL_433-docker/","text":"RTL_433 Docker \u00b6 Requirements, you will need to have a SDR dongle for you to be able to use RTL. I've tested this with a RTL2838 Make sure you can see your receiver by running lsusb $ lsusb Bus 003 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Bus 002 Device 001 : ID 1d6b:0003 Linux Foundation 3 .0 root hub Bus 001 Device 004 : ID 0bda:2838 Realtek Semiconductor Corp. RTL2838 DVB-T Bus 001 Device 002 : ID 2109 :3431 VIA Labs, Inc. Hub Bus 001 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Before starting the container please install RTL_433 from the native installs menu. This will setup your environment with the correct variables and programs. It is also advised to run RTL_433 to verify that it is working correctly on your system. The container is designed to send all detected messages over mqtt Edit the IOTstack/services/rtl_433/rtl_433.env file with your relevant settings for your mqtt server: MQTT_ADDRESS=mosquitto MQTT_PORT=1833 #MQTT_USER=myuser #MQTT_PASSWORD=mypassword MQTT_TOPIC=RTL_433 the container starts with the command rtl_433 -F mqtt:.... currently it does not filter any packets, you will need to do this in Node-RED","title":"RTL_433 Docker"},{"location":"Containers/RTL_433-docker/#rtl_433-docker","text":"Requirements, you will need to have a SDR dongle for you to be able to use RTL. I've tested this with a RTL2838 Make sure you can see your receiver by running lsusb $ lsusb Bus 003 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Bus 002 Device 001 : ID 1d6b:0003 Linux Foundation 3 .0 root hub Bus 001 Device 004 : ID 0bda:2838 Realtek Semiconductor Corp. RTL2838 DVB-T Bus 001 Device 002 : ID 2109 :3431 VIA Labs, Inc. Hub Bus 001 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Before starting the container please install RTL_433 from the native installs menu. This will setup your environment with the correct variables and programs. It is also advised to run RTL_433 to verify that it is working correctly on your system. The container is designed to send all detected messages over mqtt Edit the IOTstack/services/rtl_433/rtl_433.env file with your relevant settings for your mqtt server: MQTT_ADDRESS=mosquitto MQTT_PORT=1833 #MQTT_USER=myuser #MQTT_PASSWORD=mypassword MQTT_TOPIC=RTL_433 the container starts with the command rtl_433 -F mqtt:.... currently it does not filter any packets, you will need to do this in Node-RED","title":"RTL_433 Docker"},{"location":"Containers/TasmoAdmin/","text":"TasmoAdmin \u00b6 References \u00b6 Homepage Docker Web interface \u00b6 The web UI can be found on \"your_ip\":8088 Usage \u00b6 (instructions to follow)","title":"TasmoAdmin"},{"location":"Containers/TasmoAdmin/#tasmoadmin","text":"","title":"TasmoAdmin"},{"location":"Containers/TasmoAdmin/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/TasmoAdmin/#web-interface","text":"The web UI can be found on \"your_ip\":8088","title":"Web interface"},{"location":"Containers/TasmoAdmin/#usage","text":"(instructions to follow)","title":"Usage"},{"location":"Containers/Telegraf/","text":"Telegraf \u00b6 This document discusses an IOTstack-specific version of Telegraf built on top of influxdata/influxdata-docker/telegraf using a Dockerfile . The purpose of the Dockerfile is to: tailor the default configuration to be IOTstack-ready; and enable the container to perform self-repair if essential elements of the persistent storage area disappear. References \u00b6 influxdata Telegraf home GitHub : influxdata/influxdata-docker/telegraf DockerHub : influxdata Telegraf Significant directories and files \u00b6 ~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 telegraf \u2502 \u251c\u2500\u2500 Dockerfile \u2776 \u2502 \u251c\u2500\u2500 entrypoint.sh \u2777 \u2502 \u251c\u2500\u2500 iotstack_defaults \u2502 \u2502 \u251c\u2500\u2500 additions \u2778 \u2502 \u2502 \u2514\u2500\u2500 auto_include \u2779 \u2502 \u2514\u2500\u2500 service.yml \u277a \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 telegraf \u2502 \u2514\u2500\u2500 service.yml \u277b \u251c\u2500\u2500 docker-compose.yml \u2514\u2500\u2500 volumes \u2514\u2500\u2500 telegraf \u277c \u251c\u2500\u2500 additions \u277d \u251c\u2500\u2500 telegraf-reference.conf \u2792 \u2514\u2500\u2500 telegraf.conf \u2793 The Dockerfile used to customise Telegraf for IOTstack. A replacement for the telegraf container script of the same name, extended to handle container self-repair. The additions folder . See Applying optional additions . The auto_include folder . Additions automatically applied to telegraf.conf . See Automatic includes to telegraf.conf . The template service definition . The working service definition (only relevant to old-menu, copied from \u2779). The persistent storage area for the telegraf container. A working copy of the additions folder (copied from \u2778). See Applying optional additions . The reference configuration file . See Changing Telegraf's configuration . The active configuration file . A subset of \u2792 altered to support communication with InfluxDB running in a container in the same IOTstack instance. Everything in the persistent storage area \u277c: will be replaced if it is not present when the container starts; but will never be overwritten if altered by you. How Telegraf gets built for IOTstack \u00b6 Telegraf images ( DockerHub ) \u00b6 Periodically, the source code is recompiled and the resulting image is pushed to influxdata Telegraf on DockerHub . IOTstack menu \u00b6 When you select Telegraf in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used. IOTstack first run \u00b6 On a first install of IOTstack, you run the menu, choose your containers, and are told to do this: $ cd ~/IOTstack $ docker-compose up -d See also the Migration considerations (below). docker-compose reads the Compose file. When it arrives at the telegraf fragment, it finds: telegraf: container_name: telegraf build: ./.templates/telegraf/. \u2026 The build statement tells docker-compose to look for: ~/IOTstack/.templates/telegraf/Dockerfile The Dockerfile is in the .templates directory because it is intended to be a common build for all IOTstack users. This is different to the arrangement for Node-RED where the Dockerfile is in the services directory because it is how each individual IOTstack user's version of Node-RED is customised. The Dockerfile begins with: FROM telegraf:latest If you need to pin to a particular version of Telegraf, the Dockerfile is the place to do it. See Telegraf version pinning . The FROM statement tells the build process to pull down the base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The customisations are: Add the rsync package. This helps the container perform self-repair. Copy the default configuration file that comes with the DockerHub image (so it will be available as a fully-commented reference for the user) and make it read-only. Make a working version of the default configuration file from which comment lines and blank lines have been removed. Patch the working version to support communications with InfluxDB running in another container in the same IOTstack instance. Replace entrypoint.sh with a version which: calls rsync to perform self-repair if telegraf.conf goes missing; and enforces root:root ownership in ~/IOTstack/volumes/telegraf . The local image is instantiated to become your running container. When you run the docker images command after Telegraf has been built, you may see two rows for Telegraf: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_telegraf latest 59861b7fe9ed 2 hours ago 292MB telegraf latest a721ac170fad 3 days ago 273MB telegraf is the base image ; and iotstack_telegraf is the local image . You may see the same pattern in Portainer , which reports the base image as \"unused\". You should not remove the base image, even though it appears to be unused. Whether you see one or two rows depends on the version of docker-compose you are using and how your version of docker-compose builds local images. Migration considerations \u00b6 Under the original IOTstack implementation of Telegraf (just \"as it comes\" from DockerHub ), the service definition expected telegraf.conf to be at: ~/IOTstack/services/telegraf/telegraf.conf Under this implementation of Telegraf, the configuration file has moved to: ~/IOTstack/volumes/telegraf/telegraf.conf The change of location is one of the things that allows self-repair to work properly. With one exception, all prior and current versions of the default configuration file are identical in terms of their semantics. In other words, once you strip away comments and blank lines, and remove any \"active\" configuration options that simply repeat their default setting, you get the same subset of \"active\" configuration options. The default configuration file supplied with gcgarner/IOTstack is available here if you wish to refer to it. The exception is [[inputs.mqtt_consumer]] which is now provided as an optional addition. If your existing Telegraf configuration depends on that input, you will need to apply it. See applying optional additions . Logging \u00b6 You can inspect Telegraf's log by: $ docker logs telegraf These logs are ephemeral and will disappear when your Telegraf container is rebuilt. log message: database \"telegraf\" creation failed \u00b6 The following log message can be misleading: W! [outputs.influxdb] When writing to [http://influxdb:8086]: database \"telegraf\" creation failed: Post \"http://influxdb:8086/query\": dial tcp 172.30.0.9:8086: connect: connection refused If InfluxDB is not running when Telegraf starts, the depends_on: clause in Telegraf's service definition tells Docker to start InfluxDB (and Mosquitto) before starting Telegraf. Although it can launch the InfluxDB container first, Docker has no way of knowing when the influxd process running inside the InfluxDB container will start listening to port 8086. What this error message usually means is that Telegraf has tried to communicate with InfluxDB before the latter is ready to accept connections. Telegraf typically retries after a short delay and is then able to communicate with InfluxDB. Changing Telegraf's configuration \u00b6 The first time you launch the Telegraf container, the following structure will be created in the persistent storage area: ~/IOTstack/volumes/telegraf \u251c\u2500\u2500 [drwxr-xr-x root ] additions \u2502 \u251c\u2500\u2500 [-rw-r--r-- root ] inputs.docker.conf \u2502 \u2514\u2500\u2500 [-rw-r--r-- root ] inputs.mqtt_consumer.conf \u251c\u2500\u2500 [-rw-r--r-- root ] telegraf.conf \u2514\u2500\u2500 [-r--r--r-- root ] telegraf-reference.conf The file: telegraf-reference.conf : is a reference copy of the default configuration file that ships with the base image for Telegraf when it is downloaded from DockerHub. It is nearly 9000 lines long and is mostly comments. is not used by Telegraf but will be replaced if you delete it. is marked \"read-only\" (even for root) as a reminder that it is only for your reference. Any changes you make will be ignored. telegraf.conf : is created by removing all comment lines and blank lines from telegraf-reference.conf , leaving only the \"active\" configuration options, and then adding options necessary for IOTstack. is less than 30 lines and is significantly easier to understand than telegraf-reference.conf . inputs.docker.conf \u2013 see Applying optional additions below. The intention of this structure is that you: search telegraf-reference.conf to find the configuration option you need; read the comments to understand what the option does and how to use it; and then import the option into the correct section of telegraf.conf . When you make a change to telegraf.conf , you activate it by restarting the container: $ cd ~/IOTstack $ docker-compose restart telegraf Automatic includes to telegraf.conf \u00b6 inputs.docker.conf instructs Telegraf to collect metrics from Docker. Requires kernel control groups to be enabled to collect memory usage data. If not done during initial installation, enable by running (reboot required): echo $(cat /boot/cmdline.txt) cgroup_memory=1 cgroup_enable=memory | sudo tee /boot/cmdline.txt `inputs.cpu_temp.conf' collects cpu temperature. Applying optional additions \u00b6 The additions folder (see Significant directories and files ) is a mechanism for additional IOTstack-ready configuration options to be provided for Telegraf. Currently there is one addition: inputs.mqtt_consumer.conf which formed part of the gcgarner/IOTstack telegraf configuration and instructs Telegraf to subscribe to a metric feed from the Mosquitto broker. This assumes, of course, that something is publishing those metrics. Using inputs.mqtt_consumer.conf as the example, applying that addition to your Telegraf configuration file involves: $ cd ~/IOTstack/volumes/telegraf $ grep -v \"^#\" additions/inputs.mqtt_consumer.conf | sudo tee -a telegraf.conf >/dev/null $ cd ~/IOTstack $ docker-compose restart telegraf The grep strips comment lines and the sudo tee is a safe way of appending the result to telegraf.conf . The restart causes Telegraf to notice the change. Getting a clean slate \u00b6 Erasing the persistent storage area \u00b6 Erasing Telegraf's persistent storage area triggers self-healing and restores known defaults: $ cd ~/IOTstack $ docker-compose rm --force --stop -v telegraf $ sudo rm -rf ./volumes/telegraf $ docker-compose up -d telegraf Note: You can also remove individual files within the persistent storage area and then trigger self-healing. For example, if you decide to edit telegraf-reference.conf and make a mess, you can restore the original version like this: $ cd ~/IOTstack $ sudo rm ./volumes/telegraf/telegraf-reference.conf $ docker-compose restart telegraf Resetting the InfluxDB database \u00b6 To reset the InfluxDB database that Telegraf writes into, proceed like this: $ cd ~/IOTstack $ docker-compose rm --force --stop -v telegraf $ docker exec -it influxdb influx -precision=rfc3339 > drop database telegraf > exit $ docker-compose up -d telegraf In words: Be in the right directory. Stop the Telegraf container (while leaving the InfluxDB container running). Launch the Influx CLI inside the InfluxDB container. Delete the telegraf database, and then exit the CLI. Start the Telegraf container. This re-creates the database automatically. Upgrading Telegraf \u00b6 You can update most containers like this: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. This strategy doesn't work when a Dockerfile is used to build a local image on top of a base image downloaded from DockerHub . The local image is what is running so there is no way for the pull to sense when a newer version becomes available. The only way to know when an update to Telegraf is available is to check the Telegraf tags page on DockerHub . Once a new version appears on DockerHub , you can upgrade Telegraf like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull telegraf $ docker-compose up -d telegraf $ docker system prune $ docker system prune Breaking it down into parts: build causes the named container to be rebuilt; --no-cache tells the Dockerfile process that it must not take any shortcuts. It really must rebuild the local image ; --pull tells the Dockerfile process to actually check with DockerHub to see if there is a later version of the base image and, if so, to download it before starting the build; telegraf is the named container argument required by the build command. Your existing Telegraf container continues to run while the rebuild proceeds. Once the freshly-built local image is ready, the up tells docker-compose to do a new-for-old swap. There is barely any downtime for your service. The prune is the simplest way of cleaning up. The first call removes the old local image . The second call cleans up the old base image . Whether an old base image exists depends on the version of docker-compose you are using and how your version of docker-compose builds local images. Telegraf version pinning \u00b6 If you need to pin Telegraf to a particular version: Use your favourite text editor to open the following file: ~/IOTstack/.templates/telegraf/Dockerfile Find the line: FROM telegraf:latest Replace latest with the version you wish to pin to. For example, to pin to version 1.19.3: FROM telegraf:1.19.3 Save the file and tell docker-compose to rebuild the local image : $ cd ~/IOTstack $ docker-compose up -d --build telegraf $ docker system prune The new local image is built, then the new container is instantiated based on that image. The prune deletes the old local image . Note: As well as preventing Docker from updating the base image , pinning will also block incoming updates to the Dockerfile from a git pull . Nothing will change until you decide to remove the pin.","title":"Telegraf"},{"location":"Containers/Telegraf/#telegraf","text":"This document discusses an IOTstack-specific version of Telegraf built on top of influxdata/influxdata-docker/telegraf using a Dockerfile . The purpose of the Dockerfile is to: tailor the default configuration to be IOTstack-ready; and enable the container to perform self-repair if essential elements of the persistent storage area disappear.","title":"Telegraf"},{"location":"Containers/Telegraf/#references","text":"influxdata Telegraf home GitHub : influxdata/influxdata-docker/telegraf DockerHub : influxdata Telegraf","title":"References"},{"location":"Containers/Telegraf/#significant-directories-and-files","text":"~/IOTstack \u251c\u2500\u2500 .templates \u2502 \u2514\u2500\u2500 telegraf \u2502 \u251c\u2500\u2500 Dockerfile \u2776 \u2502 \u251c\u2500\u2500 entrypoint.sh \u2777 \u2502 \u251c\u2500\u2500 iotstack_defaults \u2502 \u2502 \u251c\u2500\u2500 additions \u2778 \u2502 \u2502 \u2514\u2500\u2500 auto_include \u2779 \u2502 \u2514\u2500\u2500 service.yml \u277a \u251c\u2500\u2500 services \u2502 \u2514\u2500\u2500 telegraf \u2502 \u2514\u2500\u2500 service.yml \u277b \u251c\u2500\u2500 docker-compose.yml \u2514\u2500\u2500 volumes \u2514\u2500\u2500 telegraf \u277c \u251c\u2500\u2500 additions \u277d \u251c\u2500\u2500 telegraf-reference.conf \u2792 \u2514\u2500\u2500 telegraf.conf \u2793 The Dockerfile used to customise Telegraf for IOTstack. A replacement for the telegraf container script of the same name, extended to handle container self-repair. The additions folder . See Applying optional additions . The auto_include folder . Additions automatically applied to telegraf.conf . See Automatic includes to telegraf.conf . The template service definition . The working service definition (only relevant to old-menu, copied from \u2779). The persistent storage area for the telegraf container. A working copy of the additions folder (copied from \u2778). See Applying optional additions . The reference configuration file . See Changing Telegraf's configuration . The active configuration file . A subset of \u2792 altered to support communication with InfluxDB running in a container in the same IOTstack instance. Everything in the persistent storage area \u277c: will be replaced if it is not present when the container starts; but will never be overwritten if altered by you.","title":"Significant directories and files"},{"location":"Containers/Telegraf/#how-telegraf-gets-built-for-iotstack","text":"","title":"How Telegraf gets built for IOTstack"},{"location":"Containers/Telegraf/#telegraf-images-dockerhub","text":"Periodically, the source code is recompiled and the resulting image is pushed to influxdata Telegraf on DockerHub .","title":"Telegraf images (DockerHub)"},{"location":"Containers/Telegraf/#iotstack-menu","text":"When you select Telegraf in the IOTstack menu, the template service definition is copied into the Compose file. Under old menu, it is also copied to the working service definition and then not really used.","title":"IOTstack menu"},{"location":"Containers/Telegraf/#iotstack-first-run","text":"On a first install of IOTstack, you run the menu, choose your containers, and are told to do this: $ cd ~/IOTstack $ docker-compose up -d See also the Migration considerations (below). docker-compose reads the Compose file. When it arrives at the telegraf fragment, it finds: telegraf: container_name: telegraf build: ./.templates/telegraf/. \u2026 The build statement tells docker-compose to look for: ~/IOTstack/.templates/telegraf/Dockerfile The Dockerfile is in the .templates directory because it is intended to be a common build for all IOTstack users. This is different to the arrangement for Node-RED where the Dockerfile is in the services directory because it is how each individual IOTstack user's version of Node-RED is customised. The Dockerfile begins with: FROM telegraf:latest If you need to pin to a particular version of Telegraf, the Dockerfile is the place to do it. See Telegraf version pinning . The FROM statement tells the build process to pull down the base image from DockerHub . It is a base image in the sense that it never actually runs as a container on your Raspberry Pi. The remaining instructions in the Dockerfile customise the base image to produce a local image . The customisations are: Add the rsync package. This helps the container perform self-repair. Copy the default configuration file that comes with the DockerHub image (so it will be available as a fully-commented reference for the user) and make it read-only. Make a working version of the default configuration file from which comment lines and blank lines have been removed. Patch the working version to support communications with InfluxDB running in another container in the same IOTstack instance. Replace entrypoint.sh with a version which: calls rsync to perform self-repair if telegraf.conf goes missing; and enforces root:root ownership in ~/IOTstack/volumes/telegraf . The local image is instantiated to become your running container. When you run the docker images command after Telegraf has been built, you may see two rows for Telegraf: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE iotstack_telegraf latest 59861b7fe9ed 2 hours ago 292MB telegraf latest a721ac170fad 3 days ago 273MB telegraf is the base image ; and iotstack_telegraf is the local image . You may see the same pattern in Portainer , which reports the base image as \"unused\". You should not remove the base image, even though it appears to be unused. Whether you see one or two rows depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"IOTstack first run"},{"location":"Containers/Telegraf/#migration-considerations","text":"Under the original IOTstack implementation of Telegraf (just \"as it comes\" from DockerHub ), the service definition expected telegraf.conf to be at: ~/IOTstack/services/telegraf/telegraf.conf Under this implementation of Telegraf, the configuration file has moved to: ~/IOTstack/volumes/telegraf/telegraf.conf The change of location is one of the things that allows self-repair to work properly. With one exception, all prior and current versions of the default configuration file are identical in terms of their semantics. In other words, once you strip away comments and blank lines, and remove any \"active\" configuration options that simply repeat their default setting, you get the same subset of \"active\" configuration options. The default configuration file supplied with gcgarner/IOTstack is available here if you wish to refer to it. The exception is [[inputs.mqtt_consumer]] which is now provided as an optional addition. If your existing Telegraf configuration depends on that input, you will need to apply it. See applying optional additions .","title":"Migration considerations"},{"location":"Containers/Telegraf/#logging","text":"You can inspect Telegraf's log by: $ docker logs telegraf These logs are ephemeral and will disappear when your Telegraf container is rebuilt.","title":"Logging"},{"location":"Containers/Telegraf/#log-message-database-telegraf-creation-failed","text":"The following log message can be misleading: W! [outputs.influxdb] When writing to [http://influxdb:8086]: database \"telegraf\" creation failed: Post \"http://influxdb:8086/query\": dial tcp 172.30.0.9:8086: connect: connection refused If InfluxDB is not running when Telegraf starts, the depends_on: clause in Telegraf's service definition tells Docker to start InfluxDB (and Mosquitto) before starting Telegraf. Although it can launch the InfluxDB container first, Docker has no way of knowing when the influxd process running inside the InfluxDB container will start listening to port 8086. What this error message usually means is that Telegraf has tried to communicate with InfluxDB before the latter is ready to accept connections. Telegraf typically retries after a short delay and is then able to communicate with InfluxDB.","title":"log message: database \"telegraf\" creation failed"},{"location":"Containers/Telegraf/#changing-telegrafs-configuration","text":"The first time you launch the Telegraf container, the following structure will be created in the persistent storage area: ~/IOTstack/volumes/telegraf \u251c\u2500\u2500 [drwxr-xr-x root ] additions \u2502 \u251c\u2500\u2500 [-rw-r--r-- root ] inputs.docker.conf \u2502 \u2514\u2500\u2500 [-rw-r--r-- root ] inputs.mqtt_consumer.conf \u251c\u2500\u2500 [-rw-r--r-- root ] telegraf.conf \u2514\u2500\u2500 [-r--r--r-- root ] telegraf-reference.conf The file: telegraf-reference.conf : is a reference copy of the default configuration file that ships with the base image for Telegraf when it is downloaded from DockerHub. It is nearly 9000 lines long and is mostly comments. is not used by Telegraf but will be replaced if you delete it. is marked \"read-only\" (even for root) as a reminder that it is only for your reference. Any changes you make will be ignored. telegraf.conf : is created by removing all comment lines and blank lines from telegraf-reference.conf , leaving only the \"active\" configuration options, and then adding options necessary for IOTstack. is less than 30 lines and is significantly easier to understand than telegraf-reference.conf . inputs.docker.conf \u2013 see Applying optional additions below. The intention of this structure is that you: search telegraf-reference.conf to find the configuration option you need; read the comments to understand what the option does and how to use it; and then import the option into the correct section of telegraf.conf . When you make a change to telegraf.conf , you activate it by restarting the container: $ cd ~/IOTstack $ docker-compose restart telegraf","title":"Changing Telegraf's configuration"},{"location":"Containers/Telegraf/#automatic-includes-to-telegrafconf","text":"inputs.docker.conf instructs Telegraf to collect metrics from Docker. Requires kernel control groups to be enabled to collect memory usage data. If not done during initial installation, enable by running (reboot required): echo $(cat /boot/cmdline.txt) cgroup_memory=1 cgroup_enable=memory | sudo tee /boot/cmdline.txt `inputs.cpu_temp.conf' collects cpu temperature.","title":"Automatic includes to telegraf.conf"},{"location":"Containers/Telegraf/#applying-optional-additions","text":"The additions folder (see Significant directories and files ) is a mechanism for additional IOTstack-ready configuration options to be provided for Telegraf. Currently there is one addition: inputs.mqtt_consumer.conf which formed part of the gcgarner/IOTstack telegraf configuration and instructs Telegraf to subscribe to a metric feed from the Mosquitto broker. This assumes, of course, that something is publishing those metrics. Using inputs.mqtt_consumer.conf as the example, applying that addition to your Telegraf configuration file involves: $ cd ~/IOTstack/volumes/telegraf $ grep -v \"^#\" additions/inputs.mqtt_consumer.conf | sudo tee -a telegraf.conf >/dev/null $ cd ~/IOTstack $ docker-compose restart telegraf The grep strips comment lines and the sudo tee is a safe way of appending the result to telegraf.conf . The restart causes Telegraf to notice the change.","title":"Applying optional additions"},{"location":"Containers/Telegraf/#getting-a-clean-slate","text":"","title":"Getting a clean slate"},{"location":"Containers/Telegraf/#erasing-the-persistent-storage-area","text":"Erasing Telegraf's persistent storage area triggers self-healing and restores known defaults: $ cd ~/IOTstack $ docker-compose rm --force --stop -v telegraf $ sudo rm -rf ./volumes/telegraf $ docker-compose up -d telegraf Note: You can also remove individual files within the persistent storage area and then trigger self-healing. For example, if you decide to edit telegraf-reference.conf and make a mess, you can restore the original version like this: $ cd ~/IOTstack $ sudo rm ./volumes/telegraf/telegraf-reference.conf $ docker-compose restart telegraf","title":"Erasing the persistent storage area"},{"location":"Containers/Telegraf/#resetting-the-influxdb-database","text":"To reset the InfluxDB database that Telegraf writes into, proceed like this: $ cd ~/IOTstack $ docker-compose rm --force --stop -v telegraf $ docker exec -it influxdb influx -precision=rfc3339 > drop database telegraf > exit $ docker-compose up -d telegraf In words: Be in the right directory. Stop the Telegraf container (while leaving the InfluxDB container running). Launch the Influx CLI inside the InfluxDB container. Delete the telegraf database, and then exit the CLI. Start the Telegraf container. This re-creates the database automatically.","title":"Resetting the InfluxDB database"},{"location":"Containers/Telegraf/#upgrading-telegraf","text":"You can update most containers like this: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune In words: docker-compose pull downloads any newer images; docker-compose up -d causes any newly-downloaded images to be instantiated as containers (replacing the old containers); and the prune gets rid of the outdated images. This strategy doesn't work when a Dockerfile is used to build a local image on top of a base image downloaded from DockerHub . The local image is what is running so there is no way for the pull to sense when a newer version becomes available. The only way to know when an update to Telegraf is available is to check the Telegraf tags page on DockerHub . Once a new version appears on DockerHub , you can upgrade Telegraf like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull telegraf $ docker-compose up -d telegraf $ docker system prune $ docker system prune Breaking it down into parts: build causes the named container to be rebuilt; --no-cache tells the Dockerfile process that it must not take any shortcuts. It really must rebuild the local image ; --pull tells the Dockerfile process to actually check with DockerHub to see if there is a later version of the base image and, if so, to download it before starting the build; telegraf is the named container argument required by the build command. Your existing Telegraf container continues to run while the rebuild proceeds. Once the freshly-built local image is ready, the up tells docker-compose to do a new-for-old swap. There is barely any downtime for your service. The prune is the simplest way of cleaning up. The first call removes the old local image . The second call cleans up the old base image . Whether an old base image exists depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"Upgrading Telegraf"},{"location":"Containers/Telegraf/#telegraf-version-pinning","text":"If you need to pin Telegraf to a particular version: Use your favourite text editor to open the following file: ~/IOTstack/.templates/telegraf/Dockerfile Find the line: FROM telegraf:latest Replace latest with the version you wish to pin to. For example, to pin to version 1.19.3: FROM telegraf:1.19.3 Save the file and tell docker-compose to rebuild the local image : $ cd ~/IOTstack $ docker-compose up -d --build telegraf $ docker system prune The new local image is built, then the new container is instantiated based on that image. The prune deletes the old local image . Note: As well as preventing Docker from updating the base image , pinning will also block incoming updates to the Dockerfile from a git pull . Nothing will change until you decide to remove the pin.","title":"Telegraf version pinning"},{"location":"Containers/WireGuard/","text":"WireGuard \u00b6 WireGuard is a fast, modern, secure Virtual Private Network (VPN) tunnel. It can securely connect you to your home network, allowing you to access your home network's local services from anywhere. It can also secure your traffic when using public internet connections. Reference: WireGuard home page Assumptions: These instructions assume that you have privileges to configure your network's gateway (router). If you are not able to make changes to your network's firewall settings, then you will not be able to finish this setup. In common with most VPN technologies, WireGuard assumes that the WAN side of your network's gateway has a public IP address which is reachable directly. WireGuard may not work if that assumption does not hold. If you strike this problem, you have to take it up with your ISP. Installing WireGuard under IOTstack \u00b6 You increase your chances of a trouble-free installation by performing the installation steps in the following order. Step 1: Update your Raspberry Pi OS \u00b6 To be able to run WireGuard successfully, your Raspberry Pi needs to be fully up-to-date. If you want to understand why, see the read only flag . $ sudo apt update $ sudo apt upgrade -y Step 2: Set up a Dynamic DNS name \u00b6 Before you can use WireGuard (or any VPN solution), you need a mechanism for your remote clients to reach your home router. You have two choices: Obtain a permanent IP address for your home router from your Internet Service Provider (ISP). Approach your ISP if you wish to pursue this option. It generally involves additional charges. Use a Dynamic DNS service. See IOTstack documentation Accessing your device from the internet . The rest of this documentation assumes you have chosen this option. Step 3: Understand the Service Definition \u00b6 This is the service definition template that IOTstack uses for WireGuard: wireguard: container_name: wireguard image: ghcr.io/linuxserver/wireguard restart: unless-stopped environment: - PUID=1000 - PGID=1000 - TZ=Etc/UTC - SERVERURL=your.dynamic.dns.name - SERVERPORT=51820 - PEERS=laptop,phone,tablet - PEERDNS=auto # - PEERDNS=172.30.0.1 - ALLOWEDIPS=0.0.0.0/0 ports: - \"51820:51820/udp\" volumes: - ./volumes/wireguard:/config - /lib/modules:/lib/modules:ro cap_add: - NET_ADMIN - SYS_MODULE sysctls: - net.ipv4.conf.all.src_valid_mark=1 Unfortunately, that service definition will not work \"as is\". It needs to be configured. Key points: Everything in the environment: section from SERVERURL= down to PEERDNS= (inclusive) affects WireGuard's generated configurations (the QR codes). In other words, any time you change any of those values, any existing QR codes will stop working. Step 4: Decide what to configure \u00b6 With most containers, you can continue to tweak environment variables and settings without upsetting the container's basic behaviour. WireGuard is a little different. You really need to think, carefully, about how you want to configure the service before you start. If you change your mind later, you generally have to start from a clean slate . Fields that you should always configure \u00b6 TZ= should be set to your local timezone. Example: - TZ=Australia/Sydney Note: Never use quote marks around the right hand side of a time-zone definition. SERVERURL= should be set to the domain name you have registered with a Dynamic DNS service provider. Example: - SERVERURL=downunda.duckdns.org PEERS= should be a comma-separated list of your client devices (all the phones, tablets, laptops, desktops you want to use remotely to get back into your home network). Example: - PEERS=jill-macbook,jack-chromebook,alex-nokia-g10 Note: Many examples on the web use \"PEERS=n\" where \"n\" is a number. In practice, that approach seems to be a little fragile and is not recommended for IOTstack. Optional configuration - DNS resolution for peers \u00b6 You have several options for how your remote peers resolve DNS requests: PEERDNS=auto DNS queries made on connected WireGuard clients should work as if they were made on the host. If you configure your ad-blocker into the host's resolveconf.conf , Wireguard clients will also automatically use it. Details: The default value of auto instructs the WireGuard service running within the WireGuard container to use a DNS-service, coredns, also running in the Wireguard container. Coredns by default directs queries to 127.0.0.11, which Docker intercepts and forwards to whichever resolvers are specified in the Raspberry Pi's /etc/resolv.conf . PEERDNS=auto with custom-cont-init This configuration instructs WireGuard to forward DNS queries from remote peers to any host daemon or container which is listening on port 53. This is the option you will want to choose if you are running an ad-blocking DNS server (eg PiHole or AdGuardHome ) in a container on the same host as WireGuard, and you want your remote clients to obtain DNS resolution via the ad-blocker, but don't want your Raspberry Pi host to use it. Acknowledgement: thanks to @ukkopahis for developing this option. To activate this feature: Make sure your WireGuard service definition contains PEERDNS=auto . Start the WireGuard container by executing: $ cd ~/IOTstack $ docker-compose up -d wireguard This ensures that the ~/IOTstack/volumes/wireguard folder structure is created and remote client configurations are (re)generated properly. Run the following commands: $ cd ~/IOTstack $ sudo cp ./.templates/wireguard/use-container-dns.sh ./volumes/wireguard/custom-cont-init.d/ $ docker-compose restart wireguard The presence of use-container-dns.sh causes WireGuard to redirect incoming DNS queries to the default gateway on the internal bridged network. That, in turn, results in the queries being forwarded to any other container that is listening for DNS traffic on port 53. It does not matter if that other container is PiHole, AdGuardHome, bind9 or any other kind of DNS server. Do note, however, that this configuration creates a dependency between WireGuard and the container providing DNS resolution. You may wish to make that explicit in your docker-compose.yml by adding these lines to your WireGuard service definition: depends_on : - pihole Substitute adguardhome or bind9 for pihole , as appropriate. Once activated, this feature will remain active until you decide to deactivate it. If you ever wish to deactivate it, run the following commands: $ cd ~/IOTstack $ sudo rm ./volumes/wireguard/custom-cont-init.d/use-container-dns.sh $ docker-compose restart wireguard PEERDNS=\u00abip address\u00bb A third possibility is if you have a local upstream DNS server. You can specify the IP address of that server so that remote peers receive DNS resolution from that host. For example: - PEERDNS=192.168.203.65 Do note that changes to PEERDNS will not be updated to existing clients, and as such you may want to use PEERDNS=auto unless you have a very specific requirement. Optional configuration - WireGuard ports \u00b6 The WireGuard service definition template follows the convention of using UDP port \"51820\" in three places. You can leave it like that and it will just work. There is no reason to change the defaults unless you want to. To understand what each port number does, it is better to think of them like this: environment: - SERVERPORT=\u00abpublic\u00bb ports: - \"\u00abexternal\u00bb:\u00abinternal\u00bb/udp\" These definitions are going to be used throughout this documentation: The \u00abpublic\u00bb port is the port number that your remote WireGuard clients (phone, laptop etc) will try to reach. This is the port number that your router needs to expose to the outside world. The \u00abexternal\u00bb port is the port number that Docker, running on your Raspberry Pi, will be listening on. Your router needs to forward WireGuard incoming traffic to the \u00abexternal\u00bb port on your Raspberry Pi. The \u00abinternal\u00bb port is the port number that WireGuard (the server process) will be listening on inside the WireGuard container. Docker handles forwarding between the \u00abexternal\u00bb and \u00abinternal\u00bb port. Rule #1: You can change the \u00abpublic\u00bb and \u00abexternal\u00bb ports but you can't change the \u00abinternal\u00bb port unless you are prepared to do a lot more work. Rule #2: The \u00abpublic\u00bb port forms part of the QR codes. If you decide to change the \u00abpublic\u00bb port after you generate the QR codes, you will have to start over from a clean slate . Rule #3: Your router needs to know about both the \u00abpublic\u00bb and \u00abexternal\u00bb ports so, if you decide to change either of those, you must also reconfigure your router. See Understanding WireGuard's port numbers if you want more information on how the various port numbers are used. Step 5: Configure WireGuard \u00b6 There are two approaches: Let the menu generate a docker-compose.yml with the default WireGuard service definition template, and then edit docker-compose.yml . Prepare a compose-override.yml file, then run the menu and have it perform the substitutions for you. Of the two, the first is generally the simpler and means you don't have to re-run the menu whenever you want to change WireGuard's configuration. Method 1: Configure WireGuard by editing docker-compose.yml \u00b6 Run the menu: $ cd ~/IOTstack $ ./menu.sh Choose the \"Build Stack\" option. If WireGuard is not already selected, select it. Press enter to begin the build. Choose Exit. Open docker-compose.yml in your favourite text editor. Navigate to the WireGuard service definition. Implement the decisions you took in decide what to configure . Save your work. Method 2: Configure WireGuard using compose-override.yml \u00b6 The Custom services and overriding default settings for IOTstack page describes how to use an override file to allow the menu to incorporate your custom configurations into the final docker-compose.yml file. You will need to create the compose-override.yml before running the menu to build your stack. If you have already built your stack, you'll have to rebuild it after creating compose-override.yml . Use your favourite text editor to create (or open) the override file. The file is expected to be at the path: ~/IOTstack/compose-override.yml Define overrides to implement the decisions you took in Decide what to configure . For example: services: wireguard: environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney - SERVERURL=downunda.duckdns.org - SERVERPORT=51820 - PEERS=laptop,phone,tablet - PEERDNS=auto - ALLOWEDIPS=0.0.0.0/0 Key points: The override file works at the section level. Therefore, you have to include all of the environment variables from the template, not just the ones you want to alter. If your override file contains configurations for other containers, make sure the file only has a single services: directive at the start. Save your work. Run the menu: $ cd ~/IOTstack $ ./menu.sh Choose the \"Build Stack\" option. If WireGuard is not already selected, select it. Press enter to begin the build. Choose Exit. Check your work by running: $ cat docker-compose.yml and verify that the wireguard service definition is as you expect. Step 6: Start WireGuard \u00b6 To start WireGuard, bring up your stack: $ cd ~/IOTstack $ docker-compose up -d Confirm that WireGuard has started properly by running: $ docker ps --format \"table {{.Names}}\\t{{.RunningFor}}\\t{{.Status}}\" --filter name = wireguard Repeat the command a few times with a short delay in between. You are looking for signs that the WireGuard container is restarting. If the container seems to be restarting then this command is your friend: $ docker logs wireguard See also discussion of the read-only flag . Confirm that WireGuard has generated the expected configurations. For example, given the following setting in docker-compose.yml : - PEERS=jill-macbook,jack-chromebook,alex-nokia-g10 you would expect a result something like this: $ tree ./volumes/wireguard volumes/wireguard/ \u251c\u2500\u2500 coredns \u2502 \u2514\u2500\u2500 Corefile \u251c\u2500\u2500 custom-cont-init.d \u251c\u2500\u2500 custom-services.d \u251c\u2500\u2500 peer_jack-chromebook \u2502 \u251c\u2500\u2500 peer_jack-chromebook.conf \u2502 \u251c\u2500\u2500 peer_jack-chromebook.png \u2502 \u251c\u2500\u2500 privatekey-peer_jack-chromebook \u2502 \u2514\u2500\u2500 publickey-peer_jack-chromebook \u251c\u2500\u2500 peer_jill-macbook \u2502 \u251c\u2500\u2500 peer_jill-macbook.conf \u2502 \u251c\u2500\u2500 peer_jill-macbook.png \u2502 \u251c\u2500\u2500 privatekey-peer_jill-macbook \u2502 \u2514\u2500\u2500 publickey-peer_jill-macbook \u251c\u2500\u2500 peer_alex-nokia-g10 \u2502 \u251c\u2500\u2500 peer_alex-nokia-g10.conf \u2502 \u251c\u2500\u2500 peer_alex-nokia-g10.png \u2502 \u251c\u2500\u2500 privatekey-peer_alex-nokia-g10 \u2502 \u2514\u2500\u2500 publickey-peer_alex-nokia-g10 \u251c\u2500\u2500 server \u2502 \u251c\u2500\u2500 privatekey-server \u2502 \u2514\u2500\u2500 publickey-server \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 peer.conf \u2502 \u2514\u2500\u2500 server.conf \u2514\u2500\u2500 wg0.conf Notice how each element in the PEERS= list is represented by a sub-directory prefixed with peer_ . You should expect the same pattern for your peers. Step 7: Save your WireGuard client configuration files (QR codes) \u00b6 The first time you launch WireGuard, it generates cryptographically protected configurations for your remote clients and encapsulates those configurations in QR codes. You can see the QR codes by running: $ docker logs wireguard WireGuard's log is ephemeral, which means it resets each time the container is re-created. In other words, you can't rely on going back to the log to obtain your QR codes if you lose them. WireGuard also records the QR codes as .png files. In fact, the QR codes shown by docker logs wireguard are just side-effects of the .png files as they are created. If your Raspberry Pi has a GUI (such as a screen attached to an HDMI port or a VNC connection), you can always retrieve the QR codes by opening the .png files in the GUI. If, however, your Raspberry Pi is running headless, you will need to copy the .png files to a system that is capable of displaying them, such as a Mac or PC. You can use SCP to do that. See ssh tutorial if you need help setting up SSH (of which SCP is a part). For example, to copy all PNG files from your Raspberry Pi to a target system: $ find ~/IOTstack/volumes/wireguard -name \"*.png\" -exec scp {} user@hostorip:. \\; Note: hostorip is the host name, fully-qualified domain name, multicast domain name or IP address of the GUI-capable target computer; and user is a valid username on the target computer. If you want to work in the other direction (ie from the GUI-capable system), you can try: $ scp pi@hostorip:IOTstack/volumes/wireguard/peer_jill-macbook/peer_jill-macbook.png . In this case: hostorip is the host name, fully-qualified domain name, multicast domain name or IP address of the Raspberry Pi that is running WireGuard. Keep in mind that each QR code contains everything needed for any device to access your home network via WireGuard. Treat your .png files as \"sensitive documents\". Step 8: Configure your router with a NAT rule \u00b6 A typical home network will have a firewall that effectively blocks all incoming attempts from the Internet to open a new connection with a device on your network. To use a VPN from outside of your home network (which is precisely the point of running the service!), you need to configure your router to allow incoming WireGuard traffic to reach the Raspberry Pi running WireGuard. These instructions assume you have the privileges to do that. If you have not used your router's administrative interface before, the default login credentials may be physically printed on the device or in its instruction manual. If you have never changed the default login credentials, you should take the time to do that. Routers have wildly different user interfaces but the concepts will be the same. This section describes the basic technique but if you are unsure how to do this on your particular router model, the best idea would be to search the web for: \"[YOUR DEVICE NAME] port forwarding configuration\"; or \"[YOUR DEVICE NAME] NAT configuration\" A typical configuration process goes something like this: The router sub-process you need to configure is called Network Address Translation (NAT) but it's not unheard of for this functionality to be grouped with FireWall. The NAT component you are looking for probably has a name like \"Port Redirection\", \"Port Forwarding\", \"NAT Forwarding\" or \"NAT Virtual Server\". It might also be under \"Open Ports\" but those are usually one-to-one mappings (ie incomingPort=outgoingPort), apply to port ranges, and are intended to target a single DMZ host. The configuration screen will contain at least the following fields: Field Value Interface router's WAN interface Private IP x.x.x.x Private Port \u00abexternal\u00bb Protocol UDP Public Port \u00abpublic\u00bb Service Name WireGuard The fields in the above list are in alphabetical order. They will almost certainly be in a different order in your router and may also have different names: Interface is typically a popup menu. Generally it will either default to the name of the physical port on your router that connects to the outside world, or be some other sensible default like \"All\". Private IP (or Internal IP ) is the IP address of the Raspberry Pi running WireGuard. Note that this pretty much forces you to give your Raspberry Pi a statically-configured IP address (either a static binding in your DHCP server or a hard-coded address in the Raspberry Pi itself). Private Port (or Internal Port ) needs to be the value you chose for \u00abexternal\u00bb in the WireGuard service definition (51820 if you didn't change it). Yes, this does sound counterintuitive but it's a matter of perspective. From the router's perspective, the port is on the private or internal part of your home network. From Docker's perspective, the port is \u00abexternal\u00bb to container-space. Protocol will usually default to \"TCP\" but you must change it to \"UDP\". Public Port or External Port needs to be the value you chose for \u00abpublic\u00bb in the WireGuard service definition (51820 if you didn't change it). Service Name (or Service Type ) is typically a text field, an editable menu (where you can either make a choice or type your own value), or a button approximating an editable menu. If you are given the option of choosing \"WireGuard\", do that, otherwise just type that name into the field. It has no significance other than reminding you what the rule is for. Step 9: Configure your remote WireGuard clients \u00b6 This is a massive topic and one which is well beyond the scope of this guide. You really will have to work it out for yourself. Start by Googling: \"[YOUR DEVICE NAME] install WireGuard client\". You will find the list of client software at WireGuard Installation . For portable devices (eg iOS and Android) it usually boils down to: Install the app on your portable device. Display the QR code the WireGuard server generated for the device. Launch the app. Point the device's camera at the QR code. Follow your nose. Understanding WireGuard's port numbers \u00b6 Here's a concrete example configuration using three different port numbers: environment: - SERVERURL=downunda.duckdns.org - SERVERPORT=51620 ports: - \"51720:51820/udp\" In other words: The \u00abpublic\u00bb port is 51620. The \u00abexternal\u00bb port is 51720. The \u00abinternal\u00bb port is 51820. You also need to make a few assumptions: The host running the remote WireGuard client (eg a mobile phone with the WireGuard app installed) has been allocated the IP address 55.66.77.88 when it connected to the Internet over 3G/4G/5G. When the remote WireGuard client initiated the session, it chose UDP port 44524 as its source port. The actual number chosen is (essentially) random and only significant to the client. Your Internet Service Provider allocated the IP address 12.13.14.15 to the WAN side of your router. You have done all the steps in Set up a Dynamic DNS name and your WAN IP address (12.13.14.15) is being propagated to your Dynamic DNS service provider. Here's a reference model to help explain what occurs: The remote WireGuard client: Obtains the Dynamic DNS domain name (\"downunda.duckdns.org\") and \u00abpublic\u00bb UDP port (51620) from the configuration contained within the QR code. Recall that those values are obtained from the SERVERURL= and SERVERPORT= environment variables in docker-compose.yml . Executes a DNS query for the domain name \"downunda.duckdns.org\" to obtains the WAN IP address (12.13.14.15) of your home router. Addresses outgoing packets to 12.13.14.15:51620. You configure a NAT port-forwarding rule in your router which accepts incoming traffic on the \u00abpublic\u00bb UDP port (51620) and uses Network Address Translation to change the destination IP address to the Raspberry Pi and destination port to the \u00abexternal\u00bb UDP port (51720). In other words, each incoming packet is readdressed to 192.168.203.60:51720. Docker is listening to the Raspberry Pi's \u00abexternal\u00bb UDP port 51720. Docker uses Network Address Translation to change the destination IP address to the WireGuard container and destination port to the \u00abinternal\u00bb UDP port (51820). In other words, each incoming packet is readdressed to 172.18.0.6:51820. The packet is then routed to the internal bridged network, and delivered to the WireGuard server process running in the container which is listening on the \u00abinternal\u00bb UDP port (51820). A reciprocal process occurs when the WireGuard server process sends packets back to the remote WireGuard client. The following table summarises the transformations as the client and server exchange information: Even if you use port 51820 everywhere (the default), all this Network Address Translation still occurs. Keep this in mind if you are trying to debug WireGuard because you may actually find it simpler to understand what is going on if you use different numbers for the \u00abpublic\u00bb and \u00abexternal\u00bb ports. This model is a slight simplification because the remote client may also be also operating behind a router performing Network Address Translation. It is just easier to understand the basic concepts if you assume the remote client has a publicly-routable IP address. Debugging techniques \u00b6 Monitor WireGuard traffic between your router and your Raspberry Pi \u00b6 If tcpdump is not installed on your Raspberry Pi, you can install it by: $ sudo apt install tcpdump After that, you can capture traffic between your router and your Raspberry Pi by: $ sudo tcpdump -i eth0 -n udp port \u00abexternal\u00bb Press ctrl c to terminate the capture. Monitor WireGuard traffic between your Raspberry Pi and the WireGuard container \u00b6 First, you need to add tcpdump to the container. You only need to do this once per debugging session. The package will remain in place until the next time you re-create the container. $ docker exec wireguard bash -c 'apt update ; apt install -y tcpdump' To monitor traffic: $ docker exec wireguard tcpdump -i eth0 -n udp port \u00abinternal\u00bb Press ctrl c to terminate the capture. Is Docker listening on the Raspberry Pi's \u00abexternal\u00bb port? \u00b6 $ PORT = \u00abexternal\u00bb ; sudo nmap -sU -p $PORT 127 .0.0.1 | grep \" $PORT /udp\" There will be a short delay. The expected answer is either: \u00abexternal\u00bb/udp open|filtered unknown = Docker is listening \u00abexternal\u00bb/udp closed unknown = Docker is not listening Success implies that the container is also listening. Is your router listening on the \u00abpublic\u00bb port? \u00b6 $ PORT = \u00abpublic\u00bb ; sudo nmap -sU -p $PORT downunda.duckdns.org | grep \" $PORT /udp\" There will be a short delay. The expected answer is either: \u00abpublic\u00bb/udp open|filtered unknown = router is listening \u00abpublic\u00bb/udp closed unknown = router is not listening The read-only flag \u00b6 The :ro at the end of the following line in WireGuard's service definition means \"read only\": - /lib/modules:/lib/modules:ro If that flag is omitted then WireGuard may try to update the /lib/modules path in your operating system. To be clear, /lib/modules is both outside the WireGuard container and outside the normal persistent storage area in the ./volumes directory. The basic idea of containers is that processes are contained , include all their own dependencies, can be added and removed cleanly, and don't change the underlying operating system. Writing into /lib/modules is not needed on a Raspberry Pi, providing that Raspberry Pi OS is up-to-date. That is why the first step in the installation procedure tells you to bring the system up-to-date. If WireGuard refuses to install and you have good reason to suspect that WireGuard may be trying to write to /lib/modules then you can consider removing the :ro flag and re-trying. Just be aware that WireGuard will likely be modifying your operating system. Updating WireGuard \u00b6 To update the WireGuard container: $ cd ~/IOTstack $ docker-compose pull wireguard If a new image comes down, then: $ docker-compose up -d wireguard $ docker system prune Getting a clean slate \u00b6 If WireGuard misbehaves, you can start over from a clean slate. You may also need to do this if you change any of the following environment variables: - SERVERURL= - SERVERPORT= - PEERS= - PEERDNS= The procedure is: If WireGuard is running, terminate it: $ cd ~/IOTstack $ docker-compose rm --force --stop -v wireguard Erase the persistent storage area (essential): $ sudo rm -rf ./volumes/wireguard Be very careful with that command and double-check your work before you hit return . Erasing the persistent storage area: destroys the old client configurations and invalidates any copies of QR codes. Existing clients will stop working until presented with a new QR code. deactivates PEERDNS=auto with custom-cont-init . Start WireGuard: $ docker-compose up -d wireguard This will generate new client configurations and QR codes for your devices. Remember to re-activate PEERDNS=auto with custom-cont-init if you need it.","title":"WireGuard"},{"location":"Containers/WireGuard/#wireguard","text":"WireGuard is a fast, modern, secure Virtual Private Network (VPN) tunnel. It can securely connect you to your home network, allowing you to access your home network's local services from anywhere. It can also secure your traffic when using public internet connections. Reference: WireGuard home page Assumptions: These instructions assume that you have privileges to configure your network's gateway (router). If you are not able to make changes to your network's firewall settings, then you will not be able to finish this setup. In common with most VPN technologies, WireGuard assumes that the WAN side of your network's gateway has a public IP address which is reachable directly. WireGuard may not work if that assumption does not hold. If you strike this problem, you have to take it up with your ISP.","title":"WireGuard"},{"location":"Containers/WireGuard/#installing-wireguard-under-iotstack","text":"You increase your chances of a trouble-free installation by performing the installation steps in the following order.","title":"Installing WireGuard under IOTstack"},{"location":"Containers/WireGuard/#step-1-update-your-raspberry-pi-os","text":"To be able to run WireGuard successfully, your Raspberry Pi needs to be fully up-to-date. If you want to understand why, see the read only flag . $ sudo apt update $ sudo apt upgrade -y","title":"Step 1: Update your Raspberry Pi OS"},{"location":"Containers/WireGuard/#step-2-set-up-a-dynamic-dns-name","text":"Before you can use WireGuard (or any VPN solution), you need a mechanism for your remote clients to reach your home router. You have two choices: Obtain a permanent IP address for your home router from your Internet Service Provider (ISP). Approach your ISP if you wish to pursue this option. It generally involves additional charges. Use a Dynamic DNS service. See IOTstack documentation Accessing your device from the internet . The rest of this documentation assumes you have chosen this option.","title":"Step 2: Set up a Dynamic DNS name"},{"location":"Containers/WireGuard/#step-3-understand-the-service-definition","text":"This is the service definition template that IOTstack uses for WireGuard: wireguard: container_name: wireguard image: ghcr.io/linuxserver/wireguard restart: unless-stopped environment: - PUID=1000 - PGID=1000 - TZ=Etc/UTC - SERVERURL=your.dynamic.dns.name - SERVERPORT=51820 - PEERS=laptop,phone,tablet - PEERDNS=auto # - PEERDNS=172.30.0.1 - ALLOWEDIPS=0.0.0.0/0 ports: - \"51820:51820/udp\" volumes: - ./volumes/wireguard:/config - /lib/modules:/lib/modules:ro cap_add: - NET_ADMIN - SYS_MODULE sysctls: - net.ipv4.conf.all.src_valid_mark=1 Unfortunately, that service definition will not work \"as is\". It needs to be configured. Key points: Everything in the environment: section from SERVERURL= down to PEERDNS= (inclusive) affects WireGuard's generated configurations (the QR codes). In other words, any time you change any of those values, any existing QR codes will stop working.","title":"Step 3: Understand the Service Definition"},{"location":"Containers/WireGuard/#step-4-decide-what-to-configure","text":"With most containers, you can continue to tweak environment variables and settings without upsetting the container's basic behaviour. WireGuard is a little different. You really need to think, carefully, about how you want to configure the service before you start. If you change your mind later, you generally have to start from a clean slate .","title":"Step 4: Decide what to configure"},{"location":"Containers/WireGuard/#fields-that-you-should-always-configure","text":"TZ= should be set to your local timezone. Example: - TZ=Australia/Sydney Note: Never use quote marks around the right hand side of a time-zone definition. SERVERURL= should be set to the domain name you have registered with a Dynamic DNS service provider. Example: - SERVERURL=downunda.duckdns.org PEERS= should be a comma-separated list of your client devices (all the phones, tablets, laptops, desktops you want to use remotely to get back into your home network). Example: - PEERS=jill-macbook,jack-chromebook,alex-nokia-g10 Note: Many examples on the web use \"PEERS=n\" where \"n\" is a number. In practice, that approach seems to be a little fragile and is not recommended for IOTstack.","title":"Fields that you should always configure "},{"location":"Containers/WireGuard/#optional-configuration-dns-resolution-for-peers","text":"You have several options for how your remote peers resolve DNS requests: PEERDNS=auto DNS queries made on connected WireGuard clients should work as if they were made on the host. If you configure your ad-blocker into the host's resolveconf.conf , Wireguard clients will also automatically use it. Details: The default value of auto instructs the WireGuard service running within the WireGuard container to use a DNS-service, coredns, also running in the Wireguard container. Coredns by default directs queries to 127.0.0.11, which Docker intercepts and forwards to whichever resolvers are specified in the Raspberry Pi's /etc/resolv.conf . PEERDNS=auto with custom-cont-init This configuration instructs WireGuard to forward DNS queries from remote peers to any host daemon or container which is listening on port 53. This is the option you will want to choose if you are running an ad-blocking DNS server (eg PiHole or AdGuardHome ) in a container on the same host as WireGuard, and you want your remote clients to obtain DNS resolution via the ad-blocker, but don't want your Raspberry Pi host to use it. Acknowledgement: thanks to @ukkopahis for developing this option. To activate this feature: Make sure your WireGuard service definition contains PEERDNS=auto . Start the WireGuard container by executing: $ cd ~/IOTstack $ docker-compose up -d wireguard This ensures that the ~/IOTstack/volumes/wireguard folder structure is created and remote client configurations are (re)generated properly. Run the following commands: $ cd ~/IOTstack $ sudo cp ./.templates/wireguard/use-container-dns.sh ./volumes/wireguard/custom-cont-init.d/ $ docker-compose restart wireguard The presence of use-container-dns.sh causes WireGuard to redirect incoming DNS queries to the default gateway on the internal bridged network. That, in turn, results in the queries being forwarded to any other container that is listening for DNS traffic on port 53. It does not matter if that other container is PiHole, AdGuardHome, bind9 or any other kind of DNS server. Do note, however, that this configuration creates a dependency between WireGuard and the container providing DNS resolution. You may wish to make that explicit in your docker-compose.yml by adding these lines to your WireGuard service definition: depends_on : - pihole Substitute adguardhome or bind9 for pihole , as appropriate. Once activated, this feature will remain active until you decide to deactivate it. If you ever wish to deactivate it, run the following commands: $ cd ~/IOTstack $ sudo rm ./volumes/wireguard/custom-cont-init.d/use-container-dns.sh $ docker-compose restart wireguard PEERDNS=\u00abip address\u00bb A third possibility is if you have a local upstream DNS server. You can specify the IP address of that server so that remote peers receive DNS resolution from that host. For example: - PEERDNS=192.168.203.65 Do note that changes to PEERDNS will not be updated to existing clients, and as such you may want to use PEERDNS=auto unless you have a very specific requirement.","title":"Optional configuration - DNS resolution for peers"},{"location":"Containers/WireGuard/#optional-configuration-wireguard-ports","text":"The WireGuard service definition template follows the convention of using UDP port \"51820\" in three places. You can leave it like that and it will just work. There is no reason to change the defaults unless you want to. To understand what each port number does, it is better to think of them like this: environment: - SERVERPORT=\u00abpublic\u00bb ports: - \"\u00abexternal\u00bb:\u00abinternal\u00bb/udp\" These definitions are going to be used throughout this documentation: The \u00abpublic\u00bb port is the port number that your remote WireGuard clients (phone, laptop etc) will try to reach. This is the port number that your router needs to expose to the outside world. The \u00abexternal\u00bb port is the port number that Docker, running on your Raspberry Pi, will be listening on. Your router needs to forward WireGuard incoming traffic to the \u00abexternal\u00bb port on your Raspberry Pi. The \u00abinternal\u00bb port is the port number that WireGuard (the server process) will be listening on inside the WireGuard container. Docker handles forwarding between the \u00abexternal\u00bb and \u00abinternal\u00bb port. Rule #1: You can change the \u00abpublic\u00bb and \u00abexternal\u00bb ports but you can't change the \u00abinternal\u00bb port unless you are prepared to do a lot more work. Rule #2: The \u00abpublic\u00bb port forms part of the QR codes. If you decide to change the \u00abpublic\u00bb port after you generate the QR codes, you will have to start over from a clean slate . Rule #3: Your router needs to know about both the \u00abpublic\u00bb and \u00abexternal\u00bb ports so, if you decide to change either of those, you must also reconfigure your router. See Understanding WireGuard's port numbers if you want more information on how the various port numbers are used.","title":"Optional configuration - WireGuard ports"},{"location":"Containers/WireGuard/#step-5-configure-wireguard","text":"There are two approaches: Let the menu generate a docker-compose.yml with the default WireGuard service definition template, and then edit docker-compose.yml . Prepare a compose-override.yml file, then run the menu and have it perform the substitutions for you. Of the two, the first is generally the simpler and means you don't have to re-run the menu whenever you want to change WireGuard's configuration.","title":"Step 5: Configure WireGuard"},{"location":"Containers/WireGuard/#method-1-configure-wireguard-by-editing-docker-composeyml","text":"Run the menu: $ cd ~/IOTstack $ ./menu.sh Choose the \"Build Stack\" option. If WireGuard is not already selected, select it. Press enter to begin the build. Choose Exit. Open docker-compose.yml in your favourite text editor. Navigate to the WireGuard service definition. Implement the decisions you took in decide what to configure . Save your work.","title":"Method 1: Configure WireGuard by editing docker-compose.yml"},{"location":"Containers/WireGuard/#method-2-configure-wireguard-using-compose-overrideyml","text":"The Custom services and overriding default settings for IOTstack page describes how to use an override file to allow the menu to incorporate your custom configurations into the final docker-compose.yml file. You will need to create the compose-override.yml before running the menu to build your stack. If you have already built your stack, you'll have to rebuild it after creating compose-override.yml . Use your favourite text editor to create (or open) the override file. The file is expected to be at the path: ~/IOTstack/compose-override.yml Define overrides to implement the decisions you took in Decide what to configure . For example: services: wireguard: environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney - SERVERURL=downunda.duckdns.org - SERVERPORT=51820 - PEERS=laptop,phone,tablet - PEERDNS=auto - ALLOWEDIPS=0.0.0.0/0 Key points: The override file works at the section level. Therefore, you have to include all of the environment variables from the template, not just the ones you want to alter. If your override file contains configurations for other containers, make sure the file only has a single services: directive at the start. Save your work. Run the menu: $ cd ~/IOTstack $ ./menu.sh Choose the \"Build Stack\" option. If WireGuard is not already selected, select it. Press enter to begin the build. Choose Exit. Check your work by running: $ cat docker-compose.yml and verify that the wireguard service definition is as you expect.","title":"Method 2: Configure WireGuard using compose-override.yml"},{"location":"Containers/WireGuard/#step-6-start-wireguard","text":"To start WireGuard, bring up your stack: $ cd ~/IOTstack $ docker-compose up -d Confirm that WireGuard has started properly by running: $ docker ps --format \"table {{.Names}}\\t{{.RunningFor}}\\t{{.Status}}\" --filter name = wireguard Repeat the command a few times with a short delay in between. You are looking for signs that the WireGuard container is restarting. If the container seems to be restarting then this command is your friend: $ docker logs wireguard See also discussion of the read-only flag . Confirm that WireGuard has generated the expected configurations. For example, given the following setting in docker-compose.yml : - PEERS=jill-macbook,jack-chromebook,alex-nokia-g10 you would expect a result something like this: $ tree ./volumes/wireguard volumes/wireguard/ \u251c\u2500\u2500 coredns \u2502 \u2514\u2500\u2500 Corefile \u251c\u2500\u2500 custom-cont-init.d \u251c\u2500\u2500 custom-services.d \u251c\u2500\u2500 peer_jack-chromebook \u2502 \u251c\u2500\u2500 peer_jack-chromebook.conf \u2502 \u251c\u2500\u2500 peer_jack-chromebook.png \u2502 \u251c\u2500\u2500 privatekey-peer_jack-chromebook \u2502 \u2514\u2500\u2500 publickey-peer_jack-chromebook \u251c\u2500\u2500 peer_jill-macbook \u2502 \u251c\u2500\u2500 peer_jill-macbook.conf \u2502 \u251c\u2500\u2500 peer_jill-macbook.png \u2502 \u251c\u2500\u2500 privatekey-peer_jill-macbook \u2502 \u2514\u2500\u2500 publickey-peer_jill-macbook \u251c\u2500\u2500 peer_alex-nokia-g10 \u2502 \u251c\u2500\u2500 peer_alex-nokia-g10.conf \u2502 \u251c\u2500\u2500 peer_alex-nokia-g10.png \u2502 \u251c\u2500\u2500 privatekey-peer_alex-nokia-g10 \u2502 \u2514\u2500\u2500 publickey-peer_alex-nokia-g10 \u251c\u2500\u2500 server \u2502 \u251c\u2500\u2500 privatekey-server \u2502 \u2514\u2500\u2500 publickey-server \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 peer.conf \u2502 \u2514\u2500\u2500 server.conf \u2514\u2500\u2500 wg0.conf Notice how each element in the PEERS= list is represented by a sub-directory prefixed with peer_ . You should expect the same pattern for your peers.","title":"Step 6: Start WireGuard"},{"location":"Containers/WireGuard/#step-7-save-your-wireguard-client-configuration-files-qr-codes","text":"The first time you launch WireGuard, it generates cryptographically protected configurations for your remote clients and encapsulates those configurations in QR codes. You can see the QR codes by running: $ docker logs wireguard WireGuard's log is ephemeral, which means it resets each time the container is re-created. In other words, you can't rely on going back to the log to obtain your QR codes if you lose them. WireGuard also records the QR codes as .png files. In fact, the QR codes shown by docker logs wireguard are just side-effects of the .png files as they are created. If your Raspberry Pi has a GUI (such as a screen attached to an HDMI port or a VNC connection), you can always retrieve the QR codes by opening the .png files in the GUI. If, however, your Raspberry Pi is running headless, you will need to copy the .png files to a system that is capable of displaying them, such as a Mac or PC. You can use SCP to do that. See ssh tutorial if you need help setting up SSH (of which SCP is a part). For example, to copy all PNG files from your Raspberry Pi to a target system: $ find ~/IOTstack/volumes/wireguard -name \"*.png\" -exec scp {} user@hostorip:. \\; Note: hostorip is the host name, fully-qualified domain name, multicast domain name or IP address of the GUI-capable target computer; and user is a valid username on the target computer. If you want to work in the other direction (ie from the GUI-capable system), you can try: $ scp pi@hostorip:IOTstack/volumes/wireguard/peer_jill-macbook/peer_jill-macbook.png . In this case: hostorip is the host name, fully-qualified domain name, multicast domain name or IP address of the Raspberry Pi that is running WireGuard. Keep in mind that each QR code contains everything needed for any device to access your home network via WireGuard. Treat your .png files as \"sensitive documents\".","title":"Step 7: Save your WireGuard client configuration files (QR codes)"},{"location":"Containers/WireGuard/#step-8-configure-your-router-with-a-nat-rule","text":"A typical home network will have a firewall that effectively blocks all incoming attempts from the Internet to open a new connection with a device on your network. To use a VPN from outside of your home network (which is precisely the point of running the service!), you need to configure your router to allow incoming WireGuard traffic to reach the Raspberry Pi running WireGuard. These instructions assume you have the privileges to do that. If you have not used your router's administrative interface before, the default login credentials may be physically printed on the device or in its instruction manual. If you have never changed the default login credentials, you should take the time to do that. Routers have wildly different user interfaces but the concepts will be the same. This section describes the basic technique but if you are unsure how to do this on your particular router model, the best idea would be to search the web for: \"[YOUR DEVICE NAME] port forwarding configuration\"; or \"[YOUR DEVICE NAME] NAT configuration\" A typical configuration process goes something like this: The router sub-process you need to configure is called Network Address Translation (NAT) but it's not unheard of for this functionality to be grouped with FireWall. The NAT component you are looking for probably has a name like \"Port Redirection\", \"Port Forwarding\", \"NAT Forwarding\" or \"NAT Virtual Server\". It might also be under \"Open Ports\" but those are usually one-to-one mappings (ie incomingPort=outgoingPort), apply to port ranges, and are intended to target a single DMZ host. The configuration screen will contain at least the following fields: Field Value Interface router's WAN interface Private IP x.x.x.x Private Port \u00abexternal\u00bb Protocol UDP Public Port \u00abpublic\u00bb Service Name WireGuard The fields in the above list are in alphabetical order. They will almost certainly be in a different order in your router and may also have different names: Interface is typically a popup menu. Generally it will either default to the name of the physical port on your router that connects to the outside world, or be some other sensible default like \"All\". Private IP (or Internal IP ) is the IP address of the Raspberry Pi running WireGuard. Note that this pretty much forces you to give your Raspberry Pi a statically-configured IP address (either a static binding in your DHCP server or a hard-coded address in the Raspberry Pi itself). Private Port (or Internal Port ) needs to be the value you chose for \u00abexternal\u00bb in the WireGuard service definition (51820 if you didn't change it). Yes, this does sound counterintuitive but it's a matter of perspective. From the router's perspective, the port is on the private or internal part of your home network. From Docker's perspective, the port is \u00abexternal\u00bb to container-space. Protocol will usually default to \"TCP\" but you must change it to \"UDP\". Public Port or External Port needs to be the value you chose for \u00abpublic\u00bb in the WireGuard service definition (51820 if you didn't change it). Service Name (or Service Type ) is typically a text field, an editable menu (where you can either make a choice or type your own value), or a button approximating an editable menu. If you are given the option of choosing \"WireGuard\", do that, otherwise just type that name into the field. It has no significance other than reminding you what the rule is for.","title":"Step 8: Configure your router with a NAT rule"},{"location":"Containers/WireGuard/#step-9-configure-your-remote-wireguard-clients","text":"This is a massive topic and one which is well beyond the scope of this guide. You really will have to work it out for yourself. Start by Googling: \"[YOUR DEVICE NAME] install WireGuard client\". You will find the list of client software at WireGuard Installation . For portable devices (eg iOS and Android) it usually boils down to: Install the app on your portable device. Display the QR code the WireGuard server generated for the device. Launch the app. Point the device's camera at the QR code. Follow your nose.","title":"Step 9: Configure your remote WireGuard clients"},{"location":"Containers/WireGuard/#understanding-wireguards-port-numbers","text":"Here's a concrete example configuration using three different port numbers: environment: - SERVERURL=downunda.duckdns.org - SERVERPORT=51620 ports: - \"51720:51820/udp\" In other words: The \u00abpublic\u00bb port is 51620. The \u00abexternal\u00bb port is 51720. The \u00abinternal\u00bb port is 51820. You also need to make a few assumptions: The host running the remote WireGuard client (eg a mobile phone with the WireGuard app installed) has been allocated the IP address 55.66.77.88 when it connected to the Internet over 3G/4G/5G. When the remote WireGuard client initiated the session, it chose UDP port 44524 as its source port. The actual number chosen is (essentially) random and only significant to the client. Your Internet Service Provider allocated the IP address 12.13.14.15 to the WAN side of your router. You have done all the steps in Set up a Dynamic DNS name and your WAN IP address (12.13.14.15) is being propagated to your Dynamic DNS service provider. Here's a reference model to help explain what occurs: The remote WireGuard client: Obtains the Dynamic DNS domain name (\"downunda.duckdns.org\") and \u00abpublic\u00bb UDP port (51620) from the configuration contained within the QR code. Recall that those values are obtained from the SERVERURL= and SERVERPORT= environment variables in docker-compose.yml . Executes a DNS query for the domain name \"downunda.duckdns.org\" to obtains the WAN IP address (12.13.14.15) of your home router. Addresses outgoing packets to 12.13.14.15:51620. You configure a NAT port-forwarding rule in your router which accepts incoming traffic on the \u00abpublic\u00bb UDP port (51620) and uses Network Address Translation to change the destination IP address to the Raspberry Pi and destination port to the \u00abexternal\u00bb UDP port (51720). In other words, each incoming packet is readdressed to 192.168.203.60:51720. Docker is listening to the Raspberry Pi's \u00abexternal\u00bb UDP port 51720. Docker uses Network Address Translation to change the destination IP address to the WireGuard container and destination port to the \u00abinternal\u00bb UDP port (51820). In other words, each incoming packet is readdressed to 172.18.0.6:51820. The packet is then routed to the internal bridged network, and delivered to the WireGuard server process running in the container which is listening on the \u00abinternal\u00bb UDP port (51820). A reciprocal process occurs when the WireGuard server process sends packets back to the remote WireGuard client. The following table summarises the transformations as the client and server exchange information: Even if you use port 51820 everywhere (the default), all this Network Address Translation still occurs. Keep this in mind if you are trying to debug WireGuard because you may actually find it simpler to understand what is going on if you use different numbers for the \u00abpublic\u00bb and \u00abexternal\u00bb ports. This model is a slight simplification because the remote client may also be also operating behind a router performing Network Address Translation. It is just easier to understand the basic concepts if you assume the remote client has a publicly-routable IP address.","title":"Understanding WireGuard's port numbers"},{"location":"Containers/WireGuard/#debugging-techniques","text":"","title":"Debugging techniques"},{"location":"Containers/WireGuard/#monitor-wireguard-traffic-between-your-router-and-your-raspberry-pi","text":"If tcpdump is not installed on your Raspberry Pi, you can install it by: $ sudo apt install tcpdump After that, you can capture traffic between your router and your Raspberry Pi by: $ sudo tcpdump -i eth0 -n udp port \u00abexternal\u00bb Press ctrl c to terminate the capture.","title":"Monitor WireGuard traffic between your router and your Raspberry Pi"},{"location":"Containers/WireGuard/#monitor-wireguard-traffic-between-your-raspberry-pi-and-the-wireguard-container","text":"First, you need to add tcpdump to the container. You only need to do this once per debugging session. The package will remain in place until the next time you re-create the container. $ docker exec wireguard bash -c 'apt update ; apt install -y tcpdump' To monitor traffic: $ docker exec wireguard tcpdump -i eth0 -n udp port \u00abinternal\u00bb Press ctrl c to terminate the capture.","title":"Monitor WireGuard traffic between your Raspberry Pi and the WireGuard container"},{"location":"Containers/WireGuard/#is-docker-listening-on-the-raspberry-pis-external-port","text":"$ PORT = \u00abexternal\u00bb ; sudo nmap -sU -p $PORT 127 .0.0.1 | grep \" $PORT /udp\" There will be a short delay. The expected answer is either: \u00abexternal\u00bb/udp open|filtered unknown = Docker is listening \u00abexternal\u00bb/udp closed unknown = Docker is not listening Success implies that the container is also listening.","title":"Is Docker listening on the Raspberry Pi's \u00abexternal\u00bb port?"},{"location":"Containers/WireGuard/#is-your-router-listening-on-the-public-port","text":"$ PORT = \u00abpublic\u00bb ; sudo nmap -sU -p $PORT downunda.duckdns.org | grep \" $PORT /udp\" There will be a short delay. The expected answer is either: \u00abpublic\u00bb/udp open|filtered unknown = router is listening \u00abpublic\u00bb/udp closed unknown = router is not listening","title":"Is your router listening on the \u00abpublic\u00bb port?"},{"location":"Containers/WireGuard/#the-read-only-flag","text":"The :ro at the end of the following line in WireGuard's service definition means \"read only\": - /lib/modules:/lib/modules:ro If that flag is omitted then WireGuard may try to update the /lib/modules path in your operating system. To be clear, /lib/modules is both outside the WireGuard container and outside the normal persistent storage area in the ./volumes directory. The basic idea of containers is that processes are contained , include all their own dependencies, can be added and removed cleanly, and don't change the underlying operating system. Writing into /lib/modules is not needed on a Raspberry Pi, providing that Raspberry Pi OS is up-to-date. That is why the first step in the installation procedure tells you to bring the system up-to-date. If WireGuard refuses to install and you have good reason to suspect that WireGuard may be trying to write to /lib/modules then you can consider removing the :ro flag and re-trying. Just be aware that WireGuard will likely be modifying your operating system.","title":"The read-only flag"},{"location":"Containers/WireGuard/#updating-wireguard","text":"To update the WireGuard container: $ cd ~/IOTstack $ docker-compose pull wireguard If a new image comes down, then: $ docker-compose up -d wireguard $ docker system prune","title":"Updating WireGuard"},{"location":"Containers/WireGuard/#getting-a-clean-slate","text":"If WireGuard misbehaves, you can start over from a clean slate. You may also need to do this if you change any of the following environment variables: - SERVERURL= - SERVERPORT= - PEERS= - PEERDNS= The procedure is: If WireGuard is running, terminate it: $ cd ~/IOTstack $ docker-compose rm --force --stop -v wireguard Erase the persistent storage area (essential): $ sudo rm -rf ./volumes/wireguard Be very careful with that command and double-check your work before you hit return . Erasing the persistent storage area: destroys the old client configurations and invalidates any copies of QR codes. Existing clients will stop working until presented with a new QR code. deactivates PEERDNS=auto with custom-cont-init . Start WireGuard: $ docker-compose up -d wireguard This will generate new client configurations and QR codes for your devices. Remember to re-activate PEERDNS=auto with custom-cont-init if you need it.","title":"Getting a clean slate"},{"location":"Containers/X2go/","text":"x2go \u00b6 x2go is an \"alternative\" to using VNC for a remote connection. It uses X11 forwarding over ssh to provide a desktop environment Reason for using: I have a Pi 4 and I didn't buy a micro hdmi cable. You can use VNC however you are limited to a 800x600 window. Installation \u00b6 Install with sudo apt install x2goserver x2go cant connect to the native Raspbian Desktop so you will need to install another with sudo tasksel I chose Xfce because it is light weight. Install the x2go client from their website Now I have a full-screen client YouTube tutorial \u00b6 Laurence systems","title":"x2go"},{"location":"Containers/X2go/#x2go","text":"x2go is an \"alternative\" to using VNC for a remote connection. It uses X11 forwarding over ssh to provide a desktop environment Reason for using: I have a Pi 4 and I didn't buy a micro hdmi cable. You can use VNC however you are limited to a 800x600 window.","title":"x2go"},{"location":"Containers/X2go/#installation","text":"Install with sudo apt install x2goserver x2go cant connect to the native Raspbian Desktop so you will need to install another with sudo tasksel I chose Xfce because it is light weight. Install the x2go client from their website Now I have a full-screen client","title":"Installation"},{"location":"Containers/X2go/#youtube-tutorial","text":"Laurence systems","title":"YouTube tutorial"},{"location":"Containers/Zigbee2MQTT/","text":"Zigbee2MQTT \u00b6 Web Guide Flashing the CC2531 Figuring-out your device identifier Service definition change - April 2021 \u00b6 The IOTstack service definition for Zigbee2MQTT is at the following path: ~/IOTstack/.templates/zigbee2mqtt/service.yml As of April 2021, the service definition changed: The Zigbee2MQTT container no longer runs in host mode. Adds timezone support. Builds the container from a Dockerfile providing appropriate defaults for IOTstack. Re-adds a port mapping for port 8080 (the Zigbee2MQTT web UI). If you were running the Zigbee2MQTT service before this change, you may wish to compare and contrast your active service definition (in docker-compose.yml ) with the revised template. Note: You may need to git pull to update your local copy of the IOTstack repository against GitHub. First startup with CC2531 adapter \u00b6 The service definition includes: devices: - /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter #- /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well The default device ( /dev/ttyAMA0 ) probably will not work for any Zigbee adapter. It is only there because /dev/ttyAMA0 exists on Raspberry Pis. Its presence permits the container to come up even though it will not actually be able to connect to an adapter. If you have a CC2531, properly flashed and connected to a USB port, you should be able to see it: $ ls -l /dev/ttyACM0 crw-rw---- 1 root dialout 166, 0 Apr 7 09:38 /dev/ttyACM0 If you see the error \"No such file or directory\", you will need to first figure out why your device is not visible. Assuming your CC2531 is visible: Change the device mapping in docker-compose.yml to deactivate ttyAMA0 in favour of activating ttyACM0 : devices: #- /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter - /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well Bring up the container: $ cd ~/IOTstack $ docker-compose up -d zigbee2mqtt You can also follow the instructions in the Zigbee2MQTT documentation to work out the identifier of your device and use that instead of /dev/ttyACM0 . Then, your docker-compose.yml might look something like this: devices: #- /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter #- /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well - \"/dev/serial/by-id/usb-Texas_Instruments_TI_CC2531_USB_CDC___xxx:/dev/ttyACM0\" First startup with other adapters \u00b6 Similar principles apply if you use other adapters. You must work out how the adapter presents itself on your Raspberry Pi and then map it to /dev/ttyACM0 inside the container (ie the common right hand side of every device definition). Configuration file \u00b6 Active configuration file \u00b6 Under IOTstack, the active configuration file for Zigbee2MQTT appears at the following path: ~/IOTstack/volumes/zigbee2mqtt/data/configuration.yaml After you make any changes to the configuration file (using sudo ), you need to inform the running container by: $ cd ~/IOTstack $ docker-compose restart zigbee2mqtt Default configuration file \u00b6 The IOTstack version of Zigbee2MQTT is built using a Dockerfile located at: ~/IOTstack/.templates/zigbee2mqtt/Dockerfile The Dockerfile downloads the base koenkk/zigbee2mqtt image from DockerHub and then alters the default configuration file as it builds a local image to: change the default MQTT server URL from \"mqtt://localhost\" to \"mqtt://mosquitto\"; and activate the Zigbee2MQTT web interface on port 8080. Those changes are intended to help new IOTstack installations get started with a minimum of fuss. However, the default configuration file will only become the active configuration file in two situations: On a first install of Zigbee2MQTT; or If you erase the container's persistent storage area. For example: $ cd ~/IOTstack $ docker-compose stop zigbee2mqtt $ docker-compose rm -f zigbee2mqtt $ sudo rm -rf ./volumes/zigbee2mqtt $ docker-compose up -d zigbee2mqtt In either of those situations, the active configuration file will be initialised by copying the default configuration file into place as the container comes up. If you have an existing configuration file \u00b6 If you have an existing active Zigbee2MQTT configuration file, you may need to make two changes: Alter the Mosquitto URL: before: server: 'mqtt://localhost' - after: server: 'mqtt://mosquitto' Enable the web interface (if necessary): append: frontend: port: 8080 Checking that the container is working \u00b6 Checking status \u00b6 $ docker ps --format \"table {{.Names}}\\t{{.RunningFor}}\\t{{.Status}}\" --filter name=\"zigbee2mqtt\" NAMES CREATED STATUS zigbee2mqtt 2 hours ago Up 2 hours You are looking for signs that the container is restarting (ie the \"Status\" column only ever shows a low number of seconds). Checking the log \u00b6 $ docker logs zigbee2mqtt You are looking for evidence of malfunction. Checking that Zigbee2MQTT is able to communicate with Mosquitto \u00b6 If you have the Mosquitto clients installed ( sudo apt install -y mosquitto-clients ), you can run the following command: $ mosquitto_sub -v -h \"localhost\" -t \"zigbee2mqtt/#\" -F \"%I %t %p\" One of two things will happen: silence, indicating that Zigbee2MQTT is not able to communicate with Mosquitto. chatter, proving that Zigbee2MQTT can communicate with Mosquitto. Terminate the mosquitto_sub command with a Control-C. Checking that the Zigbee2MQTT web GUI is working \u00b6 Open a browser, and point it to port 8080 on your Raspberry Pi. You should see the Zigbee2MQTT interface. terminal access inside the container \u00b6 To access the terminal run: $ docker exec -it zigbee2mqtt ash ash is not a typo! When you want to leave the container, either type exit and press return, or press Control-D. Setting a password for the web interface \u00b6 By default, the web interface is unprotected. If you want to set a password: Use sudo to edit the active configuration file at the path: ~/IOTstack/volumes/zigbee2mqtt/data/configuration.yaml Find the following text: frontend: port: 8080 # auth_token: PASSWORD Uncomment the auth_token line and replace \"PASSWORD\" with the password of your choice. For example, to set the password to \"mypassword\": auth_token: mypassword Note: although the name auth_token suggests something more complex, it really is no more than a simple en-clear password. If this concerns you, consider disabling the web front-end entirely, like this: #frontend: # port: 8080 # auth_token: PASSWORD Save the file and restart the container: $ cd ~/IOTstack $ docker-compose restart zigbee2mqtt Container maintenance \u00b6 Because the Zigbee2MQTT container is built from a Dockerfile, a normal pull command will not automatically download any updates released on DockerHub. When you become aware of a new version of Zigbee2MQTT being released on DockerHub, do the following: $ cd ~IOTstack $ docker-compose build --no-cache --pull zigbee2mqtt $ docker-compose up -d zigbee2mqtt $ docker system prune Note: Sometimes it is necessary to repeat the docker system prune command but it depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"Zigbee2MQTT"},{"location":"Containers/Zigbee2MQTT/#zigbee2mqtt","text":"Web Guide Flashing the CC2531 Figuring-out your device identifier","title":"Zigbee2MQTT"},{"location":"Containers/Zigbee2MQTT/#service-definition-change-april-2021","text":"The IOTstack service definition for Zigbee2MQTT is at the following path: ~/IOTstack/.templates/zigbee2mqtt/service.yml As of April 2021, the service definition changed: The Zigbee2MQTT container no longer runs in host mode. Adds timezone support. Builds the container from a Dockerfile providing appropriate defaults for IOTstack. Re-adds a port mapping for port 8080 (the Zigbee2MQTT web UI). If you were running the Zigbee2MQTT service before this change, you may wish to compare and contrast your active service definition (in docker-compose.yml ) with the revised template. Note: You may need to git pull to update your local copy of the IOTstack repository against GitHub.","title":"Service definition change - April 2021"},{"location":"Containers/Zigbee2MQTT/#first-startup-with-cc2531-adapter","text":"The service definition includes: devices: - /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter #- /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well The default device ( /dev/ttyAMA0 ) probably will not work for any Zigbee adapter. It is only there because /dev/ttyAMA0 exists on Raspberry Pis. Its presence permits the container to come up even though it will not actually be able to connect to an adapter. If you have a CC2531, properly flashed and connected to a USB port, you should be able to see it: $ ls -l /dev/ttyACM0 crw-rw---- 1 root dialout 166, 0 Apr 7 09:38 /dev/ttyACM0 If you see the error \"No such file or directory\", you will need to first figure out why your device is not visible. Assuming your CC2531 is visible: Change the device mapping in docker-compose.yml to deactivate ttyAMA0 in favour of activating ttyACM0 : devices: #- /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter - /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well Bring up the container: $ cd ~/IOTstack $ docker-compose up -d zigbee2mqtt You can also follow the instructions in the Zigbee2MQTT documentation to work out the identifier of your device and use that instead of /dev/ttyACM0 . Then, your docker-compose.yml might look something like this: devices: #- /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter #- /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well - \"/dev/serial/by-id/usb-Texas_Instruments_TI_CC2531_USB_CDC___xxx:/dev/ttyACM0\"","title":"First startup with CC2531 adapter"},{"location":"Containers/Zigbee2MQTT/#first-startup-with-other-adapters","text":"Similar principles apply if you use other adapters. You must work out how the adapter presents itself on your Raspberry Pi and then map it to /dev/ttyACM0 inside the container (ie the common right hand side of every device definition).","title":"First startup with other adapters"},{"location":"Containers/Zigbee2MQTT/#configuration-file","text":"","title":"Configuration file"},{"location":"Containers/Zigbee2MQTT/#active-configuration-file","text":"Under IOTstack, the active configuration file for Zigbee2MQTT appears at the following path: ~/IOTstack/volumes/zigbee2mqtt/data/configuration.yaml After you make any changes to the configuration file (using sudo ), you need to inform the running container by: $ cd ~/IOTstack $ docker-compose restart zigbee2mqtt","title":"Active configuration file"},{"location":"Containers/Zigbee2MQTT/#default-configuration-file","text":"The IOTstack version of Zigbee2MQTT is built using a Dockerfile located at: ~/IOTstack/.templates/zigbee2mqtt/Dockerfile The Dockerfile downloads the base koenkk/zigbee2mqtt image from DockerHub and then alters the default configuration file as it builds a local image to: change the default MQTT server URL from \"mqtt://localhost\" to \"mqtt://mosquitto\"; and activate the Zigbee2MQTT web interface on port 8080. Those changes are intended to help new IOTstack installations get started with a minimum of fuss. However, the default configuration file will only become the active configuration file in two situations: On a first install of Zigbee2MQTT; or If you erase the container's persistent storage area. For example: $ cd ~/IOTstack $ docker-compose stop zigbee2mqtt $ docker-compose rm -f zigbee2mqtt $ sudo rm -rf ./volumes/zigbee2mqtt $ docker-compose up -d zigbee2mqtt In either of those situations, the active configuration file will be initialised by copying the default configuration file into place as the container comes up.","title":"Default configuration file"},{"location":"Containers/Zigbee2MQTT/#if-you-have-an-existing-configuration-file","text":"If you have an existing active Zigbee2MQTT configuration file, you may need to make two changes: Alter the Mosquitto URL: before: server: 'mqtt://localhost' - after: server: 'mqtt://mosquitto' Enable the web interface (if necessary): append: frontend: port: 8080","title":"If you have an existing configuration file"},{"location":"Containers/Zigbee2MQTT/#checking-that-the-container-is-working","text":"","title":"Checking that the container is working"},{"location":"Containers/Zigbee2MQTT/#checking-status","text":"$ docker ps --format \"table {{.Names}}\\t{{.RunningFor}}\\t{{.Status}}\" --filter name=\"zigbee2mqtt\" NAMES CREATED STATUS zigbee2mqtt 2 hours ago Up 2 hours You are looking for signs that the container is restarting (ie the \"Status\" column only ever shows a low number of seconds).","title":"Checking status"},{"location":"Containers/Zigbee2MQTT/#checking-the-log","text":"$ docker logs zigbee2mqtt You are looking for evidence of malfunction.","title":"Checking the log"},{"location":"Containers/Zigbee2MQTT/#checking-that-zigbee2mqtt-is-able-to-communicate-with-mosquitto","text":"If you have the Mosquitto clients installed ( sudo apt install -y mosquitto-clients ), you can run the following command: $ mosquitto_sub -v -h \"localhost\" -t \"zigbee2mqtt/#\" -F \"%I %t %p\" One of two things will happen: silence, indicating that Zigbee2MQTT is not able to communicate with Mosquitto. chatter, proving that Zigbee2MQTT can communicate with Mosquitto. Terminate the mosquitto_sub command with a Control-C.","title":"Checking that Zigbee2MQTT is able to communicate with Mosquitto"},{"location":"Containers/Zigbee2MQTT/#checking-that-the-zigbee2mqtt-web-gui-is-working","text":"Open a browser, and point it to port 8080 on your Raspberry Pi. You should see the Zigbee2MQTT interface.","title":"Checking that the Zigbee2MQTT web GUI is working"},{"location":"Containers/Zigbee2MQTT/#terminal-access-inside-the-container","text":"To access the terminal run: $ docker exec -it zigbee2mqtt ash ash is not a typo! When you want to leave the container, either type exit and press return, or press Control-D.","title":"terminal access inside the container"},{"location":"Containers/Zigbee2MQTT/#setting-a-password-for-the-web-interface","text":"By default, the web interface is unprotected. If you want to set a password: Use sudo to edit the active configuration file at the path: ~/IOTstack/volumes/zigbee2mqtt/data/configuration.yaml Find the following text: frontend: port: 8080 # auth_token: PASSWORD Uncomment the auth_token line and replace \"PASSWORD\" with the password of your choice. For example, to set the password to \"mypassword\": auth_token: mypassword Note: although the name auth_token suggests something more complex, it really is no more than a simple en-clear password. If this concerns you, consider disabling the web front-end entirely, like this: #frontend: # port: 8080 # auth_token: PASSWORD Save the file and restart the container: $ cd ~/IOTstack $ docker-compose restart zigbee2mqtt","title":"Setting a password for the web interface"},{"location":"Containers/Zigbee2MQTT/#container-maintenance","text":"Because the Zigbee2MQTT container is built from a Dockerfile, a normal pull command will not automatically download any updates released on DockerHub. When you become aware of a new version of Zigbee2MQTT being released on DockerHub, do the following: $ cd ~IOTstack $ docker-compose build --no-cache --pull zigbee2mqtt $ docker-compose up -d zigbee2mqtt $ docker system prune Note: Sometimes it is necessary to repeat the docker system prune command but it depends on the version of docker-compose you are using and how your version of docker-compose builds local images.","title":"Container maintenance"},{"location":"Containers/Zigbee2mqttassistant/","text":"Zigbee2Mqtt Assistant \u00b6 References \u00b6 Docker Website About \u00b6 This service a web frontend which displays Zigbee2Mqtt service messages and able to control it over MQTT. For the servie a working MQTT server is required and that have to be configured. Environment Parameters \u00b6 Z2MA_SETTINGS__MQTTSERVER=mosquitto - The MQTT service instance which is used by Zigbee2Mqtt instance. Here, \"mosquitto\" is the name of the container. Z2MA_SETTINGS__MQTTUSERNAME=name - Used if your MQTT service has authentication enabled. Optional. Z2MA_SETTINGS__MQTTPASSWORD=password - Used if your MQTT service has authentication enabled. Optional. TZ=Etc/UTC - Set to your timezone. Optional but recommended. Accessing the UI \u00b6 The Zigbee2Mqtt Assistant UI is available using port 8880. For example: http://your.local.ip.address:8880/","title":"Zigbee2Mqtt Assistant"},{"location":"Containers/Zigbee2mqttassistant/#zigbee2mqtt-assistant","text":"","title":"Zigbee2Mqtt Assistant"},{"location":"Containers/Zigbee2mqttassistant/#references","text":"Docker Website","title":"References"},{"location":"Containers/Zigbee2mqttassistant/#about","text":"This service a web frontend which displays Zigbee2Mqtt service messages and able to control it over MQTT. For the servie a working MQTT server is required and that have to be configured.","title":"About"},{"location":"Containers/Zigbee2mqttassistant/#environment-parameters","text":"Z2MA_SETTINGS__MQTTSERVER=mosquitto - The MQTT service instance which is used by Zigbee2Mqtt instance. Here, \"mosquitto\" is the name of the container. Z2MA_SETTINGS__MQTTUSERNAME=name - Used if your MQTT service has authentication enabled. Optional. Z2MA_SETTINGS__MQTTPASSWORD=password - Used if your MQTT service has authentication enabled. Optional. TZ=Etc/UTC - Set to your timezone. Optional but recommended.","title":"Environment Parameters"},{"location":"Containers/Zigbee2mqttassistant/#accessing-the-ui","text":"The Zigbee2Mqtt Assistant UI is available using port 8880. For example: http://your.local.ip.address:8880/","title":"Accessing the UI"},{"location":"Developers/","text":"Contributing \u00b6 Writing documentation \u00b6 Documentation is is written as markdown, processed using mkdocs ( docs ) and the Material theme ( docs ). The Material theme is not just styling, but provides additional syntax extensions. Setup your system for Material: pip3 install mkdocs-material pip3 install mkdocs-git-revision-date-localized-plugin To test your local changes while writing them and before making a pull-request: cd ~/IOTstack mkdocs serve Creating a new service \u00b6 In this section you can find information on how to contribute a service to IOTstack. We are generally very accepting of new services where they are useful. Keep in mind that if it is not IOTstack, selfhosted, or automation related we may not approve the PR. Services will grow over time, we may split up the buildstack menu into subsections or create filters to make organising all the services we provide easier to find. Checks \u00b6 service.yml file is correct. To check that it parses correctly and doesn't have port conflicts with other services by running: ./scripts/template.py --check build.py file is correct Service allows for changing external WUI port from Build Stack's options menu if service uses a HTTP/S port Use a default password, or allow the user to generate a random password for the service for initial installation. If the service asks to setup an account this can be ignored. Ensure Default Configs is updated with WUI port and username/password. Must detect port confilicts with other services on BuildStack Menu. Pre and Post hooks work with no errors. Does not require user to edit config files in order to get the service running. Ensure that your service can be backed up and restored without errors or data loss. Any configs that are required before getting the service running should be configured in the service's options menu (and a BuildStack menu Issue should be displayed if not). Fork the repo and push the changes to your fork. Create a cross repo PR for the mods to review. We may request additional changes from you. Follow up \u00b6 If your new service is approved and merged then congratulations! Please watch the Issues page on github over the next few days and weeks to see if any users have questions or issues with your new service. Python development \u00b6 Running python source code tests: $ pip install -U --user pytest $ python -m pytest Static Python type checking: $ pip install -U --user mypy $ mypy scripts Links: Default configs Password configuration for Services Build Stack Menu System Coding a new service IOTstack issues","title":"Contributing"},{"location":"Developers/#contributing","text":"","title":"Contributing"},{"location":"Developers/#writing-documentation","text":"Documentation is is written as markdown, processed using mkdocs ( docs ) and the Material theme ( docs ). The Material theme is not just styling, but provides additional syntax extensions. Setup your system for Material: pip3 install mkdocs-material pip3 install mkdocs-git-revision-date-localized-plugin To test your local changes while writing them and before making a pull-request: cd ~/IOTstack mkdocs serve","title":"Writing documentation"},{"location":"Developers/#creating-a-new-service","text":"In this section you can find information on how to contribute a service to IOTstack. We are generally very accepting of new services where they are useful. Keep in mind that if it is not IOTstack, selfhosted, or automation related we may not approve the PR. Services will grow over time, we may split up the buildstack menu into subsections or create filters to make organising all the services we provide easier to find.","title":"Creating a new service"},{"location":"Developers/#checks","text":"service.yml file is correct. To check that it parses correctly and doesn't have port conflicts with other services by running: ./scripts/template.py --check build.py file is correct Service allows for changing external WUI port from Build Stack's options menu if service uses a HTTP/S port Use a default password, or allow the user to generate a random password for the service for initial installation. If the service asks to setup an account this can be ignored. Ensure Default Configs is updated with WUI port and username/password. Must detect port confilicts with other services on BuildStack Menu. Pre and Post hooks work with no errors. Does not require user to edit config files in order to get the service running. Ensure that your service can be backed up and restored without errors or data loss. Any configs that are required before getting the service running should be configured in the service's options menu (and a BuildStack menu Issue should be displayed if not). Fork the repo and push the changes to your fork. Create a cross repo PR for the mods to review. We may request additional changes from you.","title":"Checks"},{"location":"Developers/#follow-up","text":"If your new service is approved and merged then congratulations! Please watch the Issues page on github over the next few days and weeks to see if any users have questions or issues with your new service.","title":"Follow up"},{"location":"Developers/#python-development","text":"Running python source code tests: $ pip install -U --user pytest $ python -m pytest Static Python type checking: $ pip install -U --user mypy $ mypy scripts Links: Default configs Password configuration for Services Build Stack Menu System Coding a new service IOTstack issues","title":"Python development"},{"location":"Developers/BuildStack-RandomPassword/","text":"Build Stack Random Services Password \u00b6 This page explains how to have a service generate a random password during build time. This will require that your service have a working options menu. Alternatively, a service may just use %randomPassword% in a value without any options menu. This will generate a new random password the first time this service is added. The generated password may be found in the docker-compose.yml . Keep in mind that updating strings in a service's yaml config isn't limited to passwords. A word of caution \u00b6 Many services often set a password on their initial spin up and store it internally. That means if if the password is changed by the menu afterwards, it may not be reflected in the service. By default the password specified in the documentation should be used, unless the user specifically selected to use a randomly generated one. In the future, the feature to specify a password manually may be added in, much like how ports can be customised. A basic example \u00b6 Inside the service's service.yml file, a special string can be added in for the build script to find and replace. Commonly the string is %randomPassword% , but technically any string can be used. The same string can be used multiple times for the same password to be used multiple times, and/or multiple difference strings can be used for multiple passwords. mariadb: image: linuxserver/mariadb container_name: mariadb environment: - MYSQL_ROOT_PASSWORD=%randomAdminPassword% - MYSQL_DATABASE=default - MYSQL_USER=mariadbuser - MYSQL_PASSWORD=%randomPassword% These strings will be updated during the Prebuild Hook stage when building. The code to make this happen is shown below. Code commonly used to update passwords \u00b6 This code can basically be copy-pasted into your service's build.py file. You are welcome to expand upon it if required. It will probably be refactored into a utils function in the future to adear to DRY (Don't Repeat Yourself) practices. def preBuild(): # Multi-service load. Most services only include a single service. The exception being NextCloud where the database information needs to match between NextCloud and MariaDB (as defined in NextCloud's 'service.yml' file, not IOTstack's MariaDB). with open((r'%s/' % serviceTemplate) + servicesFileName) as objServiceFile: serviceYamlTemplate = yaml.load(objServiceFile) oldBuildCache = {} try: with open(r'%s' % buildCache) as objBuildCache: # Load previous build, if it exists oldBuildCache = yaml.load(objBuildCache) except: pass buildCacheServices = {} if \"services\" in oldBuildCache: # If a previous build does exist, load it so that we can reuse the password from it if required. buildCacheServices = oldBuildCache[\"services\"] if not os.path.exists(serviceService): # Create the service directory for the service os.makedirs(serviceService, exist_ok=True) # Check if buildSettings file exists (from previous build), or create one if it doesn't (in the else block). if os.path.exists(buildSettings): # Password randomisation with open(r'%s' % buildSettings) as objBuildSettingsFile: piHoleYamlBuildOptions = yaml.load(objBuildSettingsFile) if ( piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password for this build\" or piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password every build\" or deconzYamlBuildOptions[\"databasePasswordOption\"] == \"Use default password for this build\" ): if deconzYamlBuildOptions[\"databasePasswordOption\"] == \"Use default password for this build\": newAdminPassword = \"######\" # Update to what's specified in your documentation newPassword = \"######\" # Update to what's specified in your documentation else: # Generate our passwords newAdminPassword = generateRandomString() newPassword = generateRandomString() # Here we loop through each service included in the current service's `service.yml` file and update the password strings. for (index, serviceName) in enumerate(serviceYamlTemplate): dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] if \"environment\" in serviceYamlTemplate[serviceName]: for (envIndex, envName) in enumerate(serviceYamlTemplate[serviceName][\"environment\"]): envName = envName.replace(\"%randomPassword%\", newPassword) envName = envName.replace(\"%randomAdminPassword%\", newAdminPassword) dockerComposeServicesYaml[serviceName][\"environment\"][envIndex] = envName # If the user had selected to only update the password once, ensure the build options file is updated. if (piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password for this build\"): piHoleYamlBuildOptions[\"databasePasswordOption\"] = \"Do nothing\" with open(buildSettings, 'w') as outputFile: yaml.dump(piHoleYamlBuildOptions, outputFile) else: # Do nothing - don't change password for (index, serviceName) in enumerate(buildCacheServices): if serviceName in buildCacheServices: # Load service from cache if exists (to maintain password) dockerComposeServicesYaml[serviceName] = buildCacheServices[serviceName] else: dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] # Build options file didn't exist, so create one, and also use default password (default action). else: print(\"PiHole Warning: Build settings file not found, using default password\") time.sleep(1) newAdminPassword = \"######\" # Update to what's specified in your documentation newPassword = \"######\" # Update to what's specified in your documentation for (index, serviceName) in enumerate(serviceYamlTemplate): dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] if \"environment\" in serviceYamlTemplate[serviceName]: for (envIndex, envName) in enumerate(serviceYamlTemplate[serviceName][\"environment\"]): envName = envName.replace(\"%randomPassword%\", newPassword) envName = envName.replace(\"%randomAdminPassword%\", newAdminPassword) dockerComposeServicesYaml[serviceName][\"environment\"][envIndex] = envName piHoleYamlBuildOptions = { \"version\": \"1\", \"application\": \"IOTstack\", \"service\": \"PiHole\", \"comment\": \"PiHole Build Options\" } piHoleYamlBuildOptions[\"databasePasswordOption\"] = \"Do nothing\" with open(buildSettings, 'w') as outputFile: yaml.dump(piHoleYamlBuildOptions, outputFile) return True Code for your service's menu \u00b6 While not needed, since the default action is to create a random password, it is a good idea to allow the user to choose what to do. This can be achieved by giving them access to a password menu. This code can be placed in your service's build.py file, that will show a new menu option, allowing users to select it and be taken to a password settings screen. Remember that you need to have an already working menu, and to place this code into it. import signal ... def setPasswordOptions(): global needsRender global hasRebuiltAddons passwordOptionsMenuFilePath = \"./.templates/{currentService}/passwords.py\".format(currentService=currentServiceName) with open(passwordOptionsMenuFilePath, \"rb\") as pythonDynamicImportFile: code = compile(pythonDynamicImportFile.read(), passwordOptionsMenuFilePath, \"exec\") execGlobals = { \"currentServiceName\": currentServiceName, \"renderMode\": renderMode } execLocals = {} screenActive = False exec(code, execGlobals, execLocals) signal.signal(signal.SIGWINCH, onResize) screenActive = True needsRender = 1 ... def createMenu(): global yourServicesBuildOptions global serviceService yourServicesBuildOptions = [] yourServicesBuildOptions.append([ \"Your Service Password Options\", setPasswordOptions ]) yourServicesBuildOptions.append([\"Go back\", goBack]) Password settings screen \u00b6 The code for the Password settings is lengthy, but it's pasted here for convienence #!/usr/bin/env python3 import signal def main(): from blessed import Terminal from deps.chars import specialChars, commonTopBorder, commonBottomBorder, commonEmptyLine from deps.consts import servicesDirectory, templatesDirectory, buildSettingsFileName import time import subprocess import ruamel.yamls import os global signal global currentServiceName global menuSelectionInProgress global mainMenuList global currentMenuItemIndex global renderMode global paginationSize global paginationStartIndex global hideHelpText yaml = ruamel.yaml.YAML() yaml.preserve_quotes = True try: # If not already set, then set it. hideHelpText = hideHelpText except: hideHelpText = False term = Terminal() hotzoneLocation = [((term.height // 16) + 6), 0] paginationToggle = [10, term.height - 25] paginationStartIndex = 0 paginationSize = paginationToggle[0] serviceService = servicesDirectory + currentServiceName serviceTemplate = templatesDirectory + currentServiceName buildSettings = serviceService + buildSettingsFileName def goBack(): global menuSelectionInProgress global needsRender menuSelectionInProgress = False needsRender = 1 return True mainMenuList = [] hotzoneLocation = [((term.height // 16) + 6), 0] menuSelectionInProgress = True currentMenuItemIndex = 0 menuNavigateDirection = 0 # Render Modes: # 0 = No render needed # 1 = Full render # 2 = Hotzone only needsRender = 1 def onResize(sig, action): global mainMenuList global currentMenuItemIndex mainRender(1, mainMenuList, currentMenuItemIndex) def generateLineText(text, textLength=None, paddingBefore=0, lineLength=64): result = \"\" for i in range(paddingBefore): result += \" \" textPrintableCharactersLength = textLength if (textPrintableCharactersLength) == None: textPrintableCharactersLength = len(text) result += text remainingSpace = lineLength - textPrintableCharactersLength for i in range(remainingSpace): result += \" \" return result def renderHotZone(term, renderType, menu, selection, hotzoneLocation, paddingBefore = 4): global paginationSize selectedTextLength = len(\"-> \") print(term.move(hotzoneLocation[0], hotzoneLocation[1])) if paginationStartIndex >= 1: print(term.center(\"{b} {uaf} {uaf}{uaf}{uaf} {ual} {b}\".format( b=specialChars[renderMode][\"borderVertical\"], uaf=specialChars[renderMode][\"upArrowFull\"], ual=specialChars[renderMode][\"upArrowLine\"] ))) else: print(term.center(commonEmptyLine(renderMode))) for (index, menuItem) in enumerate(menu): # Menu loop if index >= paginationStartIndex and index < paginationStartIndex + paginationSize: lineText = generateLineText(menuItem[0], paddingBefore=paddingBefore) # Menu highlight logic if index == selection: formattedLineText = '-> {t.blue_on_green}{title}{t.normal} <-'.format(t=term, title=menuItem[0]) paddedLineText = generateLineText(formattedLineText, textLength=len(menuItem[0]) + selectedTextLength, paddingBefore=paddingBefore - selectedTextLength) toPrint = paddedLineText else: toPrint = '{title}{t.normal}'.format(t=term, title=lineText) # ##### # Menu check render logic if menuItem[1][\"checked\"]: toPrint = \" (X) \" + toPrint else: toPrint = \" ( ) \" + toPrint toPrint = \"{bv} {toPrint} {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"], toPrint=toPrint) # Generate border toPrint = term.center(toPrint) # Center Text (All lines should have the same amount of printable characters) # ##### print(toPrint) if paginationStartIndex + paginationSize < len(menu): print(term.center(\"{b} {daf} {daf}{daf}{daf} {dal} {b}\".format( b=specialChars[renderMode][\"borderVertical\"], daf=specialChars[renderMode][\"downArrowFull\"], dal=specialChars[renderMode][\"downArrowLine\"] ))) else: print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) def mainRender(needsRender, menu, selection): global paginationStartIndex global paginationSize term = Terminal() if selection >= paginationStartIndex + paginationSize: paginationStartIndex = selection - (paginationSize - 1) + 1 needsRender = 1 if selection <= paginationStartIndex - 1: paginationStartIndex = selection needsRender = 1 if needsRender == 1: print(term.clear()) print(term.move_y(term.height // 16)) print(term.black_on_cornsilk4(term.center('IOTstack YourServices Password Options'))) print(\"\") print(term.center(commonTopBorder(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Select Password Option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) if needsRender >= 1: renderHotZone(term, needsRender, menu, selection, hotzoneLocation) if needsRender == 1: print(term.center(commonEmptyLine(renderMode))) if not hideHelpText: if term.height < 32: print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Not enough vertical room to render controls help text {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) else: print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Controls: {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Space] to select option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Up] and [Down] to move selection cursor {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [H] Show/hide this text {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Enter] to build and save option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Escape] to cancel changes {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonBottomBorder(renderMode))) def runSelection(selection): import types if len(mainMenuList[selection]) > 1 and isinstance(mainMenuList[selection][1], types.FunctionType): mainMenuList[selection][1]() else: print(term.green_reverse('IOTstack Error: No function assigned to menu item: \"{}\"'.format(mainMenuList[selection][0]))) def isMenuItemSelectable(menu, index): if len(menu) > index: if len(menu[index]) > 1: if \"skip\" in menu[index][1] and menu[index][1][\"skip\"] == True: return False return True def loadOptionsMenu(): global mainMenuList mainMenuList.append([\"Use default password for this build\", { \"checked\": True }]) mainMenuList.append([\"Randomise database password for this build\", { \"checked\": False }]) mainMenuList.append([\"Randomise database password every build\", { \"checked\": False }]) mainMenuList.append([\"Do nothing\", { \"checked\": False }]) def checkMenuItem(selection): global mainMenuList for (index, menuItem) in enumerate(mainMenuList): mainMenuList[index][1][\"checked\"] = False mainMenuList[selection][1][\"checked\"] = True def saveOptions(): try: if not os.path.exists(serviceService): os.makedirs(serviceService, exist_ok=True) if os.path.exists(buildSettings): with open(r'%s' % buildSettings) as objBuildSettingsFile: yourServicesYamlBuildOptions = yaml.load(objBuildSettingsFile) else: yourServices = { \"version\": \"1\", \"application\": \"IOTstack\", \"service\": \"Your Service\", \"comment\": \"Your Service Build Options\" } yourServices[\"databasePasswordOption\"] = \"\" for (index, menuOption) in enumerate(mainMenuList): if menuOption[1][\"checked\"]: yourServices[\"databasePasswordOption\"] = menuOption[0] break with open(buildSettings, 'w') as outputFile: yaml.dump(yourServices, outputFile) except Exception as err: print(\"Error saving Your Services Password options\", currentServiceName) print(err) return False global hasRebuiltHardwareSelection hasRebuiltHardwareSelection = True return True def loadOptions(): try: if not os.path.exists(serviceService): os.makedirs(serviceService, exist_ok=True) if os.path.exists(buildSettings): with open(r'%s' % buildSettings) as objBuildSettingsFile: yourServicesYamlBuildOptions = yaml.load(objBuildSettingsFile) for (index, menuOption) in enumerate(mainMenuList): if menuOption[0] == yourServicesYamlBuildOptions[\"databasePasswordOption\"]: checkMenuItem(index) break except Exception as err: print(\"Error loading Your Services Password options\", currentServiceName) print(err) return False return True if __name__ == 'builtins': global signal term = Terminal() signal.signal(signal.SIGWINCH, onResize) loadOptionsMenu() loadOptions() with term.fullscreen(): menuNavigateDirection = 0 mainRender(needsRender, mainMenuList, currentMenuItemIndex) menuSelectionInProgress = True with term.cbreak(): while menuSelectionInProgress: menuNavigateDirection = 0 if not needsRender == 0: # Only rerender when changed to prevent flickering mainRender(needsRender, mainMenuList, currentMenuItemIndex) needsRender = 0 key = term.inkey(esc_delay=0.05) if key.is_sequence: if key.name == 'KEY_TAB': if paginationSize == paginationToggle[0]: paginationSize = paginationToggle[1] else: paginationSize = paginationToggle[0] mainRender(1, mainMenuList, currentMenuItemIndex) if key.name == 'KEY_DOWN': menuNavigateDirection += 1 if key.name == 'KEY_UP': menuNavigateDirection -= 1 if key.name == 'KEY_ENTER': if saveOptions(): return True else: print(\"Something went wrong. Try saving the list again.\") if key.name == 'KEY_ESCAPE': menuSelectionInProgress = False return True elif key: if key == ' ': # Space pressed checkMenuItem(currentMenuItemIndex) # Update checked list needsRender = 2 elif key == 'h': # H pressed if hideHelpText: hideHelpText = False else: hideHelpText = True mainRender(1, mainMenuList, currentMenuItemIndex) if menuNavigateDirection != 0: # If a direction was pressed, find next selectable item currentMenuItemIndex += menuNavigateDirection currentMenuItemIndex = currentMenuItemIndex % len(mainMenuList) needsRender = 2 while not isMenuItemSelectable(mainMenuList, currentMenuItemIndex): currentMenuItemIndex += menuNavigateDirection currentMenuItemIndex = currentMenuItemIndex % len(mainMenuList) return True return True originalSignalHandler = signal.getsignal(signal.SIGINT) main() signal.signal(signal.SIGWINCH, originalSignalHandler)","title":"Build Stack Random Services Password"},{"location":"Developers/BuildStack-RandomPassword/#build-stack-random-services-password","text":"This page explains how to have a service generate a random password during build time. This will require that your service have a working options menu. Alternatively, a service may just use %randomPassword% in a value without any options menu. This will generate a new random password the first time this service is added. The generated password may be found in the docker-compose.yml . Keep in mind that updating strings in a service's yaml config isn't limited to passwords.","title":"Build Stack Random Services Password"},{"location":"Developers/BuildStack-RandomPassword/#a-word-of-caution","text":"Many services often set a password on their initial spin up and store it internally. That means if if the password is changed by the menu afterwards, it may not be reflected in the service. By default the password specified in the documentation should be used, unless the user specifically selected to use a randomly generated one. In the future, the feature to specify a password manually may be added in, much like how ports can be customised.","title":"A word of caution"},{"location":"Developers/BuildStack-RandomPassword/#a-basic-example","text":"Inside the service's service.yml file, a special string can be added in for the build script to find and replace. Commonly the string is %randomPassword% , but technically any string can be used. The same string can be used multiple times for the same password to be used multiple times, and/or multiple difference strings can be used for multiple passwords. mariadb: image: linuxserver/mariadb container_name: mariadb environment: - MYSQL_ROOT_PASSWORD=%randomAdminPassword% - MYSQL_DATABASE=default - MYSQL_USER=mariadbuser - MYSQL_PASSWORD=%randomPassword% These strings will be updated during the Prebuild Hook stage when building. The code to make this happen is shown below.","title":"A basic example"},{"location":"Developers/BuildStack-RandomPassword/#code-commonly-used-to-update-passwords","text":"This code can basically be copy-pasted into your service's build.py file. You are welcome to expand upon it if required. It will probably be refactored into a utils function in the future to adear to DRY (Don't Repeat Yourself) practices. def preBuild(): # Multi-service load. Most services only include a single service. The exception being NextCloud where the database information needs to match between NextCloud and MariaDB (as defined in NextCloud's 'service.yml' file, not IOTstack's MariaDB). with open((r'%s/' % serviceTemplate) + servicesFileName) as objServiceFile: serviceYamlTemplate = yaml.load(objServiceFile) oldBuildCache = {} try: with open(r'%s' % buildCache) as objBuildCache: # Load previous build, if it exists oldBuildCache = yaml.load(objBuildCache) except: pass buildCacheServices = {} if \"services\" in oldBuildCache: # If a previous build does exist, load it so that we can reuse the password from it if required. buildCacheServices = oldBuildCache[\"services\"] if not os.path.exists(serviceService): # Create the service directory for the service os.makedirs(serviceService, exist_ok=True) # Check if buildSettings file exists (from previous build), or create one if it doesn't (in the else block). if os.path.exists(buildSettings): # Password randomisation with open(r'%s' % buildSettings) as objBuildSettingsFile: piHoleYamlBuildOptions = yaml.load(objBuildSettingsFile) if ( piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password for this build\" or piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password every build\" or deconzYamlBuildOptions[\"databasePasswordOption\"] == \"Use default password for this build\" ): if deconzYamlBuildOptions[\"databasePasswordOption\"] == \"Use default password for this build\": newAdminPassword = \"######\" # Update to what's specified in your documentation newPassword = \"######\" # Update to what's specified in your documentation else: # Generate our passwords newAdminPassword = generateRandomString() newPassword = generateRandomString() # Here we loop through each service included in the current service's `service.yml` file and update the password strings. for (index, serviceName) in enumerate(serviceYamlTemplate): dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] if \"environment\" in serviceYamlTemplate[serviceName]: for (envIndex, envName) in enumerate(serviceYamlTemplate[serviceName][\"environment\"]): envName = envName.replace(\"%randomPassword%\", newPassword) envName = envName.replace(\"%randomAdminPassword%\", newAdminPassword) dockerComposeServicesYaml[serviceName][\"environment\"][envIndex] = envName # If the user had selected to only update the password once, ensure the build options file is updated. if (piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password for this build\"): piHoleYamlBuildOptions[\"databasePasswordOption\"] = \"Do nothing\" with open(buildSettings, 'w') as outputFile: yaml.dump(piHoleYamlBuildOptions, outputFile) else: # Do nothing - don't change password for (index, serviceName) in enumerate(buildCacheServices): if serviceName in buildCacheServices: # Load service from cache if exists (to maintain password) dockerComposeServicesYaml[serviceName] = buildCacheServices[serviceName] else: dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] # Build options file didn't exist, so create one, and also use default password (default action). else: print(\"PiHole Warning: Build settings file not found, using default password\") time.sleep(1) newAdminPassword = \"######\" # Update to what's specified in your documentation newPassword = \"######\" # Update to what's specified in your documentation for (index, serviceName) in enumerate(serviceYamlTemplate): dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] if \"environment\" in serviceYamlTemplate[serviceName]: for (envIndex, envName) in enumerate(serviceYamlTemplate[serviceName][\"environment\"]): envName = envName.replace(\"%randomPassword%\", newPassword) envName = envName.replace(\"%randomAdminPassword%\", newAdminPassword) dockerComposeServicesYaml[serviceName][\"environment\"][envIndex] = envName piHoleYamlBuildOptions = { \"version\": \"1\", \"application\": \"IOTstack\", \"service\": \"PiHole\", \"comment\": \"PiHole Build Options\" } piHoleYamlBuildOptions[\"databasePasswordOption\"] = \"Do nothing\" with open(buildSettings, 'w') as outputFile: yaml.dump(piHoleYamlBuildOptions, outputFile) return True","title":"Code commonly used to update passwords"},{"location":"Developers/BuildStack-RandomPassword/#code-for-your-services-menu","text":"While not needed, since the default action is to create a random password, it is a good idea to allow the user to choose what to do. This can be achieved by giving them access to a password menu. This code can be placed in your service's build.py file, that will show a new menu option, allowing users to select it and be taken to a password settings screen. Remember that you need to have an already working menu, and to place this code into it. import signal ... def setPasswordOptions(): global needsRender global hasRebuiltAddons passwordOptionsMenuFilePath = \"./.templates/{currentService}/passwords.py\".format(currentService=currentServiceName) with open(passwordOptionsMenuFilePath, \"rb\") as pythonDynamicImportFile: code = compile(pythonDynamicImportFile.read(), passwordOptionsMenuFilePath, \"exec\") execGlobals = { \"currentServiceName\": currentServiceName, \"renderMode\": renderMode } execLocals = {} screenActive = False exec(code, execGlobals, execLocals) signal.signal(signal.SIGWINCH, onResize) screenActive = True needsRender = 1 ... def createMenu(): global yourServicesBuildOptions global serviceService yourServicesBuildOptions = [] yourServicesBuildOptions.append([ \"Your Service Password Options\", setPasswordOptions ]) yourServicesBuildOptions.append([\"Go back\", goBack])","title":"Code for your service's menu"},{"location":"Developers/BuildStack-RandomPassword/#password-settings-screen","text":"The code for the Password settings is lengthy, but it's pasted here for convienence #!/usr/bin/env python3 import signal def main(): from blessed import Terminal from deps.chars import specialChars, commonTopBorder, commonBottomBorder, commonEmptyLine from deps.consts import servicesDirectory, templatesDirectory, buildSettingsFileName import time import subprocess import ruamel.yamls import os global signal global currentServiceName global menuSelectionInProgress global mainMenuList global currentMenuItemIndex global renderMode global paginationSize global paginationStartIndex global hideHelpText yaml = ruamel.yaml.YAML() yaml.preserve_quotes = True try: # If not already set, then set it. hideHelpText = hideHelpText except: hideHelpText = False term = Terminal() hotzoneLocation = [((term.height // 16) + 6), 0] paginationToggle = [10, term.height - 25] paginationStartIndex = 0 paginationSize = paginationToggle[0] serviceService = servicesDirectory + currentServiceName serviceTemplate = templatesDirectory + currentServiceName buildSettings = serviceService + buildSettingsFileName def goBack(): global menuSelectionInProgress global needsRender menuSelectionInProgress = False needsRender = 1 return True mainMenuList = [] hotzoneLocation = [((term.height // 16) + 6), 0] menuSelectionInProgress = True currentMenuItemIndex = 0 menuNavigateDirection = 0 # Render Modes: # 0 = No render needed # 1 = Full render # 2 = Hotzone only needsRender = 1 def onResize(sig, action): global mainMenuList global currentMenuItemIndex mainRender(1, mainMenuList, currentMenuItemIndex) def generateLineText(text, textLength=None, paddingBefore=0, lineLength=64): result = \"\" for i in range(paddingBefore): result += \" \" textPrintableCharactersLength = textLength if (textPrintableCharactersLength) == None: textPrintableCharactersLength = len(text) result += text remainingSpace = lineLength - textPrintableCharactersLength for i in range(remainingSpace): result += \" \" return result def renderHotZone(term, renderType, menu, selection, hotzoneLocation, paddingBefore = 4): global paginationSize selectedTextLength = len(\"-> \") print(term.move(hotzoneLocation[0], hotzoneLocation[1])) if paginationStartIndex >= 1: print(term.center(\"{b} {uaf} {uaf}{uaf}{uaf} {ual} {b}\".format( b=specialChars[renderMode][\"borderVertical\"], uaf=specialChars[renderMode][\"upArrowFull\"], ual=specialChars[renderMode][\"upArrowLine\"] ))) else: print(term.center(commonEmptyLine(renderMode))) for (index, menuItem) in enumerate(menu): # Menu loop if index >= paginationStartIndex and index < paginationStartIndex + paginationSize: lineText = generateLineText(menuItem[0], paddingBefore=paddingBefore) # Menu highlight logic if index == selection: formattedLineText = '-> {t.blue_on_green}{title}{t.normal} <-'.format(t=term, title=menuItem[0]) paddedLineText = generateLineText(formattedLineText, textLength=len(menuItem[0]) + selectedTextLength, paddingBefore=paddingBefore - selectedTextLength) toPrint = paddedLineText else: toPrint = '{title}{t.normal}'.format(t=term, title=lineText) # ##### # Menu check render logic if menuItem[1][\"checked\"]: toPrint = \" (X) \" + toPrint else: toPrint = \" ( ) \" + toPrint toPrint = \"{bv} {toPrint} {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"], toPrint=toPrint) # Generate border toPrint = term.center(toPrint) # Center Text (All lines should have the same amount of printable characters) # ##### print(toPrint) if paginationStartIndex + paginationSize < len(menu): print(term.center(\"{b} {daf} {daf}{daf}{daf} {dal} {b}\".format( b=specialChars[renderMode][\"borderVertical\"], daf=specialChars[renderMode][\"downArrowFull\"], dal=specialChars[renderMode][\"downArrowLine\"] ))) else: print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) def mainRender(needsRender, menu, selection): global paginationStartIndex global paginationSize term = Terminal() if selection >= paginationStartIndex + paginationSize: paginationStartIndex = selection - (paginationSize - 1) + 1 needsRender = 1 if selection <= paginationStartIndex - 1: paginationStartIndex = selection needsRender = 1 if needsRender == 1: print(term.clear()) print(term.move_y(term.height // 16)) print(term.black_on_cornsilk4(term.center('IOTstack YourServices Password Options'))) print(\"\") print(term.center(commonTopBorder(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Select Password Option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) if needsRender >= 1: renderHotZone(term, needsRender, menu, selection, hotzoneLocation) if needsRender == 1: print(term.center(commonEmptyLine(renderMode))) if not hideHelpText: if term.height < 32: print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Not enough vertical room to render controls help text {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) else: print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Controls: {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Space] to select option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Up] and [Down] to move selection cursor {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [H] Show/hide this text {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Enter] to build and save option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Escape] to cancel changes {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonBottomBorder(renderMode))) def runSelection(selection): import types if len(mainMenuList[selection]) > 1 and isinstance(mainMenuList[selection][1], types.FunctionType): mainMenuList[selection][1]() else: print(term.green_reverse('IOTstack Error: No function assigned to menu item: \"{}\"'.format(mainMenuList[selection][0]))) def isMenuItemSelectable(menu, index): if len(menu) > index: if len(menu[index]) > 1: if \"skip\" in menu[index][1] and menu[index][1][\"skip\"] == True: return False return True def loadOptionsMenu(): global mainMenuList mainMenuList.append([\"Use default password for this build\", { \"checked\": True }]) mainMenuList.append([\"Randomise database password for this build\", { \"checked\": False }]) mainMenuList.append([\"Randomise database password every build\", { \"checked\": False }]) mainMenuList.append([\"Do nothing\", { \"checked\": False }]) def checkMenuItem(selection): global mainMenuList for (index, menuItem) in enumerate(mainMenuList): mainMenuList[index][1][\"checked\"] = False mainMenuList[selection][1][\"checked\"] = True def saveOptions(): try: if not os.path.exists(serviceService): os.makedirs(serviceService, exist_ok=True) if os.path.exists(buildSettings): with open(r'%s' % buildSettings) as objBuildSettingsFile: yourServicesYamlBuildOptions = yaml.load(objBuildSettingsFile) else: yourServices = { \"version\": \"1\", \"application\": \"IOTstack\", \"service\": \"Your Service\", \"comment\": \"Your Service Build Options\" } yourServices[\"databasePasswordOption\"] = \"\" for (index, menuOption) in enumerate(mainMenuList): if menuOption[1][\"checked\"]: yourServices[\"databasePasswordOption\"] = menuOption[0] break with open(buildSettings, 'w') as outputFile: yaml.dump(yourServices, outputFile) except Exception as err: print(\"Error saving Your Services Password options\", currentServiceName) print(err) return False global hasRebuiltHardwareSelection hasRebuiltHardwareSelection = True return True def loadOptions(): try: if not os.path.exists(serviceService): os.makedirs(serviceService, exist_ok=True) if os.path.exists(buildSettings): with open(r'%s' % buildSettings) as objBuildSettingsFile: yourServicesYamlBuildOptions = yaml.load(objBuildSettingsFile) for (index, menuOption) in enumerate(mainMenuList): if menuOption[0] == yourServicesYamlBuildOptions[\"databasePasswordOption\"]: checkMenuItem(index) break except Exception as err: print(\"Error loading Your Services Password options\", currentServiceName) print(err) return False return True if __name__ == 'builtins': global signal term = Terminal() signal.signal(signal.SIGWINCH, onResize) loadOptionsMenu() loadOptions() with term.fullscreen(): menuNavigateDirection = 0 mainRender(needsRender, mainMenuList, currentMenuItemIndex) menuSelectionInProgress = True with term.cbreak(): while menuSelectionInProgress: menuNavigateDirection = 0 if not needsRender == 0: # Only rerender when changed to prevent flickering mainRender(needsRender, mainMenuList, currentMenuItemIndex) needsRender = 0 key = term.inkey(esc_delay=0.05) if key.is_sequence: if key.name == 'KEY_TAB': if paginationSize == paginationToggle[0]: paginationSize = paginationToggle[1] else: paginationSize = paginationToggle[0] mainRender(1, mainMenuList, currentMenuItemIndex) if key.name == 'KEY_DOWN': menuNavigateDirection += 1 if key.name == 'KEY_UP': menuNavigateDirection -= 1 if key.name == 'KEY_ENTER': if saveOptions(): return True else: print(\"Something went wrong. Try saving the list again.\") if key.name == 'KEY_ESCAPE': menuSelectionInProgress = False return True elif key: if key == ' ': # Space pressed checkMenuItem(currentMenuItemIndex) # Update checked list needsRender = 2 elif key == 'h': # H pressed if hideHelpText: hideHelpText = False else: hideHelpText = True mainRender(1, mainMenuList, currentMenuItemIndex) if menuNavigateDirection != 0: # If a direction was pressed, find next selectable item currentMenuItemIndex += menuNavigateDirection currentMenuItemIndex = currentMenuItemIndex % len(mainMenuList) needsRender = 2 while not isMenuItemSelectable(mainMenuList, currentMenuItemIndex): currentMenuItemIndex += menuNavigateDirection currentMenuItemIndex = currentMenuItemIndex % len(mainMenuList) return True return True originalSignalHandler = signal.getsignal(signal.SIGINT) main() signal.signal(signal.SIGWINCH, originalSignalHandler)","title":"Password settings screen"},{"location":"Developers/BuildStack-Services/","text":"Build Stack Services system \u00b6 This page explains how the build stack system works for developers. How to define a new service \u00b6 A service only requires 2 files: * service.yml - Contains data for docker-compose * build.py - Contains logic that the menu system uses. A basic service \u00b6 Inside the service.yml is where the service data for docker-compose is housed, for example: adminer: container_name: adminer image: adminer restart: unless-stopped ports: - \"9080:8080\" It is important that the service name match the directory that it's in - that means that the adminer service must be placed into a folder called adminer inside the ./.templates directory. Basic build code for service \u00b6 At the very least, the build.py requires the following code: #!/usr/bin/env python3 issues = {} # Returned issues dict buildHooks = {} # Options, and others hooks haltOnErrors = True # Main wrapper function. Required to make local vars work correctly def main(): global currentServiceName # Name of the current service # This lets the menu know whether to put \" >> Options \" or not # This function is REQUIRED. def checkForOptionsHook(): try: buildHooks[\"options\"] = callable(runOptionsMenu) except: buildHooks[\"options\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPreBuildHook(): try: buildHooks[\"preBuildHook\"] = callable(preBuild) except: buildHooks[\"preBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPostBuildHook(): try: buildHooks[\"postBuildHook\"] = callable(postBuild) except: buildHooks[\"postBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForRunChecksHook(): try: buildHooks[\"runChecksHook\"] = callable(runChecks) except: buildHooks[\"runChecksHook\"] = False return buildHooks return buildHooks # Entrypoint for execution if haltOnErrors: eval(toRun)() else: try: eval(toRun)() except: pass # This check isn't required, but placed here for debugging purposes global currentServiceName # Name of the current service if currentServiceName == 'adminer': # Make sure you update this. main() else: print(\"Error. '{}' Tried to run 'adminer' config\".format(currentServiceName)) This code doesn't have any port conflicting checking or menu code in it, and just allows the service to be built as is. The best way to learn on extending the functionality of the service's build script is to look at the other services' build scripts. You can also check out the advanced sections on adding menus and checking for issues for services though for a deeper explanation of specific situations. Basic code for a service that uses bash \u00b6 If Python isn't your thing, here's a code blob you can copy and paste. Just be sure to update the lines where the comments start with --- #!/usr/bin/env python3 issues = {} # Returned issues dict buildHooks = {} # Options, and others hooks haltOnErrors = True # Main wrapper function. Required to make local vars work correctly def main(): import subprocess global dockerComposeServicesYaml # The loaded memory YAML of all checked services global toRun # Switch for which function to run when executed global buildHooks # Where to place the options menu result global currentServiceName # Name of the current service global issues # Returned issues dict global haltOnErrors # Turn on to allow erroring from deps.consts import servicesDirectory, templatesDirectory, volumesDirectory, servicesFileName # runtime vars serviceVolume = volumesDirectory + currentServiceName # Unused in example serviceService = servicesDirectory + currentServiceName # Unused in example serviceTemplate = templatesDirectory + currentServiceName # This lets the menu know whether to put \" >> Options \" or not # This function is REQUIRED. def checkForOptionsHook(): try: buildHooks[\"options\"] = callable(runOptionsMenu) except: buildHooks[\"options\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPreBuildHook(): try: buildHooks[\"preBuildHook\"] = callable(preBuild) except: buildHooks[\"preBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPostBuildHook(): try: buildHooks[\"postBuildHook\"] = callable(postBuild) except: buildHooks[\"postBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForRunChecksHook(): try: buildHooks[\"runChecksHook\"] = callable(runChecks) except: buildHooks[\"runChecksHook\"] = False return buildHooks return buildHooks # This service will not check anything unless this is set # This function is optional, and will run each time the menu is rendered def runChecks(): checkForIssues() return [] # This function is optional, and will run after the docker-compose.yml file is written to disk. def postBuild(): return True # This function is optional, and will run just before the build docker-compose.yml code. def preBuild(): execComm = \"bash {currentServiceTemplate}/build.sh\".format(currentServiceTemplate=serviceTemplate) # --- You may want to change this print(\"[Wireguard]: \", execComm) # --- Ensure to update the service name with yours subprocess.call(execComm, shell=True) # This is where the magic happens return True # ##################################### # Supporting functions below # ##################################### def checkForIssues(): return True if haltOnErrors: eval(toRun)() else: try: eval(toRun)() except: pass # This check isn't required, but placed here for debugging purposes global currentServiceName # Name of the current service if currentServiceName == 'wireguard': # --- Ensure to update the service name with yours main() else: print(\"Error. '{}' Tried to run 'wireguard' config\".format(currentServiceName)) # --- Ensure to update the service name with yours","title":"Build Stack Services system"},{"location":"Developers/BuildStack-Services/#build-stack-services-system","text":"This page explains how the build stack system works for developers.","title":"Build Stack Services system"},{"location":"Developers/BuildStack-Services/#how-to-define-a-new-service","text":"A service only requires 2 files: * service.yml - Contains data for docker-compose * build.py - Contains logic that the menu system uses.","title":"How to define a new service"},{"location":"Developers/BuildStack-Services/#a-basic-service","text":"Inside the service.yml is where the service data for docker-compose is housed, for example: adminer: container_name: adminer image: adminer restart: unless-stopped ports: - \"9080:8080\" It is important that the service name match the directory that it's in - that means that the adminer service must be placed into a folder called adminer inside the ./.templates directory.","title":"A basic service"},{"location":"Developers/BuildStack-Services/#basic-build-code-for-service","text":"At the very least, the build.py requires the following code: #!/usr/bin/env python3 issues = {} # Returned issues dict buildHooks = {} # Options, and others hooks haltOnErrors = True # Main wrapper function. Required to make local vars work correctly def main(): global currentServiceName # Name of the current service # This lets the menu know whether to put \" >> Options \" or not # This function is REQUIRED. def checkForOptionsHook(): try: buildHooks[\"options\"] = callable(runOptionsMenu) except: buildHooks[\"options\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPreBuildHook(): try: buildHooks[\"preBuildHook\"] = callable(preBuild) except: buildHooks[\"preBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPostBuildHook(): try: buildHooks[\"postBuildHook\"] = callable(postBuild) except: buildHooks[\"postBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForRunChecksHook(): try: buildHooks[\"runChecksHook\"] = callable(runChecks) except: buildHooks[\"runChecksHook\"] = False return buildHooks return buildHooks # Entrypoint for execution if haltOnErrors: eval(toRun)() else: try: eval(toRun)() except: pass # This check isn't required, but placed here for debugging purposes global currentServiceName # Name of the current service if currentServiceName == 'adminer': # Make sure you update this. main() else: print(\"Error. '{}' Tried to run 'adminer' config\".format(currentServiceName)) This code doesn't have any port conflicting checking or menu code in it, and just allows the service to be built as is. The best way to learn on extending the functionality of the service's build script is to look at the other services' build scripts. You can also check out the advanced sections on adding menus and checking for issues for services though for a deeper explanation of specific situations.","title":"Basic build code for service"},{"location":"Developers/BuildStack-Services/#basic-code-for-a-service-that-uses-bash","text":"If Python isn't your thing, here's a code blob you can copy and paste. Just be sure to update the lines where the comments start with --- #!/usr/bin/env python3 issues = {} # Returned issues dict buildHooks = {} # Options, and others hooks haltOnErrors = True # Main wrapper function. Required to make local vars work correctly def main(): import subprocess global dockerComposeServicesYaml # The loaded memory YAML of all checked services global toRun # Switch for which function to run when executed global buildHooks # Where to place the options menu result global currentServiceName # Name of the current service global issues # Returned issues dict global haltOnErrors # Turn on to allow erroring from deps.consts import servicesDirectory, templatesDirectory, volumesDirectory, servicesFileName # runtime vars serviceVolume = volumesDirectory + currentServiceName # Unused in example serviceService = servicesDirectory + currentServiceName # Unused in example serviceTemplate = templatesDirectory + currentServiceName # This lets the menu know whether to put \" >> Options \" or not # This function is REQUIRED. def checkForOptionsHook(): try: buildHooks[\"options\"] = callable(runOptionsMenu) except: buildHooks[\"options\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPreBuildHook(): try: buildHooks[\"preBuildHook\"] = callable(preBuild) except: buildHooks[\"preBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPostBuildHook(): try: buildHooks[\"postBuildHook\"] = callable(postBuild) except: buildHooks[\"postBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForRunChecksHook(): try: buildHooks[\"runChecksHook\"] = callable(runChecks) except: buildHooks[\"runChecksHook\"] = False return buildHooks return buildHooks # This service will not check anything unless this is set # This function is optional, and will run each time the menu is rendered def runChecks(): checkForIssues() return [] # This function is optional, and will run after the docker-compose.yml file is written to disk. def postBuild(): return True # This function is optional, and will run just before the build docker-compose.yml code. def preBuild(): execComm = \"bash {currentServiceTemplate}/build.sh\".format(currentServiceTemplate=serviceTemplate) # --- You may want to change this print(\"[Wireguard]: \", execComm) # --- Ensure to update the service name with yours subprocess.call(execComm, shell=True) # This is where the magic happens return True # ##################################### # Supporting functions below # ##################################### def checkForIssues(): return True if haltOnErrors: eval(toRun)() else: try: eval(toRun)() except: pass # This check isn't required, but placed here for debugging purposes global currentServiceName # Name of the current service if currentServiceName == 'wireguard': # --- Ensure to update the service name with yours main() else: print(\"Error. '{}' Tried to run 'wireguard' config\".format(currentServiceName)) # --- Ensure to update the service name with yours","title":"Basic code for a service that uses bash"},{"location":"Developers/Menu-System/","text":"Menu system \u00b6 This page explains how the menu system works for developers. Background \u00b6 Originally this script was written in bash. After a while it became obvious that bash wasn't well suited to dealing with all the different types of configuration files, and logic that goes with configuring everything. IOTstack needs to be accessible to all levels of programmers and tinkerers, not just ones experienced with Linux and bash. For this reason, it was rewritten in Python since the language syntax is easier to understand, and is more commonly used for scripting and programming than bash. Bash is still used in IOTstack where it makes sense to use it, but the menu system itself uses Python. The code it self while not being the most well structured or efficient, was intentionally made that way so that beginners and experienced programmers could contribute to the project. We are always open to improvements if you have suggestions. Menu Structure \u00b6 Each screen of the menu is its own Python script. You can find most of these in the ./scripts directory. When you select an item from the menu, and it changes screens, it actually dynamically loads and executes that Python script. It passes data as required by placing it into the global variable space so that both the child and the parent script can access it. Injecting and getting globals in a child script \u00b6 with open(childPythonScriptPath, \"rb\") as pythonDynamicImportFile: code = compile(pythonDynamicImportFile.read(), childPythonScriptPath, \"exec\") execGlobals = { \"globalKeyName\": \"globalKeyValue\" } execLocals = {} print(globalKeyName) # Will print out 'globalKeyValue' exec(code, execGlobals, execLocals) print(globalKeyName) # Will print out 'newValue' Reading and writing global variables in a child script \u00b6 def someFunction: global globalKeyName print(globalKeyName) # Will print out 'globalKeyValue' globalKeyName = \"newValue\" Each menu is its own python executable. The entry point is down the bottom of the file wrapped in a main() function to prevent variable scope creep. The code at the bottom of the main() function: if __name__ == 'builtins': Is actually where the execution path runs, all the code above it is just declared so that it can be called without ordering or scope issues. Optimisations \u00b6 It was obvious early on that the menu system would be slow on lower end devices, such as the Raspberry Pi, especially if it were rending a 4k terminal screen from a desktop via SSH. To mitigate this issue, not all of the screen is redrawn when there is a change. A \"Hotzone\" as it's called in the code, is usually rerendered when there's a change (such as pressing up or down to change an item selection, but not when scrolling). Full screen redraws are expensive and are only used when required, for example, when scrolling the pagination, selecting or deselecting a service, expanding or collapsing the menu and so on. Environments and encoding \u00b6 At the very beginning of the main menu screen ( ./scripts/main_menu.py ) the function checkRenderOptions() is run to determine what characters can be displayed on the screen. It will try various character sets, and eventually default to ASCII if none of the fancier stuff can be rendered. This setting is passed into of the sub menus through the submenu's global variables so that they don't have to recheck when they load. Sub-Menus \u00b6 From the main screen, you will see several sections leading to various submenus. Most of these menus work in the same way as the main menu. The only exception to this rule is the Build Stack menu, which is probably the most complex part of IOTstack. Build Stack Menu \u00b6 Path: ./scripts/buildstack_menu.py Loading \u00b6 Upon loading, the Build Stack menu will get a list of folders inside the ./templates directory and check for a build.py file inside each of them. This can be seen in the generateTemplateList() function, which is executed before the first rendering happens. The menu will then check if the file ./services/docker-compose.save.yml exists. This file is used to save the configuration of the last build. This happens in the loadCurrentConfigs() function. It is important that the service name in the compose file matches the folder name, any service that doesn't will either cause an error, or won't be loaded into the menu. If a previous build did exist the menu will then run the prepareMenuState() function that basically checks which items should be ticked, and check for any issues with the ticked items by running checkForIssues() . Selection and deselection \u00b6 When an item is selected, 3 things happen: 1. Update the UI variable ( menu ) with function checkMenuItem(selectionIndex) to let the user know the current state. 2. Update the array holding every checked item setCheckedMenuItems() . It uses the UI variable ( menu ) to know which items are set. 3. Check for any issues with the new list of selected items by running checkForIssues() . Check for options (submenus of services) \u00b6 During a full render sequence (this is not a hotzone render), the build stack menu checks to see if each of the services has an options menu. It does this by executing the build.py script of each of the services and passing in checkForOptionsHook into the toRun global variable property to see if the script has a runOptionsMenu function. If the service's function result is true, without error, then the options text will appear up for that menu item. Check for issues \u00b6 When a service is selected or deselected on the menu, the checkForIssues() function is run. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForRunChecksHook into the toRun global variable property to see if the script has a runChecks function. The runChecks function is different depending on the service, since each service has its own requirements. Generally though, the runChecks function should check for conflicting port conflicts again any of the other services that are enabled. The menu will still allow you to build the stack, even if issues are present, assumine there's no errors raised during the build process. Prebuild hook \u00b6 Pressing enter on the Build Stack menu kicks off the build process. The Build Stack menu will execute the runPrebuildHook() function. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForPreBuildHook into the toRun global variable property to see if the script has a preBuild function. The preBuild function is different depending on the service, since each service has its own requirements. Some services may not even use the prebuild hook. The prebuild is very useful for setting up the services' configuration however. For example, it can be used to autogenerate a password for a paticular service, or copy and modify a configuration file from the ./.templates directory into the ./services or ./volumes directory. Postbuild hook \u00b6 The Build Stack menu will execute the runPostBuildHook() function in the final step of the build process, after the docker-compose.yml file has been written to disk. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForPostBuildHook into the toRun global variable property to see if the script has a postBuild function. The postBuild function is different depending on the service, since each service has its own requirements. Most services won't require this function, but it can be useful for cleaning up temporary files and so on. The build process \u00b6 The selected services' yaml configuration is already loaded into memory before the build stack process is started. Run prebuildHooks. Create a new in memory docker-compose.yml structure. Merge the ./.templates/env.yml file into docker-compose.yml memory. If it exists merge the ./compose-override.yml file into memory Write the docker-compose in memory yaml structure to disk. Run postbuildHooks. Run postbuild.sh if it exists, with the list of services built.","title":"Menu system"},{"location":"Developers/Menu-System/#menu-system","text":"This page explains how the menu system works for developers.","title":"Menu system"},{"location":"Developers/Menu-System/#background","text":"Originally this script was written in bash. After a while it became obvious that bash wasn't well suited to dealing with all the different types of configuration files, and logic that goes with configuring everything. IOTstack needs to be accessible to all levels of programmers and tinkerers, not just ones experienced with Linux and bash. For this reason, it was rewritten in Python since the language syntax is easier to understand, and is more commonly used for scripting and programming than bash. Bash is still used in IOTstack where it makes sense to use it, but the menu system itself uses Python. The code it self while not being the most well structured or efficient, was intentionally made that way so that beginners and experienced programmers could contribute to the project. We are always open to improvements if you have suggestions.","title":"Background"},{"location":"Developers/Menu-System/#menu-structure","text":"Each screen of the menu is its own Python script. You can find most of these in the ./scripts directory. When you select an item from the menu, and it changes screens, it actually dynamically loads and executes that Python script. It passes data as required by placing it into the global variable space so that both the child and the parent script can access it.","title":"Menu Structure"},{"location":"Developers/Menu-System/#injecting-and-getting-globals-in-a-child-script","text":"with open(childPythonScriptPath, \"rb\") as pythonDynamicImportFile: code = compile(pythonDynamicImportFile.read(), childPythonScriptPath, \"exec\") execGlobals = { \"globalKeyName\": \"globalKeyValue\" } execLocals = {} print(globalKeyName) # Will print out 'globalKeyValue' exec(code, execGlobals, execLocals) print(globalKeyName) # Will print out 'newValue'","title":"Injecting and getting globals in a child script"},{"location":"Developers/Menu-System/#reading-and-writing-global-variables-in-a-child-script","text":"def someFunction: global globalKeyName print(globalKeyName) # Will print out 'globalKeyValue' globalKeyName = \"newValue\" Each menu is its own python executable. The entry point is down the bottom of the file wrapped in a main() function to prevent variable scope creep. The code at the bottom of the main() function: if __name__ == 'builtins': Is actually where the execution path runs, all the code above it is just declared so that it can be called without ordering or scope issues.","title":"Reading and writing global variables in a child script"},{"location":"Developers/Menu-System/#optimisations","text":"It was obvious early on that the menu system would be slow on lower end devices, such as the Raspberry Pi, especially if it were rending a 4k terminal screen from a desktop via SSH. To mitigate this issue, not all of the screen is redrawn when there is a change. A \"Hotzone\" as it's called in the code, is usually rerendered when there's a change (such as pressing up or down to change an item selection, but not when scrolling). Full screen redraws are expensive and are only used when required, for example, when scrolling the pagination, selecting or deselecting a service, expanding or collapsing the menu and so on.","title":"Optimisations"},{"location":"Developers/Menu-System/#environments-and-encoding","text":"At the very beginning of the main menu screen ( ./scripts/main_menu.py ) the function checkRenderOptions() is run to determine what characters can be displayed on the screen. It will try various character sets, and eventually default to ASCII if none of the fancier stuff can be rendered. This setting is passed into of the sub menus through the submenu's global variables so that they don't have to recheck when they load.","title":"Environments and encoding"},{"location":"Developers/Menu-System/#sub-menus","text":"From the main screen, you will see several sections leading to various submenus. Most of these menus work in the same way as the main menu. The only exception to this rule is the Build Stack menu, which is probably the most complex part of IOTstack.","title":"Sub-Menus"},{"location":"Developers/Menu-System/#build-stack-menu","text":"Path: ./scripts/buildstack_menu.py","title":"Build Stack Menu"},{"location":"Developers/Menu-System/#loading","text":"Upon loading, the Build Stack menu will get a list of folders inside the ./templates directory and check for a build.py file inside each of them. This can be seen in the generateTemplateList() function, which is executed before the first rendering happens. The menu will then check if the file ./services/docker-compose.save.yml exists. This file is used to save the configuration of the last build. This happens in the loadCurrentConfigs() function. It is important that the service name in the compose file matches the folder name, any service that doesn't will either cause an error, or won't be loaded into the menu. If a previous build did exist the menu will then run the prepareMenuState() function that basically checks which items should be ticked, and check for any issues with the ticked items by running checkForIssues() .","title":"Loading"},{"location":"Developers/Menu-System/#selection-and-deselection","text":"When an item is selected, 3 things happen: 1. Update the UI variable ( menu ) with function checkMenuItem(selectionIndex) to let the user know the current state. 2. Update the array holding every checked item setCheckedMenuItems() . It uses the UI variable ( menu ) to know which items are set. 3. Check for any issues with the new list of selected items by running checkForIssues() .","title":"Selection and deselection"},{"location":"Developers/Menu-System/#check-for-options-submenus-of-services","text":"During a full render sequence (this is not a hotzone render), the build stack menu checks to see if each of the services has an options menu. It does this by executing the build.py script of each of the services and passing in checkForOptionsHook into the toRun global variable property to see if the script has a runOptionsMenu function. If the service's function result is true, without error, then the options text will appear up for that menu item.","title":"Check for options (submenus of services)"},{"location":"Developers/Menu-System/#check-for-issues","text":"When a service is selected or deselected on the menu, the checkForIssues() function is run. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForRunChecksHook into the toRun global variable property to see if the script has a runChecks function. The runChecks function is different depending on the service, since each service has its own requirements. Generally though, the runChecks function should check for conflicting port conflicts again any of the other services that are enabled. The menu will still allow you to build the stack, even if issues are present, assumine there's no errors raised during the build process.","title":"Check for issues"},{"location":"Developers/Menu-System/#prebuild-hook","text":"Pressing enter on the Build Stack menu kicks off the build process. The Build Stack menu will execute the runPrebuildHook() function. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForPreBuildHook into the toRun global variable property to see if the script has a preBuild function. The preBuild function is different depending on the service, since each service has its own requirements. Some services may not even use the prebuild hook. The prebuild is very useful for setting up the services' configuration however. For example, it can be used to autogenerate a password for a paticular service, or copy and modify a configuration file from the ./.templates directory into the ./services or ./volumes directory.","title":"Prebuild hook"},{"location":"Developers/Menu-System/#postbuild-hook","text":"The Build Stack menu will execute the runPostBuildHook() function in the final step of the build process, after the docker-compose.yml file has been written to disk. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForPostBuildHook into the toRun global variable property to see if the script has a postBuild function. The postBuild function is different depending on the service, since each service has its own requirements. Most services won't require this function, but it can be useful for cleaning up temporary files and so on.","title":"Postbuild hook"},{"location":"Developers/Menu-System/#the-build-process","text":"The selected services' yaml configuration is already loaded into memory before the build stack process is started. Run prebuildHooks. Create a new in memory docker-compose.yml structure. Merge the ./.templates/env.yml file into docker-compose.yml memory. If it exists merge the ./compose-override.yml file into memory Write the docker-compose in memory yaml structure to disk. Run postbuildHooks. Run postbuild.sh if it exists, with the list of services built.","title":"The build process"},{"location":"Developers/PostBuild-Script/","text":"Postbuild BASH Script \u00b6 The postbuild bash script allows for executing arbitrary execution of bash commands after the stack has been build. How to use \u00b6 Place a file in the main directory called postbuild.sh . When the buildstack build logic finishes, it'll execute the postbuild.sh script, passing in each service selected from the buildstack menu as a parameter. This script is run each time the buildstack logic runs. Updates \u00b6 The postbuild.sh file has been added to gitignore, so it won't be updated by IOTstack when IOTstack is updated. It has also been added to the backup script so that it will be backed up with your personal IOTstack backups. Example postbuild.sh script \u00b6 The following script will print out each of the services built, and a custom message for nodered. If it was the first time the script was executed, it'll also output \"Fresh Install\" at the end, using a .install_tainted file for knowing. #!/bin/bash for iotstackService in \"$@\" do echo \"$iotstackService\" if [ \"$iotstackService\" == \"nodered\" ]; then echo \"NodeRed Installed!\" fi done if [ ! -f .install_tainted ]; then echo \"Fresh Install!\" touch .install_tainted fi What is my purpose? \u00b6 The postbuild script can be used to run custom bash commands, such as moving files, or issuing commands that your services expect to be completed before running.","title":"Postbuild BASH Script"},{"location":"Developers/PostBuild-Script/#postbuild-bash-script","text":"The postbuild bash script allows for executing arbitrary execution of bash commands after the stack has been build.","title":"Postbuild BASH Script"},{"location":"Developers/PostBuild-Script/#how-to-use","text":"Place a file in the main directory called postbuild.sh . When the buildstack build logic finishes, it'll execute the postbuild.sh script, passing in each service selected from the buildstack menu as a parameter. This script is run each time the buildstack logic runs.","title":"How to use"},{"location":"Developers/PostBuild-Script/#updates","text":"The postbuild.sh file has been added to gitignore, so it won't be updated by IOTstack when IOTstack is updated. It has also been added to the backup script so that it will be backed up with your personal IOTstack backups.","title":"Updates"},{"location":"Developers/PostBuild-Script/#example-postbuildsh-script","text":"The following script will print out each of the services built, and a custom message for nodered. If it was the first time the script was executed, it'll also output \"Fresh Install\" at the end, using a .install_tainted file for knowing. #!/bin/bash for iotstackService in \"$@\" do echo \"$iotstackService\" if [ \"$iotstackService\" == \"nodered\" ]; then echo \"NodeRed Installed!\" fi done if [ ! -f .install_tainted ]; then echo \"Fresh Install!\" touch .install_tainted fi","title":"Example postbuild.sh script"},{"location":"Developers/PostBuild-Script/#what-is-my-purpose","text":"The postbuild script can be used to run custom bash commands, such as moving files, or issuing commands that your services expect to be completed before running.","title":"What is my purpose?"}]}